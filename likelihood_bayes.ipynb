{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43db85a",
   "metadata": {},
   "source": [
    "\n",
    "<a id='likelihood-ratio-process'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4de34",
   "metadata": {},
   "source": [
    "# Likelihood Ratio Processes and Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c03a7f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture describes the role that **likelihood ratio processes** play in  **Bayesian learning**.\n",
    "\n",
    "As in [this lecture](https://python.quantecon.org/likelihood_ratio_process.html), we’ll use a simple statistical setting from [this lecture](https://python.quantecon.org/exchangeable.html).\n",
    "\n",
    "We’ll focus on how a likelihood ratio process and a **prior** probability determine a **posterior** probability.\n",
    "\n",
    "We’ll derive a convenient recursion for today’s posterior as a function of yesterday’s posterior and\n",
    "today’s multiplicative increment to a likelihood process.\n",
    "\n",
    "We’ll also present a useful generalization of that formula that represents today’s posterior in terms of an initial prior and\n",
    "today’s realization of the likelihood ratio process.\n",
    "\n",
    "We’ll study how, at least  in our setting, a Bayesian eventually learns the probability distribution that generates the data, an outcome that\n",
    "rests on the asymptotic behavior of likelihood ratio processes studied in [this lecture](https://python.quantecon.org/likelihood_ratio_process.html).\n",
    "\n",
    "We’ll also drill down into the psychology of our Bayesian learner and study dynamics  under his subjective beliefs.\n",
    "\n",
    "This lecture provides technical results that underly outcomes to be studied in [this lecture](https://python.quantecon.org/odu.html)\n",
    "and [this lecture](https://python.quantecon.org/wald_friedman.html) and [this lecture](https://python.quantecon.org/navy_captain.html).\n",
    "\n",
    "We’ll begin by loading some Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20663dbd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #set default figure size\n",
    "import numpy as np\n",
    "from numba import vectorize, jit, prange\n",
    "from math import gamma\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette()\n",
    "\n",
    "@jit\n",
    "def set_seed():\n",
    "    np.random.seed(142857)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114bf90",
   "metadata": {},
   "source": [
    "## The Setting\n",
    "\n",
    "We begin by reviewing the setting in [this lecture](https://python.quantecon.org/likelihood_ratio_process.html), which we adopt here too.\n",
    "\n",
    "A nonnegative random variable $ W $ has one of two probability density functions, either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "Before the beginning of time, nature once and for all decides whether she will draw a sequence of IID draws from $ f $ or from $ g $.\n",
    "\n",
    "We will sometimes let $ q $ be the density that nature chose once and for all, so\n",
    "that $ q $ is either $ f $ or $ g $, permanently.\n",
    "\n",
    "Nature knows which density it permanently draws from, but we the observers do not.\n",
    "\n",
    "We do know both $ f $ and $ g $, but we don’t know which density nature\n",
    "chose.\n",
    "\n",
    "But we want to know.\n",
    "\n",
    "To do that, we use observations.\n",
    "\n",
    "We observe a sequence $ \\{w_t\\}_{t=1}^T $ of $ T $ IID draws\n",
    "from either $ f $ or $ g $.\n",
    "\n",
    "We want to use these observations to infer whether nature chose $ f $ or\n",
    "$ g $.\n",
    "\n",
    "A **likelihood ratio process** is a useful tool for this task.\n",
    "\n",
    "To begin, we define the key component of a likelihood ratio process, namely, the time $ t $ likelihood ratio  as the random variable\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "We assume that $ f $ and $ g $ both put positive probabilities on the\n",
    "same intervals of possible realizations of the random variable $ W $.\n",
    "\n",
    "That means that under the $ g $ density,  $ \\ell (w_t)=\n",
    "\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)} $\n",
    "is evidently a nonnegative  random variable with mean $ 1 $.\n",
    "\n",
    "A **likelihood ratio process** for sequence\n",
    "$ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ is defined as\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "where $ w^t=\\{ w_1,\\dots,w_t\\} $ is a history of\n",
    "observations up to and including time $ t $.\n",
    "\n",
    "Sometimes for shorthand we’ll write\n",
    "\n",
    "$$\n",
    "L_t =  L(w^t) = \\frac{f(w^t)}{g(w^t)}\n",
    "$$\n",
    "\n",
    "where we use the conventions\n",
    "that $ f(w^t) = f(w_1) f(w_2) \\ldots f(w_t) $ and $ g(w^t) = g(w_1) g(w_2) \\ldots g(w_t) $.\n",
    "\n",
    "Notice that the likelihood process satisfies the *recursion* or\n",
    "*multiplicative decomposition*\n",
    "\n",
    "$$\n",
    "L(w^t) = \\ell (w_t) L (w^{t-1}) .\n",
    "$$\n",
    "\n",
    "The likelihood ratio and its logarithm are key tools for making\n",
    "inferences using a classic frequentist approach due to Neyman and\n",
    "Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id259)].\n",
    "\n",
    "We’ll again deploy the following Python code from [this lecture](https://python.quantecon.org/likelihood_ratio_process.html) that\n",
    "evaluates $ f $ and $ g $ as two different\n",
    "beta distributions, then computes and simulates an associated likelihood\n",
    "ratio process by generating a sequence $ w^t $ from *some*\n",
    "probability distribution, for example, a sequence of  IID draws from $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead78a6e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters in the two beta distributions.\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# The two density functions.\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02efedb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate(a, b, T=50, N=500):\n",
    "    '''\n",
    "    Generate N sets of T observations of the likelihood ratio,\n",
    "    return as N x T matrix.\n",
    "\n",
    "    '''\n",
    "\n",
    "    l_arr = np.empty((N, T))\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        for j in range(T):\n",
    "            w = np.random.beta(a, b)\n",
    "            l_arr[i, j] = f(w) / g(w)\n",
    "\n",
    "    return l_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f0f72",
   "metadata": {},
   "source": [
    "We’ll also use the following Python code to prepare some informative simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda16e18",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b, N=50000)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adbc7d9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_f = simulate(F_a, F_b, N=50000)\n",
    "l_seq_f = np.cumprod(l_arr_f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed44610",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Process and Bayes’ Law\n",
    "\n",
    "Let $ \\pi_{t+1} $ be a Bayesian posterior probability defined as\n",
    "\n",
    "\n",
    "<a id='equation-eq-defbayesposterior'></a>\n",
    "$$\n",
    "\\pi_{t+1} = {\\rm Prob}(q=f|w^{t+1}) \\tag{26.1}\n",
    "$$\n",
    "\n",
    "The likelihood ratio process is a principal actor in the formula that governs the evolution\n",
    "of the posterior probability $ \\pi_t $, an instance of **Bayes’ Law**.\n",
    "\n",
    "Bayes’ law is just the following application of the standardformula for conditional probability:\n",
    "\n",
    "$$\n",
    "{\\rm Prob}(q=f|w^{t+1}) = \\frac { {\\rm Prob}(q=f|w^{t} ) f(w_{t+1})}{ {\\rm Prob}(q=f|w^{t} ) f(w_{t+1}) + (1 - {\\rm Prob}(q=f|w^{t} )) g(w_{t+1})}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-bayes150'></a>\n",
    "$$\n",
    "\\pi_{t+1} = \\frac { \\pi_t f(w_{t+1})}{ \\pi_t f(w_{t+1}) + (1 - \\pi_t) g(w_{t+1})} \\tag{26.2}\n",
    "$$\n",
    "\n",
    "Evidently,  the above equation asserts that\n",
    "\n",
    "$$\n",
    "{\\rm Prob}(q=f|w^{t+1}) = \\frac{{\\rm Prob}(q=f|w^{t}) f(w_{t+1} )} {{\\rm Prob}(w_{t+1})}\n",
    "$$\n",
    "\n",
    "Dividing both  the numerator and the denominator on the right side of the  equation [(26.2)](#equation-eq-bayes150) by $ g(w_{t+1}) $ implies the recursion\n",
    "\n",
    "\n",
    "<a id='equation-eq-recur1'></a>\n",
    "$$\n",
    "\\pi_{t+1}=\\frac{\\pi_{t} l_t(w_{t+1})}{\\pi_{t} l_t(w_t)+1-\\pi_{t}} \\tag{26.3}\n",
    "$$\n",
    "\n",
    "with $ \\pi_{0} $ being a Bayesian prior probability that $ q = f $,\n",
    "i.e., a personal or subjective belief about $ q $ based on our having seen no data.\n",
    "\n",
    "Below we define a Python function that updates belief $ \\pi $ using\n",
    "likelihood ratio $ \\ell $ according to  recursion [(26.3)](#equation-eq-recur1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb434c8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(π, l):\n",
    "    \"Update π using likelihood l\"\n",
    "\n",
    "    # Update belief\n",
    "    π = π * l / (π * l + 1 - π)\n",
    "\n",
    "    return π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eeeab",
   "metadata": {},
   "source": [
    "Formula [(26.3)](#equation-eq-recur1) can be generalized  by iterating on it and thereby deriving an\n",
    "expression for  the time $ t $ posterior $ \\pi_{t+1} $ as a function\n",
    "of the time $ 0 $ prior $ \\pi_0 $ and the likelihood ratio process\n",
    "$ L(w^{t+1}) $ at time $ t $.\n",
    "\n",
    "To begin, notice that the updating rule\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}\n",
    "=\\frac{\\pi_{t}\\ell \\left(w_{t+1}\\right)}\n",
    "{\\pi_{t}\\ell \\left(w_{t+1}\\right)+\\left(1-\\pi_{t}\\right)}\n",
    "$$\n",
    "\n",
    "implies\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{\\pi_{t+1}}\n",
    "    &=\\frac{\\pi_{t}\\ell \\left(w_{t+1}\\right)\n",
    "        +\\left(1-\\pi_{t}\\right)}{\\pi_{t}\\ell \\left(w_{t+1}\\right)} \\\\\n",
    "    &=1-\\frac{1}{\\ell \\left(w_{t+1}\\right)}\n",
    "        +\\frac{1}{\\ell \\left(w_{t+1}\\right)}\\frac{1}{\\pi_{t}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\frac{1}{\\pi_{t+1}}-1\n",
    "=\\frac{1}{\\ell \\left(w_{t+1}\\right)}\\left(\\frac{1}{\\pi_{t}}-1\\right).\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{1}{\\pi_{t+1}}-1\n",
    "    =\\frac{1}{\\prod_{i=1}^{t+1}\\ell \\left(w_{i}\\right)}\n",
    "        \\left(\\frac{1}{\\pi_{0}}-1\\right)\n",
    "    =\\frac{1}{L\\left(w^{t+1}\\right)}\\left(\\frac{1}{\\pi_{0}}-1\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $ \\pi_{0}\\in\\left(0,1\\right) $ and\n",
    "$ L\\left(w^{t+1}\\right)>0 $, we can verify that\n",
    "$ \\pi_{t+1}\\in\\left(0,1\\right) $.\n",
    "\n",
    "After rearranging the preceding equation, we can express $ \\pi_{t+1} $ as a\n",
    "function of  $ L\\left(w^{t+1}\\right) $, the  likelihood ratio process at $ t+1 $,\n",
    "and the initial prior $ \\pi_{0} $\n",
    "\n",
    "\n",
    "<a id='equation-eq-bayeslaw103'></a>\n",
    "$$\n",
    "\\pi_{t+1}=\\frac{\\pi_{0}L\\left(w^{t+1}\\right)}{\\pi_{0}L\\left(w^{t+1}\\right)+1-\\pi_{0}} . \\tag{26.4}\n",
    "$$\n",
    "\n",
    "Formula [(26.4)](#equation-eq-bayeslaw103) generalizes formula [(26.3)](#equation-eq-recur1).\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Fomula [(26.4)](#equation-eq-bayeslaw103) can also be derived by starting from the formula for  conditional probability\n",
    "\n",
    "$$\n",
    "\\pi_{t+1} \\equiv {\\rm Prob}(q=f|w^{t+1}) = \\frac { \\pi_0 f(w^{t+1})}{ \\pi_0 f(w^{t+1}) + (1 - \\pi_0) g(w^{t+1})}\n",
    "$$\n",
    "\n",
    "and then dividing the numerator and the denominator on the right side by $ g(w^{t+1}) $.\n",
    "\n",
    "Formula [(26.4)](#equation-eq-bayeslaw103)  can be regarded as a one step  revision of prior probability $ \\pi_0 $ after seeing\n",
    "the batch of data $ \\left\\{ w_{i}\\right\\} _{i=1}^{t+1} $.\n",
    "\n",
    "Formula [(26.4)](#equation-eq-bayeslaw103) shows the key role that the likelihood ratio process  $ L\\left(w^{t+1}\\right) $ plays in determining\n",
    "the posterior probability $ \\pi_{t+1} $.\n",
    "\n",
    "Formula [(26.4)](#equation-eq-bayeslaw103) is the foundation for the insight that, because of how the likelihood ratio process behaves\n",
    "as $ t \\rightarrow + \\infty $, the likelihood ratio process dominates the initial prior $ \\pi_0 $ in determining the\n",
    "limiting behavior of $ \\pi_t $.\n",
    "\n",
    "To illustrate this insight, below we will plot  graphs showing **one** simulated\n",
    "path of the  likelihood ratio process $ L_t $ along with two paths of\n",
    "$ \\pi_t $ that are associated with the *same* realization of the likelihood ratio process but *different* initial prior probabilities $ \\pi_{0} $.\n",
    "\n",
    "First, we tell Python two values of $ \\pi_0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858ee80",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "π1, π2 = 0.2, 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df57a87",
   "metadata": {},
   "source": [
    "Next we generate paths of the likelihood ratio process $ L_t $ and the posterior $ \\pi_t $ for a\n",
    "history of IID draws from density $ f $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f8d6d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "T = l_arr_f.shape[1]\n",
    "π_seq_f = np.empty((2, T+1))\n",
    "π_seq_f[:, 0] = π1, π2\n",
    "\n",
    "for t in range(T):\n",
    "    for i in range(2):\n",
    "        π_seq_f[i, t+1] = update(π_seq_f[i, t], l_arr_f[0, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96579c1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "for i in range(2):\n",
    "    ax1.plot(range(T+1), π_seq_f[i, :], label=fr\"$\\pi_0$={π_seq_f[i, 0]}\")\n",
    "\n",
    "ax1.set_ylabel(r\"$\\pi_t$\")\n",
    "ax1.set_xlabel(\"t\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"when f governs data\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, T+1), np.log(l_seq_f[0, :]), '--', color='b')\n",
    "ax2.set_ylabel(\"$log(L(w^{t}))$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff695dc",
   "metadata": {},
   "source": [
    "The dotted line in the graph above records the logarithm of the  likelihood ratio process $ \\log L(w^t) $.\n",
    "\n",
    "Please note that there are two different scales on the $ y $ axis.\n",
    "\n",
    "Now let’s study what happens when the history consists of IID draws from density $ g $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d0ab5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "T = l_arr_g.shape[1]\n",
    "π_seq_g = np.empty((2, T+1))\n",
    "π_seq_g[:, 0] = π1, π2\n",
    "\n",
    "for t in range(T):\n",
    "    for i in range(2):\n",
    "        π_seq_g[i, t+1] = update(π_seq_g[i, t], l_arr_g[0, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a669a9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "for i in range(2):\n",
    "    ax1.plot(range(T+1), π_seq_g[i, :], label=fr\"$\\pi_0$={π_seq_g[i, 0]}\")\n",
    "\n",
    "ax1.set_ylabel(r\"$\\pi_t$\")\n",
    "ax1.set_xlabel(\"t\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"when g governs data\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, T+1), np.log(l_seq_g[0, :]), '--', color='b')\n",
    "ax2.set_ylabel(\"$log(L(w^{t}))$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e803f",
   "metadata": {},
   "source": [
    "Below we offer Python code that verifies that nature chose permanently to draw from density $ f $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76087ab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "π_seq = np.empty((2, T+1))\n",
    "π_seq[:, 0] = π1, π2\n",
    "\n",
    "for i in range(2):\n",
    "    πL = π_seq[i, 0] * l_seq_f[0, :]\n",
    "    π_seq[i, 1:] = πL / (πL + 1 - π_seq[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbdccf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "np.abs(π_seq - π_seq_f).max() < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1d2e6",
   "metadata": {},
   "source": [
    "We thus conclude that  the likelihood ratio process is a key ingredient of the formula [(26.4)](#equation-eq-bayeslaw103) for\n",
    "a Bayesian’s posterior probabilty that nature has drawn history $ w^t $ as repeated draws from density\n",
    "$ f $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62a2e1",
   "metadata": {},
   "source": [
    "## Another timing protocol\n",
    "\n",
    "Let’s study how the posterior probability $ \\pi_t = {\\rm Prob}(q=f|w^{t}) $ behaves when nature generates the\n",
    "history $ w^t = w_1, w_2, \\ldots, w_t $ under a different timing protocol.\n",
    "\n",
    "Until now we assumed that before time $ 1 $ nature somehow chose to draw $ w^t $ as an iid sequence from **either** $ f $ **or** $ g $.\n",
    "\n",
    "Nature’s decision about whether to draw from $ f $ or $ g $ was thus **permanent**.\n",
    "\n",
    "We now assume a different timing protocol in which before **each period** $ t =1, 2, \\ldots $ nature flips an $ x $-weighted coin and with probability\n",
    "$ x \\in (0,1) $  draws from $ f $ in period $ t $ and with probability $ 1 - x $ draws from $ g $.\n",
    "\n",
    "Under this timing protocol, nature  draws permanently from **neither** $ f $ **nor** $ g $, so a statistician who thinks that nature is drawing\n",
    "i.i.d. draws **permanently** from one of them is mistaken.\n",
    "\n",
    "- in truth, nature actually draws **permanently** from an $ x $-mixture of $ f $ and $ g $ – a distribution that is neither $ f $ nor $ g $ when\n",
    "  $ x \\in (0,1) $  \n",
    "\n",
    "\n",
    "Thus, the  Bayesian prior $ \\pi_0 $ and the sequence of posterior probabilities described by equation [(26.4)](#equation-eq-bayeslaw103) should **not** be interpreted as the statistician’s opinion about the mixing parameter  $ x $ under the alternative timing protocol in which nature draws from an $ x $-mixture of $ f $ and $ g $.\n",
    "\n",
    "This is clear when we remember  the definition of $ \\pi_t $ in equation [(26.1)](#equation-eq-defbayesposterior), which for convenience we repeat here:\n",
    "\n",
    "$$\n",
    "\\pi_{t+1} = {\\rm Prob}(q=f|w^{t+1})\n",
    "$$\n",
    "\n",
    "Let’s write some Python code to study how $ \\pi_t $ behaves when nature actually generates data as i.i.d. draws from  neither $ f $ nor from $ g $\n",
    "but instead as i.i.d. draws from an $ x $-mixture of two beta distributions.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">This is a situation in which the statistician’s model is misspecified, so we should anticipate that a Kullback-Liebler divergence with respect to an $ x $-mixture distribution will shape outcomes.\n",
    "\n",
    "We can study how $ \\pi_t $ would behave for various values of nature’s mixing probability $ x $.\n",
    "\n",
    "First, let’s create a function to simulate data under the mixture timing protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb410f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate_mixture_path(x_true, T):\n",
    "    \"\"\"\n",
    "    Simulate T observations under mixture timing protocol.\n",
    "    \"\"\"\n",
    "    w = np.empty(T)\n",
    "    for t in range(T):\n",
    "        if np.random.rand() < x_true:\n",
    "            w[t] = np.random.beta(F_a, F_b)\n",
    "        else:\n",
    "            w[t] = np.random.beta(G_a, G_b)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a588c",
   "metadata": {},
   "source": [
    "Let’s generate a sequence of observations from this mixture model with a true mixing probability of $ x=0.5 $.\n",
    "\n",
    "We will first use this sequence to study how $ \\pi_t $ behaves.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Later, we can use it to study how a statistician who knows that an $ x $-mixture of $ f $ and $ g $ could construct  maximum likelihood or Bayesian estimators of $ x $ along with the free parameters of $ f $ and $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605b5b8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_true = 0.5\n",
    "T_mix = 200\n",
    "\n",
    "# Three different priors with means 0.25, 0.5, 0.75\n",
    "prior_params = [(1, 3), (1, 1), (3, 1)]\n",
    "prior_means = [a/(a+b) for a, b in prior_params]\n",
    "\n",
    "# Generate one path of observations from the mixture\n",
    "set_seed()\n",
    "w_mix = simulate_mixture_path(x_true, T_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b48c1",
   "metadata": {},
   "source": [
    "### Behavior of $ \\pi_t $ under wrong model\n",
    "\n",
    "Let’s study how the posterior probability  $ \\pi_t $ that nature permanently draws from $ f $  behaves when data are actually generated by\n",
    "an $ x $-mixture of $ f $ and $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25734cfd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "T_plot = 200\n",
    "\n",
    "for i, mean0 in enumerate(prior_means):\n",
    "    π_wrong = np.empty(T_plot + 1)\n",
    "    π_wrong[0] = mean0\n",
    "    \n",
    "    # Compute likelihood ratios for the mixture data\n",
    "    for t in range(T_plot):\n",
    "        l_t = f(w_mix[t]) / g(w_mix[t])\n",
    "        π_wrong[t + 1] = update(π_wrong[t], l_t)\n",
    "    \n",
    "    ax.plot(range(T_plot + 1), π_wrong, \n",
    "            label=fr'$\\pi_0 = ${mean0:.2f}', \n",
    "            color=colors[i], linewidth=2)\n",
    "\n",
    "ax.axhline(y=x_true, color='black', linestyle='--', \n",
    "           label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel(r'$\\pi_t$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9e096",
   "metadata": {},
   "source": [
    "Evidently,  $ \\pi_t $ converges  to 1.\n",
    "\n",
    "This indicates that the model concludes that the data is generated by $ f $.\n",
    "\n",
    "Why does this happen?\n",
    "\n",
    "Given $ x = 0.5 $, the data generating process is a mixture of $ f $ and $ g $: $ m(w) = \\frac{1}{2}f(w) + \\frac{1}{2}g(w) $.\n",
    "\n",
    "A widely used  measure of “closeness” between two distributions is the Kullback-Leibler (KL) divergence.\n",
    "\n",
    "Let’s check the KL divergence of the mixture distribution $ m $ from both $ f $ and $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f4bd1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL(f, g):\n",
    "    \"\"\"\n",
    "    Compute KL divergence KL(f, g)\n",
    "    \"\"\"\n",
    "    integrand = lambda w: f(w) * np.log(f(w) / g(w))\n",
    "    val, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return val\n",
    "\n",
    "\n",
    "def compute_div_m(f, g):\n",
    "    \"\"\"\n",
    "    Compute Jensen-Shannon divergence\n",
    "    \"\"\"\n",
    "    def m(w):\n",
    "        return 0.5 * (f(w) + g(w))\n",
    "    \n",
    "    return compute_KL(m, f), compute_KL(m, g)\n",
    "\n",
    "\n",
    "KL_f, KL_g = compute_div_m(f, g)\n",
    "\n",
    "print(f'KL(m, f) = {KL_f:.3f}\\nKL(m, g) = {KL_g:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd349e9e",
   "metadata": {},
   "source": [
    "Since $ KL(m, f) < KL(m, g) $, $ f $ is “closer” to the mixture distribution $ m $.\n",
    "\n",
    "Hence by our discussion on KL divergence and likelihood ratio process in\n",
    "[Likelihood Ratio Processes](https://python.quantecon.org/likelihood_ratio_process.html), $ log(L_t) \\to \\infty $ as $ t \\to \\infty $.\n",
    "\n",
    "Now looking back to the key equation [(26.4)](#equation-eq-bayeslaw103).\n",
    "\n",
    "Consider the function\n",
    "\n",
    "$$\n",
    "h(z) = \\frac{\\pi_0 z}{\\pi_0 z + 1 - \\pi_0}.\n",
    "$$\n",
    "\n",
    "The limit $ \\lim_{z \\to \\infty} h(z) $ is 1.\n",
    "\n",
    "Hence $ \\pi_t \\to 1 $ as $ t \\to \\infty $ for any $ \\pi_0 \\in (0,1) $.\n",
    "\n",
    "This explains what we observed in the plot above.\n",
    "\n",
    "But how can we learn the true mixing parameter $ x $?\n",
    "\n",
    "This topic is taken up in [this lecture](https://python.quantecon.org/mix_model.html).\n",
    "\n",
    "We also explore this topic in the exrecise below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8d470",
   "metadata": {},
   "source": [
    "### Exercise 26.1\n",
    "\n",
    "The true data generating process is a mixture, and one of the  parameters to be learned is the mixing proportion $ x $.\n",
    "\n",
    "A correct Bayesian approach should directly model the uncertainty about $ x $ and update beliefs about it as new data arrives.\n",
    "\n",
    "Here is the algorithm:\n",
    "\n",
    "First we specify a prior distribution for $ x $ given by $ x \\sim \\text{Beta}(\\alpha_0, \\beta_0) $ with sexpectation $ \\mathbb{E}[x] = \\frac{\\alpha_0}{\\alpha_0 + \\beta_0} $.\n",
    "\n",
    "The likelihood for a single observation $ w_t $ is $ p(w_t|x) = x f(w_t) + (1-x) g(w_t) $.\n",
    "\n",
    "For a sequence $ w^t = (w_1, \\dots, w_t) $, the likelihood is $ p(w^t|x) = \\prod_{i=1}^t p(w_i|x) $.\n",
    "\n",
    "The posterior distribution is updated using $ p(x|w^t) \\propto p(w^t|x) p(x) $.\n",
    "\n",
    "Recursively, the posterior after $ w_t $ is $ p(x|w^t) \\propto p(w_t|x) p(x|w^{t-1}) $.\n",
    "\n",
    "Without a conjugate prior, we can approximate the posterior by discretizing $ x $ into a grid.\n",
    "\n",
    "Your task is to implement this algorithm in Python.\n",
    "\n",
    "You can verify your implementation by checking that the posterior mean converges to the true value of $ x $\n",
    "as $ t $ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b504f1",
   "metadata": {},
   "source": [
    "### Solution to[ Exercise 26.1](https://python.quantecon.org/#likelihood_bayes_ex1)\n",
    "\n",
    "Here is one solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52527f5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def learn_x_bayesian(observations, α0, β0, grid_size=2000):\n",
    "    \"\"\"\n",
    "    Sequential Bayesian learning of the mixing probability x\n",
    "    using a grid approximation.\n",
    "    \"\"\"\n",
    "    w = np.asarray(observations)\n",
    "    T = w.size\n",
    "\n",
    "    x_grid = np.linspace(1e-3, 1 - 1e-3, grid_size)\n",
    "\n",
    "    # Log prior\n",
    "    log_prior = (α0 - 1) * np.log(x_grid) + (β0 - 1) * np.log1p(-x_grid)\n",
    "\n",
    "    μ_path = np.empty(T + 1)\n",
    "    μ_path[0] = α0 / (α0 + β0)\n",
    "\n",
    "    log_post = log_prior.copy()\n",
    "\n",
    "    for t in range(T):\n",
    "        wt = w[t]\n",
    "        # P(w_t | x) = x f(w_t) + (1 - x) g(w_t)\n",
    "        like = x_grid * f(wt) + (1 - x_grid) * g(wt)\n",
    "        log_post += np.log(like)\n",
    "\n",
    "        # normalize\n",
    "        log_post -= log_post.max()\n",
    "        post = np.exp(log_post)\n",
    "        post /= post.sum()\n",
    "\n",
    "        μ_path[t + 1] = np.sum(x_grid * post)\n",
    "\n",
    "    return μ_path\n",
    "\n",
    "x_posterior_means = [learn_x_bayesian(w_mix, α0, β0) for α0, β0 in prior_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ec11a",
   "metadata": {},
   "source": [
    "Let’s visualize how the posterior mean of $ x $ evolves over time, starting from three different prior beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034515cc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (x_means, mean0) in enumerate(zip(x_posterior_means, prior_means)):\n",
    "    ax.plot(range(T_mix + 1), x_means, \n",
    "            label=fr'Prior mean = ${mean0:.2f}$', \n",
    "            color=colors[i], linewidth=2)\n",
    "\n",
    "ax.axhline(y=x_true, color='black', linestyle='--', \n",
    "           label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('Posterior mean of $x$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1440d",
   "metadata": {},
   "source": [
    "The plot shows that regardless of the initial prior belief, all three posterior means eventually converge towards the true value of $ x=0.5 $.\n",
    "\n",
    "Next, let’s look at multiple simulations with a longer time horizon, all starting from a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d600fb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "n_paths = 20\n",
    "T_long = 10_000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for j in range(n_paths):\n",
    "    w_path = simulate_mixture_path(x_true, T_long)\n",
    "    x_means = learn_x_bayesian(w_path, 1, 1)  # Uniform prior\n",
    "    ax.plot(range(T_long + 1), x_means, alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.axhline(y=x_true, color='red', linestyle='--', \n",
    "            label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_ylabel('Posterior mean of $x$')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d733a5",
   "metadata": {},
   "source": [
    "We can see that the posterior mean of $ x $ converges to the true value $ x=0.5 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06a26d",
   "metadata": {},
   "source": [
    "## Behavior of  posterior probability $ \\{\\pi_t\\} $  under the subjective probability distribution\n",
    "\n",
    "We’ll end this lecture by briefly studying what our Baysian learner expects to learn under the\n",
    "subjective beliefs $ \\pi_t $ cranked out by Bayes’ law.\n",
    "\n",
    "This will provide us with some perspective  on our application of  Bayes’s law as a theory of learning.\n",
    "\n",
    "As we shall see, at each time $ t $, the Bayesian learner knows that he will be surprised.\n",
    "\n",
    "But he expects that new information will not lead him  to change his beliefs.\n",
    "\n",
    "And it won’t on average under his subjective beliefs.\n",
    "\n",
    "We’ll continue with our setting in which a McCall worker  knows that successive\n",
    "draws of his wage are drawn from either $ F $ or $ G $, but  does not know which of these two  distributions\n",
    "nature has drawn once-and-for-all before time $ 0 $.\n",
    "\n",
    "We’ll review and reiterate and rearrange some formulas that we have encountered above and in associated lectures.\n",
    "\n",
    "The worker’s initial beliefs induce a joint probability distribution\n",
    "over a potentially infinite sequence of draws $ w_0, w_1, \\ldots $.\n",
    "\n",
    "Bayes’ law is simply an application of  laws of\n",
    "probability to compute the conditional distribution of the $ t $th draw $ w_t $ conditional on $ [w_0, \\ldots, w_{t-1}] $.\n",
    "\n",
    "After our worker puts a subjective probability $ \\pi_{-1} $ on nature having selected distribution $ F $, we have in effect assumes from the start that the   decision maker **knows** the joint distribution  for the process $ \\{w_t\\}_{t=0} $.\n",
    "\n",
    "We assume that the worker also knows the laws of probability theory.\n",
    "\n",
    "A respectable view is that Bayes’ law is less a theory of learning than a statement  about the consequences of information inflows for a decision maker who thinks he knows the truth (i.e., a joint probability distribution) from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58164d2",
   "metadata": {},
   "source": [
    "### Mechanical details again\n",
    "\n",
    "At time $ 0 $ **before** drawing a wage offer, the worker attaches probability $ \\pi_{-1} \\in (0,1) $ to the distribution being $ F $.\n",
    "\n",
    "Before drawing a wage at time $ 0 $, the  worker thus believes that the density of $ w_0 $\n",
    "is\n",
    "\n",
    "$$\n",
    "h(w_0;\\pi_{-1}) = \\pi_{-1} f(w_0) + (1-\\pi_{-1}) g(w_0).\n",
    "$$\n",
    "\n",
    "Let $ a \\in \\{ f, g\\} $ be an index that indicates whether  nature chose permanently to draw from distribution $ f $ or from distribution $ g $.\n",
    "\n",
    "After drawing $ w_0 $, the worker uses Bayes’ law to deduce that\n",
    "the posterior  probability $ \\pi_0 = {\\rm Prob} ({a = f | w_0}) $\n",
    "that the density is $ f(w) $ is\n",
    "\n",
    "$$\n",
    "\\pi_0 = { \\pi_{-1} f(w_0) \\over \\pi_{-1} f(w_0) + (1-\\pi_{-1}) g(w_0)} .\n",
    "$$\n",
    "\n",
    "More generally,  after making the $ t $th draw and having   observed   $ w_t, w_{t-1}, \\ldots, w_0 $, the worker believes that\n",
    "the probability that $ w_{t+1} $ is  being drawn from  distribution  $ F $ is\n",
    "\n",
    "\n",
    "<a id='equation-eq-like44'></a>\n",
    "$$\n",
    "\\pi_t = \\pi_t(w_t | \\pi_{t-1}) \\equiv { \\pi_{t-1} f(w_t)/g(w_t) \\over \\pi_{t-1} f(w_t)/g(w_t) + (1-\\pi_{t-1})} \\tag{26.5}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\pi_t=\\frac{\\pi_{t-1} l_t(w_t)}{\\pi_{t-1} l_t(w_t)+1-\\pi_{t-1}}\n",
    "$$\n",
    "\n",
    "and that the density of $ w_{t+1} $ conditional on $ w_t, w_{t-1}, \\ldots, w_0 $ is\n",
    "\n",
    "$$\n",
    "h(w_{t+1};\\pi_{t}) = \\pi_{t} f(w_{t+1}) + (1-\\pi_{t}) g(w_{t+1}) .\n",
    "$$\n",
    "\n",
    "Notice that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(\\pi_t | \\pi_{t-1}) & = \\int \\Bigl[  { \\pi_{t-1} f(w) \\over \\pi_{t-1} f(w) + (1-\\pi_{t-1})g(w)  } \\Bigr]\n",
    " \\Bigl[ \\pi_{t-1} f(w) + (1-\\pi_{t-1})g(w) \\Bigr]  d w \\cr\n",
    "& = \\pi_{t-1} \\int  f(w) dw  \\cr\n",
    "              & = \\pi_{t-1}, \\cr\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so that the process $ \\pi_t $ is a **martingale**.\n",
    "\n",
    "Indeed, it is a **bounded martingale** because each $ \\pi_t $, being a probability,\n",
    "is between $ 0 $ and $ 1 $.\n",
    "\n",
    "In the first line in the above string of equalities, the term in the first set of brackets\n",
    "is just $ \\pi_t $ as a function of $ w_{t} $, while the term in the second set of brackets is the density of $ w_{t} $ conditional\n",
    "on $ w_{t-1}, \\ldots , w_0 $ or equivalently conditional on the *sufficient statistic* $ \\pi_{t-1} $ for $ w_{t-1}, \\ldots , w_0 $.\n",
    "\n",
    "Notice that here we are computing $ E(\\pi_t | \\pi_{t-1}) $ under the **subjective** density described in the second\n",
    "term in brackets.\n",
    "\n",
    "Because $ \\{\\pi_t\\} $ is a bounded martingale sequence, it follows from the **martingale convergence theorem** that $ \\pi_t $ converges almost surely to a random variable in $ [0,1] $.\n",
    "\n",
    "Practically, this means that  probability one is  attached to   sample paths\n",
    "$ \\{\\pi_t\\}_{t=0}^\\infty $ that  converge.\n",
    "\n",
    "According to the theorem,  it  different sample  paths  can converge to different limiting values.\n",
    "\n",
    "Thus, let $ \\{\\pi_t(\\omega)\\}_{t=0}^\\infty $ denote a particular sample path indexed by a particular $ \\omega\n",
    "\\in \\Omega $.\n",
    "\n",
    "We can think of nature as drawing an $ \\omega \\in \\Omega $ from a probability distribution\n",
    "$ {\\textrm{Prob}} \\Omega $ and then generating a single realization (or *simulation*) $ \\{\\pi_t(\\omega)\\}_{t=0}^\\infty $ of the process.\n",
    "\n",
    "The limit points of  $ \\{\\pi_t(\\omega)\\}_{t=0}^\\infty $ as $ t \\rightarrow +\\infty $ are realizations of a random variable that  is swept out as we sample $ \\omega $ from $ \\Omega $ and construct repeated draws of $ \\{\\pi_t(\\omega)\\}_{t=0}^\\infty $.\n",
    "\n",
    "By staring at law of motion [(26.3)](#equation-eq-recur1) or [(26.5)](#equation-eq-like44) , we can figure out some things about the probability distribution of the limit points\n",
    "\n",
    "$$\n",
    "\\pi_\\infty(\\omega) = \\lim_{t \\rightarrow + \\infty} \\pi_t(\\omega).\n",
    "$$\n",
    "\n",
    "Evidently, since the likelihood ratio $ \\ell(w_t) $ differs from $ 1 $ when $ f \\neq g $,\n",
    "as we have assumed, the only possible fixed points of [(26.5)](#equation-eq-like44) are\n",
    "\n",
    "$$\n",
    "\\pi_\\infty(\\omega) =1\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\pi_\\infty(\\omega) =0\n",
    "$$\n",
    "\n",
    "Thus, for some realizations, $ \\lim_{\\rightarrow + \\infty} \\pi_t(\\omega) =1 $\n",
    "while for other realizations,  $ \\lim_{\\rightarrow + \\infty} \\pi_t(\\omega) =0 $.\n",
    "\n",
    "Now let’s remember that $ \\{\\pi_t\\}_{t=0}^\\infty $ is a martingale and apply the law of iterated expectations.\n",
    "\n",
    "The law of iterated expectations implies\n",
    "\n",
    "$$\n",
    "E_t \\pi_{t+j}  = \\pi_t\n",
    "$$\n",
    "\n",
    "and in particular\n",
    "\n",
    "$$\n",
    "E_{-1} \\pi_{t+j} = \\pi_{-1}.\n",
    "$$\n",
    "\n",
    "Applying the above formula to $ \\pi_\\infty $, we obtain\n",
    "\n",
    "$$\n",
    "E_{-1} \\pi_\\infty(\\omega) = \\pi_{-1}\n",
    "$$\n",
    "\n",
    "where the mathematical expectation $ E_{-1} $ here is taken with respect to the probability\n",
    "measure $ {\\textrm{Prob}(\\Omega)} $.\n",
    "\n",
    "Since the only two values that $ \\pi_\\infty(\\omega) $ can take are $ 1 $ and $ 0 $, we know that for some $ \\lambda \\in [0,1] $\n",
    "\n",
    "$$\n",
    "{\\textrm{Prob}}\\Bigl(\\pi_\\infty(\\omega) = 1\\Bigr) = \\lambda, \\quad {\\textrm{Prob}}\\Bigl(\\pi_\\infty(\\omega) = 0\\Bigr) = 1- \\lambda\n",
    "$$\n",
    "\n",
    "and consequently that\n",
    "\n",
    "$$\n",
    "E_{-1} \\pi_\\infty(\\omega) = \\lambda \\cdot 1 + (1-\\lambda) \\cdot 0 = \\lambda\n",
    "$$\n",
    "\n",
    "Combining this equation with equation (20), we deduce that\n",
    "the probability that $ {\\textrm{Prob}(\\Omega)} $ attaches to\n",
    "$ \\pi_\\infty(\\omega) $ being $ 1 $ must be $ \\pi_{-1} $.\n",
    "\n",
    "Thus, under the worker’s subjective distribution, $ \\pi_{-1} $ of the sample paths\n",
    "of $ \\{\\pi_t\\} $ will converge pointwise to $ 1 $ and $ 1 - \\pi_{-1} $ of the sample paths will\n",
    "converge pointwise to $ 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4d32a",
   "metadata": {},
   "source": [
    "### Some simulations\n",
    "\n",
    "Let’s watch the martingale convergence theorem at work in some simulations of our learning model under the worker’s subjective distribution.\n",
    "\n",
    "Let us simulate $ \\left\\{ \\pi_{t}\\right\\} _{t=0}^{T} $, $ \\left\\{ w_{t}\\right\\} _{t=0}^{T} $ paths where for each $ t\\geq0 $, $ w_t $ is drawn from the subjective distribution\n",
    "\n",
    "$$\n",
    "\\pi_{t-1}f\\left(w_{t}\\right)+\\left(1-\\pi_{t-1}\\right)g\\left(w_{t}\\right)\n",
    "$$\n",
    "\n",
    "We’ll plot a large sample of paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff188aa3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def martingale_simulate(π0, N=5000, T=200):\n",
    "\n",
    "    π_path = np.empty((N,T+1))\n",
    "    w_path = np.empty((N,T))\n",
    "    π_path[:,0] = π0\n",
    "\n",
    "    for n in range(N):\n",
    "        π = π0\n",
    "        for t in range(T):\n",
    "            # draw w\n",
    "            if np.random.rand() <= π:\n",
    "                w = np.random.beta(F_a, F_b)\n",
    "            else:\n",
    "                w = np.random.beta(G_a, G_b)\n",
    "            π = π*f(w)/g(w)/(π*f(w)/g(w) + 1 - π)\n",
    "            π_path[n,t+1] = π\n",
    "            w_path[n,t] = w\n",
    "\n",
    "    return π_path, w_path\n",
    "\n",
    "def fraction_0_1(π0, N, T, decimals):\n",
    "\n",
    "    π_path, w_path = martingale_simulate(π0, N=N, T=T)\n",
    "    values, counts = np.unique(np.round(π_path[:,-1], decimals=decimals), return_counts=True)\n",
    "    return values, counts\n",
    "\n",
    "def create_table(π0s, N=10000, T=500, decimals=2):\n",
    "\n",
    "    outcomes = []\n",
    "    for π0 in π0s:\n",
    "        values, counts = fraction_0_1(π0, N=N, T=T, decimals=decimals)\n",
    "        freq = counts/N\n",
    "        outcomes.append(dict(zip(values, freq)))\n",
    "    table = pd.DataFrame(outcomes).sort_index(axis=1).fillna(0)\n",
    "    table.index = π0s\n",
    "    return table\n",
    "\n",
    "# simulate\n",
    "T = 200\n",
    "π0 = .5\n",
    "\n",
    "π_path, w_path = martingale_simulate(π0=π0, T=T, N=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2923ac",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(100):\n",
    "    ax.plot(range(T+1), π_path[i, :])\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel(r'$\\pi_t$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f0fa8",
   "metadata": {},
   "source": [
    "The above graph indicates that\n",
    "\n",
    "- each of paths converges  \n",
    "- some of the paths converge to $ 1 $  \n",
    "- some of the paths converge to $ 0 $  \n",
    "- none of the paths converge to a limit point not equal to $ 0 $ or $ 1 $  \n",
    "\n",
    "\n",
    "Convergence actually occurs pretty fast, as the following graph of the cross-ensemble distribution of $ \\pi_t $ for various small $ t $’s indicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a3e18",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for t in [1, 10, T-1]:\n",
    "    ax.hist(π_path[:,t], bins=20, alpha=0.4, label=f'T={t}')\n",
    "\n",
    "ax.set_ylabel('count')\n",
    "ax.set_xlabel(r'$\\pi_T$')\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01241801",
   "metadata": {},
   "source": [
    "Evidently, by $ t = 199 $, $ \\pi_t $ has converged to either $ 0 $ or $ 1 $.\n",
    "\n",
    "The fraction of paths that have converged to $ 1 $ is $ .5 $\n",
    "\n",
    "The fractions of paths that have converged to $ 0 $ is also $ .5 $.\n",
    "\n",
    "Does the fraction $ .5 $ ring a bell?\n",
    "\n",
    "Yes, it does: it equals the value of $ \\pi_0 = .5 $ that we used to generate each sequence\n",
    "in the ensemble.\n",
    "\n",
    "So let’s change $ \\pi_0 $ to $ .3 $ and watch what happens to the distribution of the ensemble of\n",
    "$ \\pi_t $’s for various $ t $’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e2134",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# simulate\n",
    "T = 200\n",
    "π0 = .3\n",
    "\n",
    "π_path3, w_path3 = martingale_simulate(π0=π0, T=T, N=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74998133",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for t in [1, 10, T-1]:\n",
    "    ax.hist(π_path3[:,t], bins=20, alpha=0.4, label=f'T={t}')\n",
    "\n",
    "ax.set_ylabel('count')\n",
    "ax.set_xlabel(r'$\\pi_T$')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d01510",
   "metadata": {},
   "source": [
    "For the preceding ensemble that assumed $ \\pi_0 = .5 $, the following graph shows two  paths of\n",
    "$ w_t $’s and the $ \\pi_t $ sequences that gave rise to them.\n",
    "\n",
    "Notice that one of the paths involves systematically higher $ w_t $’s, outcomes that push $ \\pi_t $ upward.\n",
    "\n",
    "The luck of the draw early in a simulation push the subjective distribution to draw from\n",
    "$ F $ more frequently along a sample path, and this pushes $ \\pi_t $ toward $ 0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574c032",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i, j in enumerate([10, 100]):\n",
    "    ax.plot(range(T+1), π_path[j,:], color=colors[i], label=fr'$\\pi$_path, {j}-th simulation')\n",
    "    ax.plot(range(1,T+1), w_path[j,:], color=colors[i], label=fr'$w$_path, {j}-th simulation', alpha=0.3)\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel(r'$\\pi_t$')\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"$w_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c6eb0",
   "metadata": {},
   "source": [
    "## Initial Prior is Verified by Paths Drawn from Subjective Conditional Densities\n",
    "\n",
    "Now let’s use our Python code to generate a table that checks out our earlier claims about the\n",
    "probability distribution of the pointwise limits $ \\pi_{\\infty}(\\omega) $.\n",
    "\n",
    "We’ll use our simulations to generate a histogram of this distribution.\n",
    "\n",
    "In the following table, the left column in bold face reports an assumed value of $ \\pi_{-1} $.\n",
    "\n",
    "The second column reports the fraction of $ N = 10000 $ simulations for which $ \\pi_{t} $  had converged to $ 0 $  at the terminal date $ T=500 $ for each simulation.\n",
    "\n",
    "The third column reports the fraction of $ N = 10000 $ simulations for which $ \\pi_{t} $  had converged to $ 1 $ as the terminal date $ T=500 $ for each simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb784d2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# create table\n",
    "table = create_table(list(np.linspace(0,1,11)), N=10000, T=500)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5a645",
   "metadata": {},
   "source": [
    "The fraction of simulations for which $ \\pi_{t} $  had converged to $ 1 $ is indeed always  close  to $ \\pi_{-1} $, as anticipated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f4a85",
   "metadata": {},
   "source": [
    "## Drilling Down a Little Bit\n",
    "\n",
    "To understand how the local dynamics of $ \\pi_t $ behaves, it is enlightening to consult the  variance of $ \\pi_{t} $ conditional on $ \\pi_{t-1} $.\n",
    "\n",
    "Under the subjective distribution this conditional variance is defined as\n",
    "\n",
    "$$\n",
    "\\sigma^2(\\pi_t | \\pi_{t-1})  = \\int \\Bigl[  { \\pi_{t-1} f(w) \\over \\pi_{t-1} f(w) + (1-\\pi_{t-1})g(w)  } - \\pi_{t-1} \\Bigr]^2\n",
    " \\Bigl[ \\pi_{t-1} f(w) + (1-\\pi_{t-1})g(w) \\Bigr]  d w\n",
    "$$\n",
    "\n",
    "We can use  a Monte Carlo simulation to approximate this conditional variance.\n",
    "\n",
    "We approximate it for  a grid of points $ \\pi_{t-1} \\in [0,1] $.\n",
    "\n",
    "Then we’ll plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be443063",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_cond_var(pi, mc_size=int(1e6)):\n",
    "    # create monte carlo draws\n",
    "    mc_draws = np.zeros(mc_size)\n",
    "\n",
    "    for i in prange(mc_size):\n",
    "        if np.random.rand() <= pi:\n",
    "            mc_draws[i] = np.random.beta(F_a, F_b)\n",
    "        else:\n",
    "            mc_draws[i] = np.random.beta(G_a, G_b)\n",
    "\n",
    "    dev = pi*f(mc_draws)/(pi*f(mc_draws) + (1-pi)*g(mc_draws)) - pi\n",
    "    return np.mean(dev**2)\n",
    "\n",
    "pi_array = np.linspace(0, 1, 40)\n",
    "cond_var_array = []\n",
    "\n",
    "for pi in pi_array:\n",
    "    cond_var_array.append(compute_cond_var(pi))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(pi_array, cond_var_array)\n",
    "ax.set_xlabel(r'$\\pi_{t-1}$')\n",
    "ax.set_ylabel(r'$\\sigma^{2}(\\pi_{t}\\vert \\pi_{t-1})$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eebb43",
   "metadata": {},
   "source": [
    "The shape of the the conditional variance as a function of $ \\pi_{t-1} $ is informative about the behavior of sample paths of $ \\{\\pi_t\\} $.\n",
    "\n",
    "Notice how the conditional variance approaches $ 0 $ for $ \\pi_{t-1} $ near  either $ 0 $ or $ 1 $.\n",
    "\n",
    "The conditional variance is nearly zero only when the agent  is almost sure that $ w_t $ is drawn from $ F $,  or is almost sure it is drawn from $ G $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a81317",
   "metadata": {},
   "source": [
    "## Related Lectures\n",
    "\n",
    "This lecture has been devoted to building some useful infrastructure that will help us understand inferences that are the foundations of\n",
    "results described  in [this lecture](https://python.quantecon.org/odu.html) and [this lecture](https://python.quantecon.org/wald_friedman.html) and [this lecture](https://python.quantecon.org/navy_captain.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23349c89",
   "metadata": {},
   "source": [
    "## Exercise 26.2\n",
    "\n",
    "In the [first exercise](#likelihood_bayes_ex1), we implemented a Bayesian learning algorithm to estimate the mixing parameter $ x $ in a mixture model using a grid approximation method.\n",
    "\n",
    "In this exercise, we will explore sequential Bayesian updating using NumPyro.\n",
    "\n",
    "Please follow these steps:\n",
    "\n",
    "1. Generate a dataset from a mixture model with known mixing parameter $ x_{true} = 0.5 $.  \n",
    "1. Process the data in chunks of 100 observations, updating the posterior sequentially.  \n",
    "1. For each chunk, use the posterior from the previous chunk as the prior for the next chunk (using moment matching to fit a Beta distribution).  \n",
    "1. Create a visualization showing how the posterior distribution evolves, using gradually darker shades to represent later time periods.  \n",
    "\n",
    "\n",
    "In the exercise, set $ \\alpha_0 = 1 $ and $ \\beta_0 = 2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60943c2",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 26.2](https://python.quantecon.org/#likelihood_bayes_ex2)\n",
    "\n",
    "First, let’s install and import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b7a8d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install numpyro jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bee56a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c617e",
   "metadata": {},
   "source": [
    "Define the mixture model and helper functions that helps us\n",
    "to fit a Beta distribution to the posterior and obtain the parameter for the next chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84030e1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def mixture_model(w, α0=1.0, β0=1.0):\n",
    "    x = numpyro.sample(\"x\", dist.Beta(α0, β0))\n",
    "    f_dist = dist.Beta(F_a, F_b)\n",
    "    g_dist = dist.Beta(G_a, G_b)\n",
    "    mix = dist.Mixture(\n",
    "        dist.Categorical(probs=jnp.array([x, 1 - x])),\n",
    "        [f_dist, g_dist]\n",
    "    )\n",
    "    with numpyro.plate(\"data\", w.shape[0]):\n",
    "        numpyro.sample(\"w\", mix, obs=w)\n",
    "\n",
    "def β_moment_match(samples, eps=1e-12):\n",
    "    m = float(samples.mean())\n",
    "    v = float(samples.var())\n",
    "    v = max(v, eps)\n",
    "    t = m * (1 - m) / v - 1.0\n",
    "    α = max(m * t, eps)\n",
    "    β = max((1 - m) * t, eps)\n",
    "    return α, β"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68945c4d",
   "metadata": {},
   "source": [
    "Now we implement sequential Bayesian updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53f63a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sequential_update(w_all, α0=1.0, β0=1.0, \n",
    "                     chunk_size=100, \n",
    "                     num_warmup=1000, \n",
    "                     num_samples=2000, \n",
    "                     seed=0):\n",
    "    n = len(w_all)\n",
    "\n",
    "    # Create chunks\n",
    "    chunks = [slice(i, min(i + chunk_size, n)) \n",
    "                    for i in range(0, n, chunk_size)]\n",
    "    α, β = α0, β0\n",
    "    \n",
    "    keys = jax.random.split(\n",
    "        jax.random.PRNGKey(seed), len(chunks))\n",
    "    means = [α / (α + β)]\n",
    "    posts = []\n",
    "    \n",
    "    # Run MCMC for each chunk\n",
    "    for i, sl in enumerate(chunks):\n",
    "        kernel = NUTS(mixture_model)\n",
    "        mcmc = MCMC(kernel, \n",
    "                    num_warmup=num_warmup, \n",
    "                    num_samples=num_samples)\n",
    "        mcmc.run(keys[i], w_all[sl], α, β)\n",
    "        xs = mcmc.get_samples()[\"x\"]\n",
    "        \n",
    "        posts.append(xs)\n",
    "\n",
    "        # Posterior becomes prior for next chunk\n",
    "        α, β = β_moment_match(xs)\n",
    "        means.append(xs.mean())\n",
    "    \n",
    "    return np.array(means), posts, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cb427",
   "metadata": {},
   "source": [
    "Generate data and run sequential updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1b544",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_true = 0.5\n",
    "T_total = 2000\n",
    "\n",
    "set_seed()\n",
    "w_mix = simulate_mixture_path(x_true, T_total)\n",
    "\n",
    "means, posts, chunks = sequential_update(\n",
    "    w_mix, α0=1.0, β0=2.0, chunk_size=200,\n",
    "    num_warmup=2000, num_samples=1000, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137782a3",
   "metadata": {},
   "source": [
    "Create visualization with gradually darker lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c235528",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot posterior means over time\n",
    "n_seen = np.cumsum([0] + [c.stop - c.start for c in chunks])\n",
    "\n",
    "# Posterior mean trajectory\n",
    "ax1.plot(n_seen, means, 'o-', \n",
    "    color='darkblue', label='Posterior mean of $x$')\n",
    "ax1.axhline(x_true, ls=\"--\", \n",
    "    color=\"red\", alpha=0.7, label=f'True $x$ = {x_true}')\n",
    "ax1.set_xlabel('$t$')\n",
    "ax1.set_ylabel('Posterior mean of $x$')\n",
    "ax1.legend()    \n",
    "\n",
    "# Posterior densities at each chunk\n",
    "n_chunks = len(posts)\n",
    "colors = plt.cm.Blues(np.linspace(0.01, 0.99, n_chunks))\n",
    "\n",
    "for i, (xs, color) in enumerate(zip(posts, colors)):\n",
    "    sns.kdeplot(xs, color=color, ax=ax2,\n",
    "                alpha=0.7, label=f'n={chunks[i].stop}')\n",
    "\n",
    "ax2.axvline(x_true, ls=\"--\", color=\"red\", \n",
    "            alpha=0.7, label=f'True $x$ = {x_true}')\n",
    "ax2.set_xlabel('$x$')\n",
    "ax2.set_ylabel('density')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81a703",
   "metadata": {},
   "source": [
    "The left panel shows how the posterior mean converges to the true value as more data is observed.\n",
    "\n",
    "The right panel shows that the distribution of $ x $ becomes more concentrated around the true value with more observations."
   ]
  }
 ],
 "metadata": {
  "date": 1753756053.1932359,
  "filename": "likelihood_bayes.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Likelihood Ratio Processes and Bayesian Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}