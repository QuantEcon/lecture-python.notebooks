{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e618ed50",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c668f",
   "metadata": {},
   "source": [
    "\n",
    "<a id='doubts-or-variability'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ea7b0",
   "metadata": {},
   "source": [
    "# Doubts or Variability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5451efc",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Doubts or Variability?](#Doubts-or-Variability?)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [The equity premium and risk-free rate puzzles](#The-equity-premium-and-risk-free-rate-puzzles)  \n",
    "  - [The choice setting](#The-choice-setting)  \n",
    "  - [Preferences, distortions, and detection](#Preferences,-distortions,-and-detection)  \n",
    "  - [Detection-error probabilities](#Detection-error-probabilities)  \n",
    "  - [Unify the two models using detection-error probabilities](#Unify-the-two-models-using-detection-error-probabilities)  \n",
    "  - [What do risk premia measure?](#What-do-risk-premia-measure?)  \n",
    "  - [Visualizing the welfare decomposition](#Visualizing-the-welfare-decomposition)  \n",
    "  - [Welfare gains from removing model uncertainty](#Welfare-gains-from-removing-model-uncertainty)  \n",
    "  - [Learning doesn’t eliminate misspecification fears](#Learning-doesn’t-eliminate-misspecification-fears)  \n",
    "  - [Concluding remarks](#Concluding-remarks)  \n",
    "  - [Exercises](#Exercises)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0d92b",
   "metadata": {},
   "source": [
    "> No one has found risk aversion parameters of 50 or 100 in the diversification of\n",
    "individual portfolios, in the level of insurance deductibles, in the wage premiums\n",
    "associated with occupations with high earnings risk, or in the revenues raised by\n",
    "state-operated lotteries. It\n",
    "would be good to have the equity premium resolved, but I think we need to look beyond high\n",
    "estimates of risk aversion to do it. – Robert E. Lucas Jr., [[Lucas, 2003](https://python.quantecon.org/zreferences.html#id242)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a920fc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Tallarini [[2000](https://python.quantecon.org/zreferences.html#id132)] showed that a recursive preference specification could match the equity premium and the risk-free rate puzzle simultaneously.\n",
    "\n",
    "But matching required setting the risk-aversion coefficient $ \\gamma $ to around 50 for a random-walk consumption model and around 75 for a trend-stationary model, exactly the range that provoked the  skepticism in the above quote from Lucas [[2003](https://python.quantecon.org/zreferences.html#id242)].\n",
    "\n",
    "Barillas *et al.* [[2009](https://python.quantecon.org/zreferences.html#id50)] ask whether those large $ \\gamma $ values really measure aversion to atemporal risk, or whether they instead measure the agent’s doubts about the underlying probability model.\n",
    "\n",
    "Their answer, and the theme of this lecture, is that much of what looks like “risk aversion” can be reinterpreted as **model uncertainty**.\n",
    "\n",
    "The same recursion that defines Tallarini’s risk-sensitive agent is observationally equivalent to a another recursion that expresses an agent’s concern  that the probability model governing consumption growth may be wrong.\n",
    "\n",
    "Under this reading, a  parameter value  that indicates  extreme risk aversion in one interpretation of the recursion  indicates concerns about *misspecification* in another interpretation of the same recursion.\n",
    "\n",
    "Barillas *et al.* [[2009](https://python.quantecon.org/zreferences.html#id50)] show that modest amounts of model uncertainty can substitute for large amounts of risk aversion in terms of choices and effects on asset prices.\n",
    "\n",
    "This reinterpretation changes the welfare question that asset prices answer.\n",
    "\n",
    "Do large risk premia measure the benefits from reducing well-understood aggregate fluctuations, or do they measure  benefits from reducing doubts about the  model describing consumption growth?\n",
    "\n",
    "We begin with a Hansen and Jagannathan [[1991](https://python.quantecon.org/zreferences.html#id133)] bound, then specify the statistical environment, lay out four related preference specifications and the connections among them, and finally revisit Tallarini’s calibration through the lens of detection-error probabilities.\n",
    "\n",
    "Along the way, we draw on ideas and techniques from\n",
    "\n",
    "- [Asset Pricing: Finite State Models](https://python.quantecon.org/markov_asset.html#mass), where we introduce stochastic discount factors, and  \n",
    "- [Likelihood Ratio Processes](https://python.quantecon.org/likelihood_ratio_process.html#likelihood-ratio-process), where we develop the likelihood-ratio machinery that reappears here as the worst-case distortion $ \\hat g $.  \n",
    "\n",
    "\n",
    "In addition to what’s in Anaconda, this lecture will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f4bb2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886cbfe3",
   "metadata": {},
   "source": [
    "We use the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdae6b2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data as web\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19834246",
   "metadata": {},
   "source": [
    "We also set up calibration inputs and compute the covariance matrix of equity and risk-free returns from reported moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d452384",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "β = 0.995\n",
    "T = 235  \n",
    "\n",
    "# Table 2 parameters\n",
    "rw = dict(μ=0.00495, σ_ε=0.0050)\n",
    "ts = dict(μ=0.00418, σ_ε=0.0050, ρ=0.980, ζ=-4.48)\n",
    "\n",
    "# Table 1 moments, converted from percent to decimals\n",
    "r_e_mean, r_e_std = 0.0227, 0.0768\n",
    "r_f_mean, r_f_std = 0.0032, 0.0061\n",
    "r_excess_std = 0.0767\n",
    "\n",
    "R_mean = np.array([1.0 + r_e_mean, 1.0 + r_f_mean])  \n",
    "cov_erf = (r_e_std**2 + r_f_std**2 - r_excess_std**2) / 2.0\n",
    "Σ_R = np.array(\n",
    "    [\n",
    "        [r_e_std**2, cov_erf],\n",
    "        [cov_erf, r_f_std**2],\n",
    "    ]\n",
    ")\n",
    "Σ_R_inv = np.linalg.inv(Σ_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bea603",
   "metadata": {},
   "source": [
    "## The equity premium and risk-free rate puzzles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40016161",
   "metadata": {},
   "source": [
    "### Pricing kernel and the risk-free rate\n",
    "\n",
    "Let’s briefly review a few key concepts from [Asset Pricing: Finite State Models](https://python.quantecon.org/markov_asset.html#mass).\n",
    "\n",
    "A random variable $ m_{t+1} $ is called a **stochastic discount factor** if, for a one-period payoff $ y_{t+1} $ with time-$ t $ price $ p_t $, it satisfies\n",
    "\n",
    "\n",
    "<a id='equation-bhs-pricing-eq'></a>\n",
    "$$\n",
    "p_t = E_t(m_{t+1}  y_{t+1}), \\tag{84.1}\n",
    "$$\n",
    "\n",
    "where $ E_t $ denotes the mathematical expectation conditioned on date-$ t $ information.\n",
    "\n",
    "For time-separable CRRA preferences with discount factor $ \\beta $ and coefficient of relative risk aversion $ \\gamma $, the marginal rate of substitution gives\n",
    "\n",
    "\n",
    "<a id='equation-bhs-crra-sdf'></a>\n",
    "$$\n",
    "m_{t+1} = \\beta \\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\gamma}, \\tag{84.2}\n",
    "$$\n",
    "\n",
    "where $ C_t $ is consumption at time $ t $.\n",
    "\n",
    "Setting $ y_{t+1} = 1 $ (a risk-free bond) in [(84.1)](#equation-bhs-pricing-eq) yields the reciprocal of the gross one-period risk-free rate:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-riskfree'></a>\n",
    "$$\n",
    "\\frac{1}{R_t^f} = E_t[m_{t+1}] = E_t \\left[\\beta\\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\gamma}\\right]. \\tag{84.3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e08025",
   "metadata": {},
   "source": [
    "### The Hansen–Jagannathan bound\n",
    "\n",
    "Let $ R_{t+1}^e $ denote the gross return on a risky asset (e.g., the market portfolio) and $ R_{t+1}^f $ the gross return on a one-period risk-free bond.\n",
    "\n",
    "The **excess return** is\n",
    "\n",
    "$$\n",
    "\\xi_{t+1} = R_{t+1}^e - R_{t+1}^f.\n",
    "$$\n",
    "\n",
    "An excess return is the payoff on a zero-cost portfolio that is long one dollar of the risky asset and short one dollar of the risk-free bond.\n",
    "\n",
    "Because the portfolio costs nothing to enter, its price is $ p_t = 0 $, so [(84.1)](#equation-bhs-pricing-eq) implies\n",
    "\n",
    "$$\n",
    "0 = E_t[m_{t+1} \\xi_{t+1}].\n",
    "$$\n",
    "\n",
    "We can decompose the expectation of a product into a covariance plus a product of expectations:\n",
    "\n",
    "$$\n",
    "E_t[m_{t+1} \\xi_{t+1}]\n",
    "=\n",
    "\\operatorname{cov}_t(m_{t+1},\\xi_{t+1}) + E_t[m_{t+1}] E_t[\\xi_{t+1}],\n",
    "$$\n",
    "\n",
    "where $ \\operatorname{cov}_t $ denotes the conditional covariance and $ \\sigma_t $ will denote the conditional standard deviation.\n",
    "\n",
    "Setting the left-hand side to zero and solving for the expected excess return gives\n",
    "\n",
    "$$\n",
    "E_t[\\xi_{t+1}] = -\\frac{\\operatorname{cov}_t(m_{t+1}, \\xi_{t+1})}{E_t[m_{t+1}]}.\n",
    "$$\n",
    "\n",
    "Taking absolute values and applying the **Cauchy–Schwarz inequality** $ |\\operatorname{cov}(X,Y)| \\leq \\sigma(X) \\sigma(Y) $ yields\n",
    "\n",
    "\n",
    "<a id='equation-bhs-hj-bound'></a>\n",
    "$$\n",
    "\\frac{|E_t[\\xi_{t+1}]|}{\\sigma_t(\\xi_{t+1})}\n",
    "\\leq\n",
    "\\frac{\\sigma_t(m_{t+1})}{E_t[m_{t+1}]}. \\tag{84.4}\n",
    "$$\n",
    "\n",
    "The left-hand side of [(84.4)](#equation-bhs-hj-bound) is the **Sharpe ratio**: the expected excess return per unit of return volatility.\n",
    "\n",
    "The right-hand side, $ \\sigma_t(m)/E_t(m) $, is the **market price of risk**: the maximum Sharpe ratio attainable in the market.\n",
    "\n",
    "In words, no asset’s Sharpe ratio can exceed the market price of risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e58e3",
   "metadata": {},
   "source": [
    "#### Unconditional version\n",
    "\n",
    "The bound [(84.4)](#equation-bhs-hj-bound) is stated in conditional terms.\n",
    "\n",
    "There is  an unconditional counterpart that involves a vector of $ n $ gross returns $ R_{t+1} $ (e.g., equity and risk-free) with unconditional mean $ E(R) $ and covariance matrix $ \\Sigma_R $:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-hj-unconditional'></a>\n",
    "$$\n",
    "\\sigma(m)\n",
    "\\geq\n",
    "\\sqrt{b^\\top \\Sigma_R^{-1} b},\n",
    "\\qquad\n",
    "b = \\mathbf{1} - E(m) E(R). \\tag{84.5}\n",
    "$$\n",
    "\n",
    "[Exercise 1](#dov_ex1) walks through a derivation of this unconditional bound.\n",
    "\n",
    "The function below computes the right-hand side of [(84.5)](#equation-bhs-hj-unconditional) for any given value of $ E(m) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b1f91",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def hj_std_bound(E_m):\n",
    "    b = np.ones(2) - E_m * R_mean\n",
    "    var_lb = b @ Σ_R_inv @ b\n",
    "    return np.sqrt(np.maximum(var_lb, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a830f2",
   "metadata": {},
   "source": [
    "### Two puzzles\n",
    "\n",
    "Reconciling formula [(84.2)](#equation-bhs-crra-sdf) with the market price of risk extracted from data on asset returns (like those in Table 1 below) requires a value of $ \\gamma $ so high that it provokes skepticism.\n",
    "\n",
    "This is the **equity premium puzzle**.\n",
    "\n",
    "But high values of $ \\gamma $  bring another difficulty.\n",
    "\n",
    "High values of $ \\gamma $ that deliver enough volatility $ \\sigma(m) $ also push $ E(m) $, the reciprocal of the gross risk-free rate, too far down, away from the Hansen–Jagannathan bound.\n",
    "\n",
    "This is the **risk-free rate puzzle** of Weil [[1989](https://python.quantecon.org/zreferences.html#id134)].\n",
    "\n",
    "Tallarini [[2000](https://python.quantecon.org/zreferences.html#id132)] showed that recursive preferences with IES $ = 1 $ can clear the HJ bar while avoiding the risk-free rate puzzle.\n",
    "\n",
    "The figure below reproduces Tallarini’s key diagnostic.\n",
    "\n",
    "Because it motivates much of what follow, we show Tallarini’s figure  before developing the underlying theory.\n",
    "\n",
    "Closed-form expressions for the Epstein–Zin SDF moments used in the plot are derived in [Exercise 2](#dov_ex2).\n",
    "\n",
    "The code below implements them alongside the corresponding CRRA moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9120977",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def moments_type1_rw(γ):\n",
    "    μ, σ = rw[\"μ\"], rw[\"σ_ε\"]\n",
    "    E_m = β * np.exp(-μ + 0.5 * σ**2 * (2.0 * γ - 1.0))\n",
    "    var_log_m = (σ * γ) ** 2\n",
    "    mpr = np.sqrt(np.exp(var_log_m) - 1.0)\n",
    "    return E_m, mpr\n",
    "\n",
    "\n",
    "def moments_type1_ts(γ):\n",
    "    μ, σ, ρ = ts[\"μ\"], ts[\"σ_ε\"], ts[\"ρ\"]\n",
    "    mean_term = 1.0 - (2.0 * (1.0 - β) * (1.0 - γ)) / (1.0 - β * ρ) \\\n",
    "                + (1.0 - ρ) / (1.0 + ρ)\n",
    "    E_m = β * np.exp(-μ + 0.5 * σ**2 * mean_term)\n",
    "    var_term = (((1.0 - β) * (1.0 - γ)) / (1.0 - β * ρ) - 1.0) ** 2 \\\n",
    "                + (1.0 - ρ) / (1.0 + ρ)\n",
    "    var_log_m = σ**2 * var_term\n",
    "    mpr = np.sqrt(np.exp(var_log_m) - 1.0)\n",
    "    return E_m, mpr\n",
    "\n",
    "\n",
    "def moments_crra_rw(γ):\n",
    "    μ, σ = rw[\"μ\"], rw[\"σ_ε\"]\n",
    "    var_log_m = (γ * σ) ** 2\n",
    "    mean_log_m = np.log(β) - γ * μ\n",
    "    E_m = np.exp(mean_log_m + 0.5 * var_log_m)\n",
    "    mpr = np.sqrt(np.exp(var_log_m) - 1.0)\n",
    "    return E_m, mpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70482317",
   "metadata": {},
   "source": [
    "For each value of $ \\gamma \\in \\{1, 5, 10, \\ldots, 51\\} $, we plot the implied $ (E(m),\\sigma(m)) $ pair for three combinations of specifications of preferences and consumption growth processes.\n",
    "\n",
    "These are time-separable CRRA (crosses), Epstein–Zin preferences with random-walk consumption (circles), and Epstein–Zin preferences with trend-stationary consumption (pluses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46516326",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "γ_grid = np.arange(1, 55, 5)\n",
    "\n",
    "Em_rw = np.array([moments_type1_rw(γ)[0] for γ in γ_grid])\n",
    "σ_m_rw = np.array(\n",
    "    [moments_type1_rw(γ)[0] * moments_type1_rw(γ)[1] for γ in γ_grid])\n",
    "\n",
    "Em_ts = np.array([moments_type1_ts(γ)[0] for γ in γ_grid])\n",
    "σ_m_ts = np.array(\n",
    "    [moments_type1_ts(γ)[0] * moments_type1_ts(γ)[1] for γ in γ_grid])\n",
    "\n",
    "Em_crra = np.array([moments_crra_rw(γ)[0] for γ in γ_grid])\n",
    "σ_m_crra = np.array(\n",
    "    [moments_crra_rw(γ)[0] * moments_crra_rw(γ)[1] for γ in γ_grid])\n",
    "\n",
    "Em_grid = np.linspace(0.8, 1.01, 1000)\n",
    "HJ_std = np.array([hj_std_bound(x) for x in Em_grid])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(Em_grid, HJ_std, lw=2, color=\"black\",\n",
    "                            label=\"Hansen-Jagannathan bound\")\n",
    "ax.plot(Em_rw, σ_m_rw, \"o\", lw=2,\n",
    "                            label=\"Epstein-Zin, random walk\")\n",
    "ax.plot(Em_ts, σ_m_ts, \"+\", lw=2,\n",
    "                            label=\"Epstein-Zin, trend stationary\")\n",
    "ax.plot(Em_crra, σ_m_crra, \"x\", lw=2,\n",
    "                            label=\"time-separable CRRA\")\n",
    "\n",
    "ax.set_xlabel(r\"$E(m)$\")\n",
    "ax.set_ylabel(r\"$\\sigma(m)$\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlim(0.8, 1.01)\n",
    "ax.set_ylim(0.0, 0.42)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1560a7",
   "metadata": {},
   "source": [
    "The crosses tell the story of the risk-free-rate puzzle (Weil [[1989](https://python.quantecon.org/zreferences.html#id134)]).\n",
    "\n",
    "As $ \\gamma $ rises, $ \\sigma(m)/E(m) $ grows but $ E(m) $ drifts well below the range consistent with the observed risk-free rate.\n",
    "\n",
    "The circles and pluses show Tallarini’s way out.\n",
    "\n",
    "Recursive utility with IES $ = 1 $ pushes volatility upward while keeping $ E(m) $ roughly pinned near $ 1/(1+r^f) $.\n",
    "\n",
    "For the random-walk model, the bound is reached at around $ \\gamma = 50 $.\n",
    "\n",
    "For the trend-stationary model, it is reached at around $ \\gamma = 75 $.\n",
    "\n",
    "The quantitative achievement is impressive, but Lucas’s challenge still stands.\n",
    "\n",
    "Where is the microeconomic evidence for $ \\gamma = 50 $?\n",
    "\n",
    "Barillas *et al.* [[2009](https://python.quantecon.org/zreferences.html#id50)] argue that these large $ \\gamma $ values are not really about risk aversion.\n",
    "\n",
    "Instead, they reflect the agent’s doubts about the probability model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2442c",
   "metadata": {},
   "source": [
    "## The choice setting\n",
    "\n",
    "To understand  their reinterpretation, we first need to describe their statistical models of consumption growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2364631",
   "metadata": {},
   "source": [
    "### Shocks and consumption plans\n",
    "\n",
    "We work with a general class of consumption plans.\n",
    "\n",
    "Let $ x_t $ be an $ n \\times 1 $ state vector and $ \\varepsilon_{t+1} $ an $ m \\times 1 $ shock.\n",
    "\n",
    "A consumption plan belongs to the set $ \\mathcal{C}(A, B, H; x_0) $ if it admits the recursive representation\n",
    "\n",
    "\n",
    "<a id='equation-bhs-state-space'></a>\n",
    "$$\n",
    "x_{t+1} = A x_t + B \\varepsilon_{t+1},\n",
    "\\qquad\n",
    "c_t = H x_t, \\tag{84.6}\n",
    "$$\n",
    "\n",
    "where the eigenvalues of $ A $ are bounded in modulus by $ 1/\\sqrt{\\beta} $.\n",
    "\n",
    "The time-$ t $ consumption can therefore be written as\n",
    "\n",
    "$$\n",
    "c_t = H \\left(B\\varepsilon_t + AB\\varepsilon_{t-1} + \\cdots + A^{t-1}B\\varepsilon_1\\right) + HA^t x_0.\n",
    "$$\n",
    "\n",
    "The equivalence theorems and Bellman equations that follow hold for arbitrary plans in $ \\mathcal{C}(A,B,H;x_0) $.\n",
    "\n",
    "We focus on the random-walk and trend-stationary models as two special cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987915e8",
   "metadata": {},
   "source": [
    "### Consumption dynamics\n",
    "\n",
    "Let $ c_t = \\log C_t $ be log consumption.\n",
    "\n",
    "The *geometric-random-walk* specification is\n",
    "\n",
    "$$\n",
    "c_{t+1} = c_t + \\mu + \\sigma_\\varepsilon \\varepsilon_{t+1}, \\qquad \\varepsilon_{t+1} \\sim \\mathcal{N}(0, 1).\n",
    "$$\n",
    "\n",
    "Iterating forward yields\n",
    "\n",
    "$$\n",
    "c_t = c_0 + t\\mu + \\sigma_\\varepsilon(\\varepsilon_t + \\varepsilon_{t-1} + \\cdots + \\varepsilon_1),\n",
    "\\qquad\n",
    "t \\geq 1.\n",
    "$$\n",
    "\n",
    "The *geometric-trend-stationary* specification can be written as a deterministic trend plus a stationary AR(1) component:\n",
    "\n",
    "$$\n",
    "c_t = \\zeta + \\mu t + z_t,\n",
    "\\qquad\n",
    "z_{t+1} = \\rho z_t + \\sigma_\\varepsilon \\varepsilon_{t+1},\n",
    "\\qquad\n",
    "\\varepsilon_{t+1} \\sim \\mathcal{N}(0, 1).\n",
    "$$\n",
    "\n",
    "With $ z_0 = c_0 - \\zeta $, this implies the representation\n",
    "\n",
    "$$\n",
    "c_t\n",
    "=\n",
    "\\rho^t c_0 + \\mu t + (1-\\rho^t)\\zeta\n",
    "+\n",
    "\\sigma_\\varepsilon(\\varepsilon_t + \\rho \\varepsilon_{t-1} + \\cdots + \\rho^{t-1}\\varepsilon_1),\n",
    "\\qquad\n",
    "t \\geq 1.\n",
    "$$\n",
    "\n",
    "Equivalently, defining the detrended series $ \\tilde c_t := c_t - \\mu t $,\n",
    "\n",
    "$$\n",
    "\\tilde c_{t+1} - \\zeta = \\rho(\\tilde c_t - \\zeta) + \\sigma_\\varepsilon \\varepsilon_{t+1}.\n",
    "$$\n",
    "\n",
    "The estimated parameters are $ (\\mu, \\sigma_\\varepsilon) $ for the random walk and $ (\\mu, \\sigma_\\varepsilon, \\rho, \\zeta) $ for the trend-stationary case.\n",
    "\n",
    "We record these parameters and moments from the paper’s tables for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d39171",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(\"Table 2 parameters\")\n",
    "print(f\"random walk: μ={rw['μ']:.5f}, σ_ε={rw['σ_ε']:.5f}\")\n",
    "print(\n",
    "    f\"trend stationary: μ={ts['μ']:.5f}, σ_ε={ts['σ_ε']:.5f}, \"\n",
    "    f\"ρ={ts['ρ']:.3f}, ζ={ts['ζ']:.2f}\"\n",
    ")\n",
    "print()\n",
    "print(\"Table 1 moments\")\n",
    "print(f\"E[r_e]={r_e_mean:.4f}, std[r_e]={r_e_std:.4f}\")\n",
    "print(f\"E[r_f]={r_f_mean:.4f}, std[r_f]={r_f_std:.4f}\")\n",
    "print(f\"std[r_e-r_f]={r_excess_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ea1e0",
   "metadata": {},
   "source": [
    "\n",
    "<a id='pref-equiv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17ac44",
   "metadata": {},
   "source": [
    "## Preferences, distortions, and detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ec6d7",
   "metadata": {},
   "source": [
    "### Overview of agents I, II, III, and IV\n",
    "\n",
    "We compare four preference specifications over consumption plans $ C^\\infty \\in \\mathcal{C} $.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For  origins of the names **multipler** and **constraint**  preferences, see Hansen and Sargent [[2001](https://python.quantecon.org/zreferences.html#id90)].\n",
    "The risk-sensitive preference specification used here comes from Hansen and Sargent [[1995](https://python.quantecon.org/zreferences.html#id131)], which adjusts specifications used earlier by\n",
    "Jacobson [[1973](https://python.quantecon.org/zreferences.html#id130)], Whittle [[1981](https://python.quantecon.org/zreferences.html#id55)], and  Whittle [[1990](https://python.quantecon.org/zreferences.html#id54)] to accommodate discounting in a way that preserves time-invariant optimal decision rules.\n",
    "\n",
    "*Type I agent (Kreps–Porteus–Epstein–Zin–Tallarini)* with\n",
    "\n",
    "- a discount factor $ \\beta \\in (0,1) $;  \n",
    "- an intertemporal elasticity of substitution fixed at $ 1 $;  \n",
    "- a risk-aversion parameter $ \\gamma \\geq 1 $; and  \n",
    "- an approximating conditional density $ \\pi(\\cdot) $ for shocks and its implied joint distribution $ \\Pi_\\infty(\\cdot \\mid x_0) $.  \n",
    "\n",
    "\n",
    "*Type II agent (multiplier preferences)* with\n",
    "\n",
    "- $ \\beta \\in (0,1) $;  \n",
    "- IES $ =1 $;  \n",
    "- unit risk aversion;  \n",
    "- an approximating model $ \\Pi_\\infty(\\cdot \\mid x_0) $; and  \n",
    "- a penalty parameter $ \\theta > 0 $ that discourages probability distortions using relative entropy.  \n",
    "\n",
    "\n",
    "*Type III agent (constraint preferences)* with\n",
    "\n",
    "- $ \\beta \\in (0,1) $;  \n",
    "- IES $ =1 $;  \n",
    "- unit risk aversion;  \n",
    "- an approximating model $ \\Pi_\\infty(\\cdot \\mid x_0) $; and  \n",
    "- a bound $ \\eta $ on discounted relative entropy.  \n",
    "\n",
    "\n",
    "*Type IV agent (pessimistic ex post Bayesian)* with\n",
    "\n",
    "- $ \\beta \\in (0,1) $;  \n",
    "- IES $ =1 $;  \n",
    "- unit risk aversion; and  \n",
    "- a single pessimistic joint distribution $ \\hat\\Pi_\\infty(\\cdot \\mid x_0, \\theta) $ induced by the type II worst-case distortion.  \n",
    "\n",
    "\n",
    "Two sets of equivalence results tie these agents together.\n",
    "\n",
    "Types I and II turn out to be observationally equivalent in a strong sense, having identical preferences over $ \\mathcal{C} $.\n",
    "\n",
    "Types III and IV are equivalent in a weaker but still useful sense, delivering the same worst-case pricing implications as a type II agent for a given endowment process.\n",
    "\n",
    "We now formalize each agent type and describe relationships among them.\n",
    "\n",
    "For each type, we derive a Bellman equation that pins down the agent’s value function and stochastic discount factor.\n",
    "\n",
    "The stochastic discount factor for all four types takes the form\n",
    "\n",
    "$$\n",
    "m_{t+1} = \\beta \\frac{\\partial U_{t+1}/\\partial c_{t+1}}{\\partial U_t/\\partial c_t} \\hat g_{t+1},\n",
    "$$\n",
    "\n",
    "where $ \\hat g_{t+1} $ is a likelihood-ratio distortion that we will define in each case.\n",
    "\n",
    "Along the way, we introduce the likelihood-ratio distortion that enters the stochastic discount factor and describe  detection-error probabilities that will serve as our new calibration tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdf40c",
   "metadata": {},
   "source": [
    "### Type I: Kreps–Porteus–Epstein–Zin–Tallarini preferences\n",
    "\n",
    "The Epstein–Zin–Weil specification combines current consumption with a certainty equivalent of future utility through a CES aggregator:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-ez-general'></a>\n",
    "$$\n",
    "V_t = \\left[(1-\\beta) C_t^{\\rho} + \\beta \\mathcal{R}_t(V_{t+1})^{\\rho}\\right]^{1/\\rho},\n",
    "\\qquad\n",
    "\\rho := 1 - \\frac{1}{\\psi}, \\tag{84.7}\n",
    "$$\n",
    "\n",
    "where $ \\psi > 0 $ is the intertemporal elasticity of substitution and the certainty equivalent uses the risk-aversion parameter $ \\gamma \\geq 1 $:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-certainty-equiv'></a>\n",
    "$$\n",
    "\\mathcal{R}_t(V_{t+1})\n",
    "=\n",
    "\\left(E_t\\left[V_{t+1}^{1-\\gamma}\\right]\\right)^{\\frac{1}{1-\\gamma}}. \\tag{84.8}\n",
    "$$\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For readers interested in a general class of aggregators and certainty equivalents, see Section\n",
    "7.3 of Sargent and Stachurski [[2025](https://python.quantecon.org/zreferences.html#id3)].\n",
    "\n",
    "Let $ \\psi = 1 $, so $ \\rho \\to 0 $.\n",
    "\n",
    "In this limit the CES aggregator reduces to\n",
    "\n",
    "$$\n",
    "V_t = C_t^{1-\\beta} \\cdot \\mathcal{R}_t(V_{t+1})^{\\beta}.\n",
    "$$\n",
    "\n",
    "Taking logs and expanding the certainty equivalent [(84.8)](#equation-bhs-certainty-equiv) gives the *type I recursion*:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-type1-recursion'></a>\n",
    "$$\n",
    "\\log V_t\n",
    "=\n",
    "(1-\\beta)c_t\n",
    "+\n",
    "\\frac{\\beta}{1-\\gamma}\n",
    "\\log E_t\\left[(V_{t+1})^{1-\\gamma}\\right]. \\tag{84.9}\n",
    "$$\n",
    "\n",
    "A useful change of variables is to define the transformed continuation value\n",
    "\n",
    "\n",
    "<a id='equation-bhs-ut-def'></a>\n",
    "$$\n",
    "U_t \\equiv \\frac{\\log V_t}{1-\\beta} \\tag{84.10}\n",
    "$$\n",
    "\n",
    "and the robustness parameter\n",
    "\n",
    "\n",
    "<a id='equation-bhs-theta-def'></a>\n",
    "$$\n",
    "\\theta = \\frac{-1}{(1-\\beta)(1-\\gamma)}. \\tag{84.11}\n",
    "$$\n",
    "\n",
    "Substituting into [(84.9)](#equation-bhs-type1-recursion) yields the *risk-sensitive recursion* ([Exercise 3](#dov_ex3) asks you to verify this step)\n",
    "\n",
    "\n",
    "<a id='equation-bhs-risk-sensitive'></a>\n",
    "$$\n",
    "U_t = c_t - \\beta\\theta \\log E_t\\left[\\exp\\left(\\frac{-U_{t+1}}{\\theta}\\right)\\right]. \\tag{84.12}\n",
    "$$\n",
    "\n",
    "When $ \\gamma = 1 $ (equivalently $ \\theta = +\\infty $), the $ \\log E \\exp $ term reduces to $ E_t U_{t+1} $ and the recursion becomes standard discounted expected log utility, $ U_t = c_t + \\beta E_t U_{t+1} $.\n",
    "\n",
    "For consumption plans in $ \\mathcal{C}(A, B, H; x_0) $, the recursion [(84.12)](#equation-bhs-risk-sensitive) implies the Bellman equation\n",
    "\n",
    "\n",
    "<a id='equation-bhs-bellman-type1'></a>\n",
    "$$\n",
    "U(x) = c - \\beta\\theta \\log \\int \\exp\\left[\\frac{-U(Ax + B\\varepsilon)}{\\theta}\\right] \\pi(\\varepsilon)d\\varepsilon. \\tag{84.13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848a7a5",
   "metadata": {},
   "source": [
    "#### Deriving the stochastic discount factor\n",
    "\n",
    "The stochastic discount factor is the intertemporal marginal rate of substitution, the ratio of marginal utilities at dates $ t+1 $ and $ t $.\n",
    "\n",
    "Because $ c_t $ enters [(84.12)](#equation-bhs-risk-sensitive) linearly, $ \\partial U_t / \\partial c_t = 1 $.\n",
    "\n",
    "Converting from log consumption to the consumption good gives $ \\partial U_t / \\partial C_t = 1/C_t $.\n",
    "\n",
    "A perturbation to $ c_{t+1} $ in a particular state feeds into $ U_t $ through the $ \\log E_t \\exp $ term.\n",
    "\n",
    "Differentiating [(84.12)](#equation-bhs-risk-sensitive):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial U_t}{\\partial c_{t+1}}\n",
    "=\n",
    "-\\beta\\theta\n",
    "\\frac{\\exp(-U_{t+1}/\\theta)  (-1/\\theta)}{E_t[\\exp(-U_{t+1}/\\theta)]}\n",
    "\\underbrace{\\frac{\\partial U_{t+1}}{\\partial c_{t+1}}}_{=1}\n",
    "=\n",
    "\\beta \\frac{\\exp(-U_{t+1}/\\theta)}{E_t[\\exp(-U_{t+1}/\\theta)]}.\n",
    "$$\n",
    "\n",
    "Converting to consumption levels gives\n",
    "$ \\partial U_t / \\partial C_{t+1} = \\beta \\frac{\\exp(-U_{t+1}/\\theta)}{E_t[\\exp(-U_{t+1}/\\theta)]} \\frac{1}{C_{t+1}} $.\n",
    "\n",
    "The ratio of these marginal utilities gives the SDF:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-sdf-ut'></a>\n",
    "$$\n",
    "m_{t+1}\n",
    "=\n",
    "\\frac{\\partial U_t / \\partial C_{t+1}}{\\partial U_t / \\partial C_t}\n",
    "=\n",
    "\\beta \\frac{C_t}{C_{t+1}}\n",
    "\\frac{\\exp(-U_{t+1}/\\theta)}{E_t[\\exp(-U_{t+1}/\\theta)]}. \\tag{84.14}\n",
    "$$\n",
    "\n",
    "The second factor is the likelihood-ratio distortion $ \\hat g_{t+1} $: an exponential tilt that overweights states where the continuation value $ U_{t+1} $ is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319065d6",
   "metadata": {},
   "source": [
    "### Type II: multiplier preferences\n",
    "\n",
    "We now turn to the type II (multiplier) agent.\n",
    "\n",
    "Before writing down the preferences, we need the machinery of martingale likelihood ratios that formalizes what it means to distort a probability model.\n",
    "\n",
    "These tools build on [Likelihood Ratio Processes](https://python.quantecon.org/likelihood_ratio_process.html#likelihood-ratio-process), which develops properties of likelihood ratios in detail, and [Divergence Measures](https://python.quantecon.org/divergence_measures.html#divergence-measures), which covers relative entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce23c06",
   "metadata": {},
   "source": [
    "#### Martingale likelihood ratios\n",
    "\n",
    "Consider a nonnegative martingale $ G_t $ with $ E(G_t \\mid x_0) = 1 $.\n",
    "\n",
    "Its one-step increments\n",
    "\n",
    "$$\n",
    "g_{t+1} = \\frac{G_{t+1}}{G_t},\n",
    "\\qquad\n",
    "E_t[g_{t+1}] = 1,\n",
    "\\quad\n",
    "g_{t+1} \\geq 0,\n",
    "\\qquad\n",
    "G_0 = 1,\n",
    "$$\n",
    "\n",
    "define distorted conditional expectations: $ \\tilde E_t[b_{t+1}] = E_t[g_{t+1}b_{t+1}] $.\n",
    "\n",
    "The conditional relative entropy of the distortion is $ E_t[g_{t+1}\\log g_{t+1}] $, and the discounted entropy over the entire path is $ \\beta E\\bigl[\\sum_{t=0}^{\\infty} \\beta^t G_tE_t(g_{t+1}\\log g_{t+1})\\big|x_0\\bigr] $.\n",
    "\n",
    "A type II agent’s *multiplier* preference ordering over consumption plans $ C^\\infty \\in \\mathcal{C}(A,B,H;x_0) $ is defined by\n",
    "\n",
    "\n",
    "<a id='equation-bhs-type2-objective'></a>\n",
    "$$\n",
    "\\min_{\\{g_{t+1}\\}}\n",
    "\\sum_{t=0}^{\\infty} E\\left\\{\\beta^t G_t\n",
    "\\left[c_t + \\beta\\theta E_t\\left(g_{t+1}\\log g_{t+1}\\right)\\right]\n",
    "\\Big| x_0\\right\\}, \\tag{84.15}\n",
    "$$\n",
    "\n",
    "where $ G_{t+1} = g_{t+1}G_t $, $ E_t[g_{t+1}] = 1 $, $ g_{t+1} \\geq 0 $, and $ G_0 = 1 $.\n",
    "\n",
    "A larger $ \\theta $ makes probability distortions more expensive, discouraging departures from the approximating model.\n",
    "\n",
    "The value function satisfies the Bellman equation\n",
    "\n",
    "\n",
    "<a id='equation-bhs-bellman-type2'></a>\n",
    "$$\n",
    "W(x)\n",
    "=\n",
    "c + \\min_{g(\\varepsilon) \\geq 0}\n",
    "\\beta \\int \\bigl[g(\\varepsilon) W(Ax + B\\varepsilon)\n",
    "+ \\theta g(\\varepsilon)\\log g(\\varepsilon)\\bigr] \\pi(\\varepsilon) d\\varepsilon \\tag{84.16}\n",
    "$$\n",
    "\n",
    "subject to $ \\int g(\\varepsilon) \\pi(\\varepsilon) d\\varepsilon = 1 $.\n",
    "\n",
    "Inside the integral, $ g(\\varepsilon) W(Ax + B\\varepsilon) $ is the continuation value under the distorted model $ g\\pi $, while $ \\theta g(\\varepsilon)\\log g(\\varepsilon) $ is the entropy penalty that makes large departures from the approximating model $ \\pi $ costly.\n",
    "\n",
    "The minimizer is ([Exercise 4](#dov_ex4) derives this and verifies the equivalence $ W \\equiv U $)\n",
    "\n",
    "\n",
    "<a id='equation-bhs-ghat'></a>\n",
    "$$\n",
    "\\hat g_{t+1}\n",
    "=\n",
    "\\frac{\\exp \\bigl(-W(Ax_t + B\\varepsilon_{t+1})/\\theta\\bigr)}{E_t \\left[\\exp \\bigl(-W(Ax_t + B\\varepsilon_{t+1})/\\theta\\bigr)\\right]}. \\tag{84.17}\n",
    "$$\n",
    "\n",
    "Notice that $ g(\\varepsilon) $ multiplies both the continuation value $ W $ and the entropy penalty.\n",
    "\n",
    "This is the key structural feature that makes $ \\hat g $ a likelihood ratio.\n",
    "\n",
    "Substituting [(84.17)](#equation-bhs-ghat) back into [(84.16)](#equation-bhs-bellman-type2) gives\n",
    "\n",
    "$$\n",
    "W(x) = c - \\beta\\theta \\log \\int \\exp \\left[\\frac{-W(Ax + B\\varepsilon)}{\\theta}\\right]\\pi(\\varepsilon) d\\varepsilon,\n",
    "$$\n",
    "\n",
    "which is identical to [(84.13)](#equation-bhs-bellman-type1).\n",
    "\n",
    "Therefore $ W(x) \\equiv U(x) $, establishing that *types I and II are observationally equivalent* over elements of $ \\mathcal{C}(A,B,H;x_0) $.\n",
    "\n",
    "The mapping between parameters is\n",
    "\n",
    "$$\n",
    "\\theta = \\left[(1-\\beta)(\\gamma - 1)\\right]^{-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd26e8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def θ_from_γ(γ, β=β):\n",
    "    if γ <= 1:\n",
    "        return np.inf\n",
    "    return 1.0 / ((1.0 - β) * (γ - 1.0))\n",
    "\n",
    "\n",
    "def γ_from_θ(θ, β=β):\n",
    "    if np.isinf(θ):\n",
    "        return 1.0\n",
    "    return 1.0 + 1.0 / ((1.0 - β) * θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3b393",
   "metadata": {},
   "source": [
    "### Type III: constraint preferences\n",
    "\n",
    "Type III (constraint) preferences swap the entropy penalty for a hard bound.\n",
    "\n",
    "Rather than penalizing distortions through $ \\theta $, the agent minimizes expected discounted log consumption under the worst-case model subject to a cap $ \\eta $ on discounted relative entropy:\n",
    "\n",
    "$$\n",
    "J(x_0)\n",
    "=\n",
    "\\min_{\\{g_{t+1}\\}}\n",
    "\\sum_{t=0}^{\\infty} E \\left[\\beta^t G_t c_t \\Big|  x_0\\right]\n",
    "$$\n",
    "\n",
    "subject to $ G_{t+1} = g_{t+1}G_t $, $ E_t[g_{t+1}] = 1 $, $ g_{t+1} \\geq 0 $, $ G_0 = 1 $, and\n",
    "\n",
    "$$\n",
    "\\beta E \\left[\\sum_{t=0}^{\\infty} \\beta^t G_t E_t\\left(g_{t+1}\\log g_{t+1}\\right)\\Big|x_0\\right] \\leq \\eta.\n",
    "$$\n",
    "\n",
    "The Lagrangian for the type III problem is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "\\sum_{t=0}^{\\infty} E\\left[\\beta^t G_t c_t \\Big| x_0\\right]\n",
    "+\n",
    "\\theta \\left[\n",
    "\\beta E \\left(\\sum_{t=0}^{\\infty} \\beta^t G_t E_t(g_{t+1}\\log g_{t+1})\\Big| x_0 \\right) - \\eta\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $ \\theta \\geq 0 $ is the multiplier on the entropy constraint.\n",
    "\n",
    "Collecting terms inside the expectation gives\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "\\sum_{t=0}^{\\infty} E \\left \\{\\beta^t G_t\n",
    "\\left[c_t + \\beta \\theta E_t(g_{t+1}\\log g_{t+1})\\right]\n",
    "\\Big| x_0\\right\\} - \\theta\\eta,\n",
    "$$\n",
    "\n",
    "which, apart from the constant $ -\\theta\\eta $, has the same structure as the type II objective [(84.15)](#equation-bhs-type2-objective).\n",
    "\n",
    "The first-order condition for $ g_{t+1} $ is therefore identical, and the optimal distortion is the same $ \\hat g_{t+1} $ as in [(84.17)](#equation-bhs-ghat), evaluated at the $ \\theta $ that makes the entropy constraint bind.\n",
    "\n",
    "The SDF is again $ m_{t+1} = \\beta(C_t/C_{t+1})\\hat g_{t+1} $.\n",
    "\n",
    "So for the particular endowment process and the $ \\theta $ that enforces the entropy bound, a type III agent and a type II agent assign the same shadow prices to uncertain claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd6fe4",
   "metadata": {},
   "source": [
    "### Type IV: ex post Bayesian\n",
    "\n",
    "The type IV agent is the simplest of the four: an ordinary expected-utility agent with log preferences who happens to hold a pessimistic probability model $ \\hat\\Pi_\\infty $:\n",
    "\n",
    "$$\n",
    "\\hat E_0 \\sum_{t=0}^{\\infty} \\beta^t c_t.\n",
    "$$\n",
    "\n",
    "$ \\hat E_0 $ denotes expectation under the pessimistic model $ \\hat\\Pi_\\infty $.\n",
    "\n",
    "Here $ \\hat\\Pi_\\infty(\\cdot \\mid x_0, \\theta) $ is the joint distribution generated by the type II agent’s worst-case distortion.\n",
    "\n",
    "Since the agent has log utility under $ \\hat\\Pi_\\infty $, the Euler equation for any gross return $ R_{t+1} $ is\n",
    "\n",
    "$$\n",
    "1 = \\hat E_t \\left[\\beta \\frac{C_t}{C_{t+1}} R_{t+1}\\right].\n",
    "$$\n",
    "\n",
    "To express this in terms of the approximating model $ \\Pi_\\infty $, apply a change of measure using the one-step likelihood ratio $ \\hat g_{t+1} = d\\hat\\Pi / d\\Pi $:\n",
    "\n",
    "$$\n",
    "1 = E_t\\left[\\hat g_{t+1} \\cdot \\beta \\frac{C_t}{C_{t+1}} R_{t+1}\\right]\n",
    "= E_t\\left[m_{t+1} R_{t+1}\\right],\n",
    "$$\n",
    "\n",
    "so the effective SDF under the approximating model is $ m_{t+1} = \\beta(C_t/C_{t+1})\\hat g_{t+1} $.\n",
    "\n",
    "For the particular $ A, B, H $ and $ \\theta $ used to construct $ \\hat\\Pi_\\infty $, the type IV value function equals $ J(x) $ from type III."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d005c",
   "metadata": {},
   "source": [
    "### Stochastic discount factor\n",
    "\n",
    "Pulling together the results for all four agent types, the stochastic discount factor can be written compactly as\n",
    "\n",
    "\n",
    "<a id='equation-bhs-sdf'></a>\n",
    "$$\n",
    "m_{t+1}\n",
    "=\n",
    "\\beta \\frac{C_t}{C_{t+1}} \\hat g_{t+1}. \\tag{84.18}\n",
    "$$\n",
    "\n",
    "The factor $ \\hat g_{t+1} $ is a likelihood ratio between the approximating and worst-case one-step models.\n",
    "\n",
    "With log utility, $ C_t/C_{t+1} = \\exp(-(c_{t+1}-c_t)) $ is the usual intertemporal marginal rate of substitution.\n",
    "\n",
    "Robustness multiplies it by $ \\hat g_{t+1} $, so uncertainty aversion enters pricing entirely through the distortion.\n",
    "\n",
    "For the constraint-preference agent, the worst-case distortion coincides with the multiplier agent’s at the $ \\theta $ that makes the entropy constraint bind.\n",
    "\n",
    "For the ex post Bayesian, it is simply a change of measure from the approximating model to the pessimistic one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf81def",
   "metadata": {},
   "source": [
    "### Value function decomposition\n",
    "\n",
    "Substituting the minimizing $ \\hat g $ back into the Bellman equation [(84.16)](#equation-bhs-bellman-type2) yields a revealing decomposition of the type II value function:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-w-decomp-bellman'></a>\n",
    "$$\n",
    "W(x) = c + \\beta \\int \\bigl[\\hat g(\\varepsilon) W(Ax + B\\varepsilon) + \\theta \\hat g(\\varepsilon)\\log \\hat g(\\varepsilon)\\bigr] \\pi(\\varepsilon)d\\varepsilon. \\tag{84.19}\n",
    "$$\n",
    "\n",
    "Define two components:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-j-recursion'></a>\n",
    "$$\n",
    "J(x) = c + \\beta \\int \\hat g(\\varepsilon) J(Ax + B\\varepsilon) \\pi(\\varepsilon)d\\varepsilon, \\tag{84.20}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-bhs-n-recursion'></a>\n",
    "$$\n",
    "N(x) = \\beta \\int \\hat g(\\varepsilon)\\bigl[\\log \\hat g(\\varepsilon) + N(Ax + B\\varepsilon)\\bigr] \\pi(\\varepsilon)d\\varepsilon. \\tag{84.21}\n",
    "$$\n",
    "\n",
    "Then $ W(x) = J(x) + \\theta N(x) $.\n",
    "\n",
    "Here $ J(x_t) = \\hat E_t \\sum_{j=0}^{\\infty} \\beta^j c_{t+j} $ is expected discounted log consumption under the *worst-case* model.\n",
    "\n",
    "$ J $ is the value function shared by both the type III and type IV agents.\n",
    "\n",
    "For the type III agent, once the worst-case model is pinned down by the entropy constraint, the resulting value is simply expected discounted consumption under that model.\n",
    "\n",
    "The type IV agent adopts the same model as a fixed belief, so she evaluates the same expectation.\n",
    "\n",
    "The term $ N(x) $ is discounted continuation entropy, measuring the total information cost of the probability distortion from date $ t $ onward.\n",
    "\n",
    "This decomposition plays a central role in the welfare calculations of [the welfare section](#welfare-experiments) below, where it explains why type III uncertainty compensation is twice that of type II."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d361e",
   "metadata": {},
   "source": [
    "### Gaussian mean-shift distortions\n",
    "\n",
    "Everything so far holds for general distortions $ \\hat g $.\n",
    "\n",
    "We now specialize to the Gaussian case that underlies our two consumption models.\n",
    "\n",
    "Under both models, the shock is $ \\varepsilon_{t+1} \\sim \\mathcal{N}(0,1) $.\n",
    "\n",
    "As we verify in the next subsection, the value function $ W $ is linear in the state, so the exponent in the worst-case distortion [(84.17)](#equation-bhs-ghat) is linear in $ \\varepsilon_{t+1} $.\n",
    "\n",
    "Exponentially tilting a Gaussian by a linear function produces another Gaussian with the same variance but a shifted mean.\n",
    "\n",
    "The worst-case model therefore keeps the variance at one but shifts the mean of $ \\varepsilon_{t+1} $ to some $ w < 0 $.\n",
    "\n",
    "The resulting likelihood ratio is ([Exercise 5](#dov_ex5) verifies its properties)\n",
    "\n",
    "\n",
    "<a id='equation-bhs-ghat-gaussian'></a>\n",
    "$$\n",
    "\\hat g_{t+1}\n",
    "=\n",
    "\\exp\\left(w \\varepsilon_{t+1} - \\frac{1}{2}w^2\\right),\n",
    "\\qquad\n",
    "E_t[\\hat g_{t+1}] = 1. \\tag{84.22}\n",
    "$$\n",
    "\n",
    "Hence $ \\log \\hat g_{t+1} $ is normal with mean $ -w^2/2 $ and variance $ w^2 $, and\n",
    "\n",
    "$$\n",
    "\\operatorname{std}(\\hat g_{t+1}) = \\sqrt{e^{w^2}-1}.\n",
    "$$\n",
    "\n",
    "The mean shift $ w $ is determined by how strongly each shock $ \\varepsilon_{t+1} $ affects continuation value.\n",
    "\n",
    "From [(84.17)](#equation-bhs-ghat), the worst-case distortion puts $ \\hat g \\propto \\exp(-W(x_{t+1})/\\theta) $.\n",
    "\n",
    "If $ W(x_{t+1}) $ loads on $ \\varepsilon_{t+1} $ with coefficient $ \\lambda $, then the Gaussian mean shift is $ w = -\\lambda/\\theta $.\n",
    "\n",
    "By guessing linear value functions and matching coefficients in the Bellman equation ([Exercise 6](#dov_ex6) works out both cases), we obtain the worst-case mean shifts\n",
    "\n",
    "\n",
    "<a id='equation-bhs-w-formulas'></a>\n",
    "$$\n",
    "w_{rw}(\\theta) = -\\frac{\\sigma_\\varepsilon}{(1-\\beta)\\theta},\n",
    "\\qquad\n",
    "w_{ts}(\\theta) = -\\frac{\\sigma_\\varepsilon}{(1-\\rho\\beta)\\theta}. \\tag{84.23}\n",
    "$$\n",
    "\n",
    "The denominator $ (1-\\beta) $ in the random-walk case becomes $ (1-\\beta\\rho) $ in the trend-stationary case.\n",
    "\n",
    "Because the AR(1) component is persistent, each shock has a larger cumulative effect on continuation utility, so the worst-case distortion is more aggressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80837f2c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def w_from_θ(θ, model):\n",
    "    if np.isinf(θ):\n",
    "        return 0.0\n",
    "    if model == \"rw\":\n",
    "        return -rw[\"σ_ε\"] / ((1.0 - β) * θ)\n",
    "    if model == \"ts\":\n",
    "        return -ts[\"σ_ε\"] / ((1.0 - β * ts[\"ρ\"]) * θ)\n",
    "    raise ValueError(\"model must be 'rw' or 'ts'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152879e",
   "metadata": {},
   "source": [
    "### Discounted entropy\n",
    "\n",
    "When the approximating and worst-case conditional densities are $ \\mathcal{N}(0,1) $ and $ \\mathcal{N}(w(\\theta),1) $, the likelihood ratio is $ \\hat g(\\varepsilon) = \\exp(w(\\theta)\\varepsilon - \\frac{1}{2}w(\\theta)^2) $, so $ \\log \\hat g(\\varepsilon) = w(\\theta)\\varepsilon - \\frac{1}{2}w(\\theta)^2 $.\n",
    "\n",
    "Under the worst-case measure $ \\varepsilon \\sim \\mathcal{N}(w(\\theta),1) $, so $ E_{\\hat\\pi}[\\varepsilon] = w(\\theta) $, giving conditional relative entropy\n",
    "\n",
    "\n",
    "<a id='equation-bhs-conditional-entropy'></a>\n",
    "$$\n",
    "E_t[\\hat g_{t+1}\\log \\hat g_{t+1}] = w(\\theta) \\cdot w(\\theta) - \\frac{1}{2}w(\\theta)^2 = \\frac{1}{2}w(\\theta)^2. \\tag{84.24}\n",
    "$$\n",
    "\n",
    "Because the distortion is i.i.d., the conditional entropy $ E_t[\\hat g_{t+1}\\log \\hat g_{t+1}] = \\frac{1}{2}w(\\theta)^2 $ from [(84.24)](#equation-bhs-conditional-entropy) is constant and $ N(x) $ does not depend on $ x $.\n",
    "\n",
    "The recursion [(84.21)](#equation-bhs-n-recursion) then reduces to $ N(x) = \\beta(\\frac{1}{2}w(\\theta)^2 + N(x)) $, where we have used $ \\int \\hat g(\\varepsilon)\\pi(\\varepsilon)d\\varepsilon = 1 $ (since $ \\hat g $ is a likelihood ratio).\n",
    "\n",
    "Solving for $ N(x) $,\n",
    "\n",
    "$$\n",
    "N(x)(1-\\beta) = \\frac{\\beta}{2}w(\\theta)^2,\n",
    "$$\n",
    "\n",
    "gives discounted entropy\n",
    "\n",
    "\n",
    "<a id='equation-bhs-eta-formula'></a>\n",
    "$$\n",
    "\\eta = N(x) = \\frac{\\beta}{2(1-\\beta)} w(\\theta)^2. \\tag{84.25}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a70234",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def η_from_θ(θ, model):\n",
    "    w = w_from_θ(θ, model)\n",
    "    return β * w**2 / (2.0 * (1.0 - β))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dae824",
   "metadata": {},
   "source": [
    "This gives a clean mapping between $ \\theta $ and $ \\eta $ that aligns multiplier and constraint preferences along an exogenous endowment process.\n",
    "\n",
    "As we will see in the [detection-error section](#detection-error-section) below, it is more natural to hold $ \\eta $ (or equivalently the detection-error probability $ p $) fixed rather than $ \\theta $ when comparing across consumption models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaacdd9",
   "metadata": {},
   "source": [
    "### Value functions for random-walk consumption\n",
    "\n",
    "We now solve the recursions [(84.19)](#equation-bhs-w-decomp-bellman), [(84.20)](#equation-bhs-j-recursion), and [(84.21)](#equation-bhs-n-recursion) in closed form for the random-walk model, where $ W $ is the type II (multiplier) value function, $ J $ is the type III/IV value function, and $ N $ is discounted continuation entropy.\n",
    "\n",
    "Substituting $ w_{rw}(\\theta) = -\\sigma_\\varepsilon / [(1-\\beta)\\theta] $ from [(84.23)](#equation-bhs-w-formulas) into [(84.25)](#equation-bhs-eta-formula) gives\n",
    "\n",
    "$$\n",
    "N(x) = \\frac{\\beta}{2(1-\\beta)} w_{rw}(\\theta)^2\n",
    "  = \\frac{\\beta}{2(1-\\beta)} \\left(\\frac{-\\sigma_\\varepsilon}{(1-\\beta)\\theta}\\right)^2\n",
    "  = \\frac{\\beta}{2(1-\\beta)} \\cdot \\frac{\\sigma_\\varepsilon^2}{(1-\\beta)^2\\theta^2}\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "\n",
    "<a id='equation-bhs-n-rw'></a>\n",
    "$$\n",
    "N(x) = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^3\\theta^2}. \\tag{84.26}\n",
    "$$\n",
    "\n",
    "For $ W $, we guess $ W(x_t) = \\frac{1}{1-\\beta}[c_t + d] $ for some constant $ d $ and verify it in the risk-sensitive Bellman equation [(84.13)](#equation-bhs-bellman-type1).\n",
    "\n",
    "Under the random walk, $ W(x_{t+1}) = \\frac{1}{1-\\beta}[c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} + d] $, so $ -W(x_{t+1})/\\theta $ is affine in the standard normal $ \\varepsilon_{t+1} $.\n",
    "\n",
    "Using the fact that $ \\log E[e^Z] = \\mu_Z + \\frac{1}{2}\\sigma_Z^2 $ for a normal random variable $ Z $, the Bellman equation [(84.13)](#equation-bhs-bellman-type1) reduces to a constant-matching condition that pins down $ d $ ([Exercise 7](#dov_ex7) works through the algebra):\n",
    "\n",
    "\n",
    "<a id='equation-bhs-w-rw'></a>\n",
    "$$\n",
    "W(x_t) = \\frac{1}{1-\\beta}\\left[c_t + \\frac{\\beta}{1-\\beta}\\left(\\mu - \\frac{\\sigma_\\varepsilon^2}{2(1-\\beta)\\theta}\\right)\\right]. \\tag{84.27}\n",
    "$$\n",
    "\n",
    "Using $ W = J + \\theta N $, the type III/IV value function is\n",
    "\n",
    "\n",
    "<a id='equation-bhs-j-rw'></a>\n",
    "$$\n",
    "J(x_t) = W(x_t) - \\theta N(x_t) = \\frac{1}{1-\\beta}\\left[c_t + \\frac{\\beta}{1-\\beta}\\left(\\mu - \\frac{\\sigma_\\varepsilon^2}{(1-\\beta)\\theta}\\right)\\right]. \\tag{84.28}\n",
    "$$\n",
    "\n",
    "Notice that the coefficient on $ \\sigma_\\varepsilon^2/[(1-\\beta)\\theta] $ doubles from $ \\tfrac{1}{2} $ in $ W $ to $ 1 $ in $ J $.\n",
    "\n",
    "The reason is that $ W $ includes the entropy “rebate” $ \\theta N $, which partially offsets the pessimistic tilt, while $ J $ evaluates consumption purely under the worst-case model with no such offset.\n",
    "\n",
    "\n",
    "<a id='detection-error-section'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041190c",
   "metadata": {},
   "source": [
    "## Detection-error probabilities\n",
    "\n",
    "So far we have expressed SDF moments, value functions, and worst-case distortions as functions of $ \\gamma $ (or equivalently $ \\theta $).\n",
    "\n",
    "But if $ \\gamma $ should not be calibrated by introspection about atemporal gambles, what replaces it?\n",
    "\n",
    "The answer proposed by Barillas *et al.* [[2009](https://python.quantecon.org/zreferences.html#id50)] is a statistical test: how easily could an econometrician distinguish the approximating model from its worst-case alternative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66298ade",
   "metadata": {},
   "source": [
    "### Likelihood-ratio testing and detection errors\n",
    "\n",
    "Let $ L_T $ be the log likelihood ratio between the worst-case and approximating models based on a sample of length $ T $.\n",
    "\n",
    "Define\n",
    "\n",
    "$$\n",
    "p_A = \\Pr_A(L_T < 0),\n",
    "\\qquad\n",
    "p_B = \\Pr_B(L_T > 0),\n",
    "$$\n",
    "\n",
    "where $ \\Pr_A $ and $ \\Pr_B $ denote probabilities under the approximating and worst-case models.\n",
    "\n",
    "Then $ p(\\theta^{-1}) = \\frac{1}{2}(p_A + p_B) $ is the average probability of choosing the wrong model.\n",
    "\n",
    "Fix a sample size $ T $ (here 235 quarters, matching the postwar US data used in the paper).\n",
    "\n",
    "For a given $ \\theta $, compute the worst-case model and imagine that a Bayesian runs a likelihood-ratio test to distinguish it from the approximating model.\n",
    "\n",
    "What fraction of the time would she pick the wrong one?\n",
    "\n",
    "That fraction is the **detection-error probability** $ p(\\theta^{-1}) $.\n",
    "\n",
    "When $ p $ is close to 0.5 the two models are nearly indistinguishable, so the consumer’s fear is hard to rule out.\n",
    "\n",
    "When $ p $ is small the worst-case model is easy to reject and the robustness concern carries less force."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99aa9c8",
   "metadata": {},
   "source": [
    "### Market price of model uncertainty\n",
    "\n",
    "The **market price of model uncertainty** (MPU) is the conditional standard deviation of the distortion:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-mpu-formula'></a>\n",
    "$$\n",
    "\\text{MPU}\n",
    "=\n",
    "\\operatorname{std}(\\hat g_{t+1})\n",
    "=\n",
    "\\sqrt{e^{w(\\theta)^2}-1}\n",
    "\\approx |w(\\theta)|. \\tag{84.29}\n",
    "$$\n",
    "\n",
    "In the Gaussian mean-shift setting, $ L_T $ is normal with mean $ \\pm \\tfrac{1}{2}w^2T $ and variance $ w^2T $, so the detection-error probability has the closed form ([Exercise 8](#dov_ex8) derives this)\n",
    "\n",
    "\n",
    "<a id='equation-bhs-detection-formula'></a>\n",
    "$$\n",
    "p(\\theta^{-1})\n",
    "=\n",
    "\\frac{1}{2}\\left(p_A + p_B\\right), \\tag{84.30}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-bhs-detection-closed'></a>\n",
    "$$\n",
    "p(\\theta^{-1}) = \\Phi \\left(-\\frac{|w(\\theta)|\\sqrt{T}}{2}\\right). \\tag{84.31}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff88b8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def detection_probability(θ, model):\n",
    "    w = abs(w_from_θ(θ, model))\n",
    "    return norm.cdf(-0.5 * w * np.sqrt(T))\n",
    "\n",
    "\n",
    "def θ_from_detection_probability(p, model):\n",
    "    if p >= 0.5:\n",
    "        return np.inf\n",
    "    w_abs = -2.0 * norm.ppf(p) / np.sqrt(T)\n",
    "    if model == \"rw\":\n",
    "        return rw[\"σ_ε\"] / ((1.0 - β) * w_abs)\n",
    "    if model == \"ts\":\n",
    "        return ts[\"σ_ε\"] / ((1.0 - β * ts[\"ρ\"]) * w_abs)\n",
    "    raise ValueError(\"model must be 'rw' or 'ts'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72aff9",
   "metadata": {},
   "source": [
    "### Interpreting the calibration objects\n",
    "\n",
    "Let us trace the chain of mappings that connects preference parameters to statistical distinguishability.\n",
    "\n",
    "The parameter $ \\theta $ governs how expensive it is for the minimizing player to distort the approximating model.\n",
    "\n",
    "A small $ \\theta $ means cheap distortions and therefore stronger robustness concerns.\n",
    "\n",
    "The associated $ \\gamma = 1 + \\left[(1-\\beta)\\theta\\right]^{-1} $ can be large even when we do not want to interpret behavior as extreme atemporal risk aversion.\n",
    "\n",
    "The distortion magnitude $ |w(\\theta)| $ directly measures how pessimistically the agent tilts one-step probabilities.\n",
    "\n",
    "The detection-error probability $ p(\\theta^{-1}) $ translates that tilt into a statistical statement about finite-sample distinguishability.\n",
    "\n",
    "High $ p $ means the two models are hard to tell apart, while low $ p $ means the worst case is easier to reject.\n",
    "\n",
    "This chain bridges econometric identification and preference calibration.\n",
    "\n",
    "Finally, recall from [(84.25)](#equation-bhs-eta-formula) that discounted entropy is $ \\eta = \\frac{\\beta}{2(1-\\beta)}w(\\theta)^2 $, so when the distortion is a Gaussian mean shift, discounted entropy is proportional to the squared market price of model uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a8c9",
   "metadata": {},
   "source": [
    "### Detection probabilities across the two models\n",
    "\n",
    "The left panel below plots $ p(\\theta^{-1}) $ against $ \\theta^{-1} $ for both consumption specifications.\n",
    "\n",
    "Because the baseline dynamics differ, the same numerical $ \\theta $ implies very different detection probabilities across the two models.\n",
    "\n",
    "The right panel resolves this by plotting detection probabilities against discounted relative entropy $ \\eta $, which normalizes the statistical distance.\n",
    "\n",
    "Once indexed by $ \\eta $, the two curves fall on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784ea94",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "θ_inv_grid = np.linspace(0.0, 1.8, 400)\n",
    "θ_grid = np.full_like(θ_inv_grid, np.inf)\n",
    "mask_θ = θ_inv_grid > 0.0\n",
    "θ_grid[mask_θ] = 1.0 / θ_inv_grid[mask_θ]\n",
    "\n",
    "p_rw = np.array([detection_probability(θ, \"rw\") for θ in θ_grid])\n",
    "p_ts = np.array([detection_probability(θ, \"ts\") for θ in θ_grid])\n",
    "\n",
    "η_rw = np.array([η_from_θ(θ, \"rw\") for θ in θ_grid])\n",
    "η_ts = np.array([η_from_θ(θ, \"ts\") for θ in θ_grid])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(θ_inv_grid, 100.0 * p_rw, lw=2, label=\"random walk\")\n",
    "axes[0].plot(θ_inv_grid, 100.0 * p_ts, lw=2, label=\"trend stationary\")\n",
    "axes[0].set_xlabel(r\"$\\theta^{-1}$\")\n",
    "axes[0].set_ylabel(\"detection error probability (percent)\")\n",
    "axes[0].legend(frameon=False)\n",
    "\n",
    "axes[1].plot(η_rw, 100.0 * p_rw, lw=2, label=\"random walk\")\n",
    "axes[1].plot(η_ts, 100.0 * p_ts, lw=2, ls=\"--\", label=\"trend stationary\")\n",
    "axes[1].set_xlabel(r\"discounted entropy $\\eta$\")\n",
    "axes[1].set_ylabel(\"detection error probability (percent)\")\n",
    "axes[1].set_xlim(0.0, 10)\n",
    "axes[1].legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e87059",
   "metadata": {},
   "source": [
    "Detection-error probabilities (or equivalently, discounted entropy) are therefore the right yardstick for cross-model comparisons.\n",
    "\n",
    "If we hold $ \\theta $ fixed when switching from a random walk to a trend-stationary specification, we implicitly change how much misspecification the consumer fears.\n",
    "\n",
    "Holding $ \\eta $ or $ p $ fixed instead keeps the statistical difficulty of detecting misspecification constant.\n",
    "\n",
    "The explicit mapping that equates discounted entropy across models is ([Exercise 9](#dov_ex9) derives it):\n",
    "\n",
    "\n",
    "<a id='equation-bhs-theta-cross-model'></a>\n",
    "$$\n",
    "\\theta_{\\text{TS}}\n",
    "=\n",
    "\\left(\\frac{\\sigma_\\varepsilon^{\\text{TS}}}{\\sigma_\\varepsilon^{\\text{RW}}}\\right)\n",
    "\\frac{1-\\beta}{1-\\rho\\beta} \\theta_{\\text{RW}}. \\tag{84.32}\n",
    "$$\n",
    "\n",
    "At our calibration $ \\sigma_\\varepsilon^{\\text{TS}} = \\sigma_\\varepsilon^{\\text{RW}} $, this simplifies to $ \\theta_{\\text{TS}} = \\frac{1-\\beta}{1-\\rho\\beta}\\theta_{\\text{RW}} $.\n",
    "\n",
    "Because $ \\rho = 0.98 $ and $ \\beta = 0.995 $, the ratio $ (1-\\beta)/(1-\\rho\\beta) $ is much less than one, so holding entropy fixed requires a substantially smaller $ \\theta $ (stronger robustness) for the trend-stationary model than for the random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b620a7",
   "metadata": {},
   "source": [
    "## Unify the two models using detection-error probabilities\n",
    "\n",
    "With this machinery in hand, we can redraw Tallarini’s figure using detection-error probabilities as the common index.\n",
    "\n",
    "For each $ p(\\theta^{-1}) = 0.50, 0.45, \\ldots, 0.01 $, we invert to find the model-specific $ \\theta $, convert to $ \\gamma $, and plot the implied $ (E(m), \\sigma(m)) $ pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fa20f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p_points = np.array(\n",
    "    [0.50, 0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05, 0.01])\n",
    "\n",
    "θ_rw_points = np.array(\n",
    "    [θ_from_detection_probability(p, \"rw\") for p in p_points])\n",
    "θ_ts_points = np.array(\n",
    "    [θ_from_detection_probability(p, \"ts\") for p in p_points])\n",
    "\n",
    "γ_rw_points = np.array([γ_from_θ(θ) for θ in θ_rw_points])\n",
    "γ_ts_points = np.array([γ_from_θ(θ) for θ in θ_ts_points])\n",
    "\n",
    "Em_rw_p = np.array(\n",
    "    [moments_type1_rw(γ)[0] for γ in γ_rw_points])\n",
    "σ_m_rw_p = np.array(\n",
    "    [moments_type1_rw(γ)[0] * moments_type1_rw(γ)[1] for γ in γ_rw_points])\n",
    "Em_ts_p = np.array(\n",
    "    [moments_type1_ts(γ)[0] for γ in γ_ts_points])\n",
    "σ_m_ts_p = np.array(\n",
    "    [moments_type1_ts(γ)[0] * moments_type1_ts(γ)[1] for γ in γ_ts_points])\n",
    "\n",
    "print(\"p      γ_rw      γ_ts\")\n",
    "for p, g1, g2 in zip(p_points, γ_rw_points, γ_ts_points):\n",
    "    print(f\"{p:>4.2f} {g1:>9.2f} {g2:>9.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a10a4d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Empirical Sharpe ratio — the minimum of the HJ bound curve\n",
    "sharpe = (r_e_mean - r_f_mean) / r_excess_std\n",
    "\n",
    "def sharpe_gap(p, model):\n",
    "    \"\"\"Market price of risk minus Sharpe ratio, as a function of p.\"\"\"\n",
    "    if p >= 0.5:\n",
    "        return -sharpe\n",
    "    θ = θ_from_detection_probability(p, model)\n",
    "    γ = γ_from_θ(θ)\n",
    "    _, mpr = moments_type1_rw(γ) if model == \"rw\" else moments_type1_ts(γ)\n",
    "    return mpr - sharpe\n",
    "\n",
    "p_hj_rw = brentq(sharpe_gap, 1e-4, 0.49, args=(\"rw\",))\n",
    "p_hj_ts = brentq(sharpe_gap, 1e-4, 0.49, args=(\"ts\",))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(Em_rw_p, σ_m_rw_p, \"o\",\n",
    "            label=\"random walk\")\n",
    "ax.plot(Em_ts_p, σ_m_ts_p, \"+\", markersize=12,\n",
    "            label=\"trend stationary\")\n",
    "ax.plot(Em_grid, HJ_std, lw=2,\n",
    "            color=\"black\", label=\"Hansen-Jagannathan bound\")\n",
    "\n",
    "# Mark p where each model's market price of risk reaches the Sharpe ratio\n",
    "for p_hj, model, color, name, marker in [\n",
    "    (p_hj_rw, \"rw\", \"C0\", \"RW\", \"o\"),\n",
    "    (p_hj_ts, \"ts\", \"C1\", \"TS\", \"+\"),\n",
    "]:\n",
    "    θ_hj = θ_from_detection_probability(p_hj, model)\n",
    "    γ_hj = γ_from_θ(θ_hj)\n",
    "    Em_hj, mpr_hj = (moments_type1_rw(γ_hj) if model == \"rw\"\n",
    "                      else moments_type1_ts(γ_hj))\n",
    "    σ_m_hj = Em_hj * mpr_hj\n",
    "    ax.axhline(σ_m_hj, ls=\"--\", lw=1, color=color,\n",
    "               label=f\"{name} reaches bound at $p = {p_hj:.3f}$\")\n",
    "    if model == \"ts\":\n",
    "        ax.plot(Em_hj, σ_m_hj, marker, markersize=12, color=color)\n",
    "    else:\n",
    "        ax.plot(Em_hj, σ_m_hj, marker, color=color)\n",
    "\n",
    "ax.set_xlabel(r\"$E(m)$\")\n",
    "ax.set_ylabel(r\"$\\sigma(m)$\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlim(0.96, 1.05)\n",
    "ax.set_ylim(0.0, 0.34)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc423da",
   "metadata": {},
   "source": [
    "The result is striking.\n",
    "\n",
    "The random-walk and trend-stationary loci nearly coincide.\n",
    "\n",
    "Recall that under Tallarini’s $ \\gamma $-calibration, reaching the Hansen–Jagannathan bound required $ \\gamma \\approx 50 $ for the random walk but $ \\gamma \\approx 75 $ for the trend-stationary model.\n",
    "\n",
    "These are very different numbers for what is supposed to be the “same” preference parameter.\n",
    "\n",
    "Under detection-error calibration, both models reach the bound at essentially the same detectability level.\n",
    "\n",
    "The apparent model dependence was an artifact of using $ \\gamma $ as the cross-model yardstick.\n",
    "\n",
    "Once we measure robustness concerns in units of statistical detectability, the two consumption specifications tell a single, coherent story.\n",
    "\n",
    "A representative consumer with moderate, difficult-to-dismiss fears about model misspecification behaves as though she has very high risk aversion.\n",
    "\n",
    "The following figure brings together the two key ideas of this section: a small one-step density shift that is hard to detect (left panel) compounds into a large gap in expected log consumption (right panel).\n",
    "\n",
    "At $ p = 0.03 $ both models share the same innovation mean shift $ w $, and the left panel shows that the approximating and worst-case one-step densities nearly coincide.\n",
    "\n",
    "The right panel reveals the cumulative consequence: a per-period shift that is virtually undetectable compounds into a large gap in expected log consumption, especially under random-walk dynamics where each shock has a permanent effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f830f14",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p_star = 0.03\n",
    "θ_star = θ_from_detection_probability(p_star, \"rw\")\n",
    "w_star = w_from_θ(θ_star, \"rw\") \n",
    "σ_ε = rw[\"σ_ε\"]\n",
    "ρ = ts[\"ρ\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ε = np.linspace(-4.5, 4.5, 500)\n",
    "f0 = norm.pdf(ε, 0, 1)\n",
    "fw = norm.pdf(ε, w_star, 1)\n",
    "\n",
    "ax1.fill_between(ε, f0, alpha=0.15, color='k')\n",
    "ax1.plot(ε, f0, 'k', lw=2.5,\n",
    "         label=r'Approximating $\\mathcal{N}(0, 1)$')\n",
    "ax1.fill_between(ε, fw, alpha=0.15, color='C3')\n",
    "ax1.plot(ε, fw, 'C3', lw=2, ls='--',\n",
    "         label=fr'Worst case $\\mathcal{{N}}({w_star:.2f},1)$')\n",
    "\n",
    "peak = norm.pdf(0, 0, 1)\n",
    "ax1.annotate('', xy=(w_star, 0.55 * peak), xytext=(0, 0.55 * peak),\n",
    "             arrowprops=dict(arrowstyle='->', color='C3', lw=1.8))\n",
    "ax1.text(w_star / 2, 0.59 * peak, f'$w = {w_star:.2f}$',\n",
    "         ha='center', fontsize=11, color='C3')\n",
    "\n",
    "ax1.set_xlabel(r'$\\varepsilon_{t+1}$')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend(frameon=False)\n",
    "\n",
    "quarters = np.arange(0, 241)\n",
    "years = quarters / 4\n",
    "\n",
    "gap_rw = 100 * σ_ε * w_star * quarters\n",
    "gap_ts = 100 * σ_ε * w_star * (1 - ρ**quarters) / (1 - ρ)\n",
    "\n",
    "ax2.plot(years, gap_rw, 'C0', lw=2.5, label='Random walk')\n",
    "ax2.plot(years, gap_ts, 'C1', lw=2.5, label='Trend stationary')\n",
    "ax2.fill_between(years, gap_rw, alpha=0.1, color='C0')\n",
    "ax2.fill_between(years, gap_ts, alpha=0.1, color='C1')\n",
    "ax2.axhline(0, color='k', lw=0.5, alpha=0.3)\n",
    "\n",
    "# Endpoint labels\n",
    "ax2.text(61, gap_rw[-1], f'{gap_rw[-1]:.1f}%',\n",
    "         fontsize=10, color='C0', va='center')\n",
    "ax2.text(61, gap_ts[-1], f'{gap_ts[-1]:.1f}%',\n",
    "         fontsize=10, color='C1', va='center')\n",
    "\n",
    "ax2.set_xlabel('Years')\n",
    "ax2.set_ylabel('Gap in expected log consumption (%)')\n",
    "ax2.legend(frameon=False, loc='lower left')\n",
    "ax2.set_xlim(0, 68)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a88753",
   "metadata": {},
   "source": [
    "The next figure makes the “doubts or variability?” question by decomposing the log SDF into two additive components.\n",
    "\n",
    "Taking logs of [(84.18)](#equation-bhs-sdf) gives\n",
    "\n",
    "$$\n",
    "\\log m_{t+1}\n",
    "=\n",
    "\\underbrace{\\log \\beta - \\Delta c_{t+1}}_{\\text{log-utility intertemporal MRS}}\n",
    "+\n",
    "\\underbrace{\\log \\hat g_{t+1}}_{\\text{worst-case distortion}}.\n",
    "$$\n",
    "\n",
    "Under the random-walk model, $ \\Delta c_{t+1} = \\mu + \\sigma_\\varepsilon \\varepsilon_{t+1} $, and the Gaussian distortion [(84.22)](#equation-bhs-ghat-gaussian) gives $ \\log \\hat g_{t+1} = w \\varepsilon_{t+1} - \\tfrac{1}{2}w^2 $.\n",
    "\n",
    "Substituting, we can write\n",
    "\n",
    "$$\n",
    "\\log m_{t+1}\n",
    "=\n",
    "\\bigl(\\log\\beta - \\mu - \\tfrac{1}{2}w^2\\bigr)\n",
    "-\n",
    "(\\sigma_\\varepsilon - w)\\varepsilon_{t+1},\n",
    "$$\n",
    "\n",
    "so the slope of $ \\log m_{t+1} $ in $ \\varepsilon_{t+1} $ is $ \\sigma_\\varepsilon - w $.\n",
    "\n",
    "Since $ w < 0 $, the distortion steepens the SDF relative to what log utility alone would deliver.\n",
    "\n",
    "The figure below reveals how little work log utility does on its own.\n",
    "\n",
    "The intertemporal marginal rate of substitution (IMRS) is nearly flat.\n",
    "\n",
    "At postwar calibrated volatility ($ \\sigma_\\varepsilon = 0.005 $), it contributes almost nothing to the pricing kernel’s slope.\n",
    "\n",
    "The worst-case distortion accounts for virtually all of the SDF’s volatility.\n",
    "\n",
    "What looks like extreme risk aversion ($ \\gamma \\approx 34 $) is really just log utility combined with moderate fears of model misspecification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8cb29",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "θ_cal = θ_from_detection_probability(0.10, \"rw\")\n",
    "γ_cal = γ_from_θ(θ_cal)\n",
    "w_cal = w_from_θ(θ_cal, \"rw\")\n",
    "\n",
    "μ_c, σ_c = rw[\"μ\"], rw[\"σ_ε\"]\n",
    "Δc = np.linspace(μ_c - 3.5 * σ_c, μ_c + 3.5 * σ_c, 300)\n",
    "ε = (Δc - μ_c) / σ_c\n",
    "\n",
    "log_imrs = np.log(β) - Δc\n",
    "log_ghat = w_cal * ε - 0.5 * w_cal**2\n",
    "log_sdf = log_imrs + log_ghat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.plot(100 * Δc, log_imrs, 'C1', lw=2,\n",
    "        label=r'IMRS: $\\log\\beta - \\Delta c$')\n",
    "ax.plot(100 * Δc, log_ghat, 'C3', lw=2, ls='--',\n",
    "        label=r'Distortion: $\\log\\hat{g}$')\n",
    "ax.plot(100 * Δc, log_sdf, 'k', lw=2,\n",
    "        label=r'SDF: $\\log m = \\log\\mathrm{IMRS} + \\log\\hat{g}$')\n",
    "ax.axhline(0, color='k', lw=0.5, alpha=0.3)\n",
    "ax.set_xlabel(r'Consumption growth $\\Delta c_{t+1}$ (%)')\n",
    "ax.set_ylabel('Log SDF component')\n",
    "ax.legend(frameon=False, fontsize=10, loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31377b3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='welfare-experiments'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b8a3c",
   "metadata": {},
   "source": [
    "## What do risk premia measure?\n",
    "\n",
    "Lucas [[2003](https://python.quantecon.org/zreferences.html#id242)] asked how much consumption a representative consumer would sacrifice to eliminate aggregate fluctuations.\n",
    "\n",
    "His answer rested on the assumption that the consumer knows the true data-generating process.\n",
    "\n",
    "The robust reinterpretation opens up a second, quite different thought experiment.\n",
    "\n",
    "Instead of eliminating all randomness, suppose we keep the randomness but remove the consumer’s fear of model misspecification (set $ \\theta = \\infty $).\n",
    "\n",
    "How much would she pay for that relief?\n",
    "\n",
    "To answer this, we seek a permanent proportional reduction $ c_0 - c_0^k $ in initial log consumption that leaves an agent of type $ k $ indifferent between the original risky plan and a deterministic certainty-equivalent path.\n",
    "\n",
    "Because utility is log and the consumption process is Gaussian, these compensations can be computed in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a497ac",
   "metadata": {},
   "source": [
    "### The certainty equivalent path\n",
    "\n",
    "The point of comparison is the deterministic path with the same mean level of consumption as the stochastic plan:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-ce-path'></a>\n",
    "$$\n",
    "c_{t+1}^{ce} - c_t^{ce} = \\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2. \\tag{84.33}\n",
    "$$\n",
    "\n",
    "The additional $ \\tfrac{1}{2}\\sigma_\\varepsilon^2 $ term is a Jensen’s inequality correction.\n",
    "\n",
    "Since $ E[C_t] = E[e^{c_t}] = \\exp(c_0 + t\\mu + \\tfrac{1}{2}t\\sigma_\\varepsilon^2) $, [(84.33)](#equation-bhs-ce-path) matches the mean *level* of consumption at every date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf804b",
   "metadata": {},
   "source": [
    "### Compensating variations from the value functions\n",
    "\n",
    "We use the closed-form value functions derived earlier: [(84.27)](#equation-bhs-w-rw) for the type I/II value function $ W $ and [(84.28)](#equation-bhs-j-rw) for the type III/IV value function $ J $.\n",
    "\n",
    "For the certainty-equivalent path [(84.33)](#equation-bhs-ce-path), there is no risk and no model uncertainty ($ \\theta = \\infty $, so $ \\hat g = 1 $), so the value function reduces to discounted expected log utility.\n",
    "\n",
    "With $ c_t^{ce} = c_0^J + t(\\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2) $, we have\n",
    "\n",
    "$$\n",
    "U^{ce}(c_0^J)\n",
    "= \\sum_{t=0}^{\\infty}\\beta^t c_t^{ce}\n",
    "= \\sum_{t=0}^{\\infty}\\beta^t \\bigl[c_0^J + t(\\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2)\\bigr]\n",
    "= \\frac{c_0^J}{1-\\beta} + \\frac{\\beta(\\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2)}{(1-\\beta)^2},\n",
    "$$\n",
    "\n",
    "where we used $ \\sum_{t \\geq 0}\\beta^t = \\frac{1}{1-\\beta} $ and $ \\sum_{t \\geq 0}t\\beta^t = \\frac{\\beta}{(1-\\beta)^2} $.\n",
    "\n",
    "Factoring gives\n",
    "\n",
    "$$\n",
    "U^{ce}(c_0^J) = \\frac{1}{1-\\beta}\\left[c_0^J + \\frac{\\beta}{1-\\beta}\\left(\\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2\\right)\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018c239",
   "metadata": {},
   "source": [
    "### Type I (Epstein–Zin) compensation\n",
    "\n",
    "Setting $ U^{ce}(c_0^I) = W(x_0) $ from [(84.27)](#equation-bhs-w-rw):\n",
    "\n",
    "$$\n",
    "\\frac{1}{1-\\beta}\\left[c_0^I + \\frac{\\beta}{1-\\beta}\\left(\\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2\\right)\\right]\n",
    "=\n",
    "\\frac{1}{1-\\beta}\\left[c_0 + \\frac{\\beta}{1-\\beta}\\left(\\mu - \\frac{\\sigma_\\varepsilon^2}{2(1-\\beta)\\theta}\\right)\\right].\n",
    "$$\n",
    "\n",
    "Multiplying both sides by $ (1-\\beta) $ and cancelling the common $ \\frac{\\beta\\mu}{1-\\beta} $ terms gives\n",
    "\n",
    "$$\n",
    "c_0^I + \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)}\n",
    "=\n",
    "c_0 - \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta}.\n",
    "$$\n",
    "\n",
    "Solving for $ c_0 - c_0^I $:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-comp-type1'></a>\n",
    "$$\n",
    "c_0 - c_0^I\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)}\\left(1 + \\frac{1}{(1-\\beta)\\theta}\\right)\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon^2\\gamma}{2(1-\\beta)}, \\tag{84.34}\n",
    "$$\n",
    "\n",
    "where the last step uses $ \\gamma = 1 + [(1-\\beta)\\theta]^{-1} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6daab85",
   "metadata": {},
   "source": [
    "### Type II (multiplier) decomposition\n",
    "\n",
    "Because $ W \\equiv U $, we have $ c_0^{II} = c_0^I $ and the total compensation is the same.\n",
    "\n",
    "However, the interpretation differs because we can now decompose it into *risk* and *model uncertainty* components.\n",
    "\n",
    "A type II agent with $ \\theta = \\infty $ (no model uncertainty) has log preferences and requires\n",
    "\n",
    "\n",
    "<a id='equation-bhs-type2-rw-decomp'></a>\n",
    "$$\n",
    "\\Delta c_0^{risk}\n",
    "=\n",
    "\\frac{\\beta \\sigma_\\varepsilon^2}{2(1-\\beta)},\n",
    "\\qquad\n",
    "\\Delta c_0^{uncertainty}\n",
    "=\n",
    "\\frac{\\beta \\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta}. \\tag{84.35}\n",
    "$$\n",
    "\n",
    "The risk term $ \\Delta c_0^{risk} $ is Lucas’s cost of business cycles.\n",
    "\n",
    "At postwar consumption volatility ($ \\sigma_\\varepsilon \\approx 0.005 $), it is negligibly small.\n",
    "\n",
    "The uncertainty term $ \\Delta c_0^{uncertainty} $ captures the additional compensation a type II agent demands for facing model misspecification.\n",
    "\n",
    "With $ \\theta $ in the denominator, this term can be first-order even when the detection-error probability is only moderate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707580cc",
   "metadata": {},
   "source": [
    "### Type III (constraint) compensation\n",
    "\n",
    "For a type III agent, we set $ U^{ce}(c_0^{III}) = J(x_0) $ using the value function $ J $ from [(84.28)](#equation-bhs-j-rw):\n",
    "\n",
    "$$\n",
    "\\frac{1}{1-\\beta}\\left[c_0^{III} + \\frac{\\beta}{1-\\beta}\\left(\\mu + \\tfrac{1}{2}\\sigma_\\varepsilon^2\\right)\\right]\n",
    "=\n",
    "\\frac{1}{1-\\beta}\\left[c_0 + \\frac{\\beta}{1-\\beta}\\left(\\mu - \\frac{\\sigma_\\varepsilon^2}{(1-\\beta)\\theta}\\right)\\right].\n",
    "$$\n",
    "\n",
    "Following the same algebra as for type I but with the doubled uncertainty correction in $ J $:\n",
    "\n",
    "$$\n",
    "c_0 - c_0^{III}\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)} + \\frac{\\beta\\sigma_\\varepsilon^2}{(1-\\beta)^2\\theta}.\n",
    "$$\n",
    "\n",
    "Using $ \\frac{1}{(1-\\beta)\\theta} = \\gamma - 1 $, this simplifies to\n",
    "\n",
    "\n",
    "<a id='equation-bhs-type3-rw-decomp'></a>\n",
    "$$\n",
    "c_0 - c_0^{III}\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)}(2\\gamma - 1). \\tag{84.36}\n",
    "$$\n",
    "\n",
    "The risk component is the same $ \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)} $ as before.\n",
    "\n",
    "The uncertainty component alone is\n",
    "\n",
    "$$\n",
    "c_0^{III}(r) - c_0^{III}\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon^2}{(1-\\beta)^2\\theta},\n",
    "$$\n",
    "\n",
    "which is *twice* the type II uncertainty compensation [(84.35)](#equation-bhs-type2-rw-decomp).\n",
    "\n",
    "The factor of two traces back to the difference between $ W $ and $ J $ noted after [(84.28)](#equation-bhs-j-rw).\n",
    "\n",
    "The entropy rebate $ \\theta N $ in $ W = J + \\theta N $ partially offsets the pessimistic tilt for the type II agent, but not for the type III agent who evaluates consumption purely under the worst-case model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446210b",
   "metadata": {},
   "source": [
    "### Type IV (ex post Bayesian) compensation\n",
    "\n",
    "A type IV agent believes the pessimistic model, so the perceived drift is $ \\tilde\\mu = \\mu - \\sigma_\\varepsilon^2/[(1-\\beta)\\theta] $.\n",
    "\n",
    "The compensation for moving to the certainty-equivalent path is the same as [(84.36)](#equation-bhs-type3-rw-decomp), because this agent ranks plans using the same value function $ J $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142af05d",
   "metadata": {},
   "source": [
    "### Comparison with a risky but free-of-model-uncertainty path\n",
    "\n",
    "The certainty equivalents above compare a risky plan to a deterministic path, thereby eliminating both risk and uncertainty at once.\n",
    "\n",
    "We now describe an alternative measure that isolates compensation for model uncertainty alone by keeping risk intact.\n",
    "\n",
    "The idea is to compare two situations with identical risky consumption for all dates $ t \\geq 1 $, concentrating all compensation for model uncertainty in a single adjustment to date-zero consumption.\n",
    "\n",
    "Specifically, we seek $ c_0^{II}(u) $ that makes a type II agent indifferent between:\n",
    "\n",
    "1. Facing the stochastic plan under $ \\theta < \\infty $ (fear of model misspecification), consuming $ c_0 $ at date zero.  \n",
    "1. Facing the *same* stochastic plan under $ \\theta = \\infty $ (no fear of misspecification), but consuming only $ c_0^{II}(u) < c_0 $ at date zero.  \n",
    "\n",
    "\n",
    "In both cases, continuation consumptions $ c_t $ for $ t \\geq 1 $ are generated by the random walk starting from the *same* $ c_0 $.\n",
    "\n",
    "For the type II agent under $ \\theta < \\infty $, the total value is $ W(c_0) $ from [(84.27)](#equation-bhs-w-rw).\n",
    "\n",
    "For the agent liberated from model uncertainty ($ \\theta = \\infty $), the value is\n",
    "\n",
    "$$\n",
    "c_0^{II}(u) + \\beta E\\left[V^{\\log}(c_1)\\right],\n",
    "$$\n",
    "\n",
    "where $ V^{\\log}(c_t) = \\frac{1}{1-\\beta} \\left[c_t + \\frac{\\beta\\mu}{1-\\beta}\\right] $ is the log-utility value function and $ c_1 = c_0 + \\mu + \\sigma_\\varepsilon \\varepsilon_1 $.\n",
    "\n",
    "Since $ c_1 $ is built from $ c_0 $ (not $ c_0^{II}(u) $), the continuation is\n",
    "\n",
    "$$\n",
    "\\beta E\\left[V^{\\log}(c_1)\\right]\n",
    "= \\frac{\\beta}{1-\\beta} E\\left[c_1 + \\frac{\\beta\\mu}{1-\\beta}\\right]\n",
    "= \\frac{\\beta}{1-\\beta}\\left[c_0 + \\mu + \\frac{\\beta\\mu}{1-\\beta}\\right]\n",
    "= \\frac{\\beta}{1-\\beta}\\left[c_0 + \\frac{\\mu}{1-\\beta}\\right],\n",
    "$$\n",
    "\n",
    "where we used $ E[c_1] = c_0 + \\mu $ (the noise term has zero mean).\n",
    "\n",
    "Expanding gives\n",
    "\n",
    "$$\n",
    "\\beta E\\left[V^{\\log}(c_1)\\right]\n",
    "= \\frac{\\beta c_0}{1-\\beta} + \\frac{\\beta\\mu}{(1-\\beta)^2}.\n",
    "$$\n",
    "\n",
    "Setting $ W(c_0) $ equal to the liberation value and simplifying:\n",
    "\n",
    "$$\n",
    "\\frac{c_0}{1-\\beta} + \\frac{\\beta\\mu}{(1-\\beta)^2} - \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^3\\theta}\n",
    "=\n",
    "c_0^{II}(u) + \\frac{\\beta c_0}{1-\\beta} + \\frac{\\beta\\mu}{(1-\\beta)^2}.\n",
    "$$\n",
    "\n",
    "Because $ \\frac{c_0}{1-\\beta} - \\frac{\\beta c_0}{1-\\beta} = c_0 $, solving for the compensation gives\n",
    "\n",
    "\n",
    "<a id='equation-bhs-comp-type2u'></a>\n",
    "$$\n",
    "c_0 - c_0^{II}(u) = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^3\\theta} = \\frac{\\beta\\sigma_\\varepsilon^2(\\gamma - 1)}{2(1-\\beta)^2}. \\tag{84.37}\n",
    "$$\n",
    "\n",
    "This is $ \\frac{1}{1-\\beta} $ times the uncertainty compensation $ \\Delta c_0^{\\text{uncertainty}} $ from [(84.35)](#equation-bhs-type2-rw-decomp).\n",
    "\n",
    "The extra factor of $ \\frac{1}{1-\\beta} $ arises because all compensation is packed into a single period.\n",
    "\n",
    "Adjusting $ c_0 $ alone must offset the cumulative loss in continuation value that the uncertainty penalty imposes in every future period.\n",
    "\n",
    "An analogous calculation for a **type III** agent, using $ J(c_0) $ from [(84.28)](#equation-bhs-j-rw), gives\n",
    "\n",
    "\n",
    "<a id='equation-bhs-comp-type3u'></a>\n",
    "$$\n",
    "c_0 - c_0^{III}(u) = \\frac{\\beta\\sigma_\\varepsilon^2}{(1-\\beta)^3\\theta} = \\frac{\\beta\\sigma_\\varepsilon^2(\\gamma - 1)}{(1-\\beta)^2}, \\tag{84.38}\n",
    "$$\n",
    "\n",
    "which is $ \\frac{1}{1-\\beta} $ times the type III uncertainty compensation and *twice* the type II compensation [(84.37)](#equation-bhs-comp-type2u), again reflecting the absence of the entropy rebate in $ J $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3d2da",
   "metadata": {},
   "source": [
    "### Summary of welfare compensations (random walk)\n",
    "\n",
    "The following table collects all compensating variations for the random walk model.\n",
    "\n",
    "|Agent|Compensation|Formula|Measures|\n",
    "|:-----------------------:|:-----------------------:|:-----------------------:|:-----------------------:|\n",
    "|I, II|$ c_0 - c_0^{II} $|$ \\frac{\\beta\\sigma_\\varepsilon^2\\gamma}{2(1-\\beta)} $|risk + uncertainty (vs. deterministic)|\n",
    "|II|$ c_0 - c_0^{II}(r) $|$ \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)} $|risk only (vs. deterministic)|\n",
    "|II|$ c_0^{II}(r) - c_0^{II} $|$ \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta} $|uncertainty only (vs. deterministic)|\n",
    "|II|$ c_0 - c_0^{II}(u) $|$ \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^3\\theta} $|uncertainty only (vs. risky path)|\n",
    "|III|$ c_0 - c_0^{III} $|$ \\frac{\\beta\\sigma_\\varepsilon^2(2\\gamma-1)}{2(1-\\beta)} $|risk + uncertainty (vs. deterministic)|\n",
    "|III|$ c_0^{III}(r) - c_0^{III} $|$ \\frac{\\beta\\sigma_\\varepsilon^2}{(1-\\beta)^2\\theta} $|uncertainty only (vs. deterministic)|\n",
    "|III|$ c_0 - c_0^{III}(u) $|$ \\frac{\\beta\\sigma_\\varepsilon^2}{(1-\\beta)^3\\theta} $|uncertainty only (vs. risky path)|\n",
    "The “vs. deterministic” rows use the certainty-equivalent path [(84.33)](#equation-bhs-ce-path) as a benchmark.\n",
    "\n",
    "The “vs. risky path” rows use the risky-but-uncertainty-free comparison of [(84.37)](#equation-bhs-comp-type2u)–[(84.38)](#equation-bhs-comp-type3u)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad54c31",
   "metadata": {},
   "source": [
    "### Trend-stationary formulas\n",
    "\n",
    "For the trend-stationary model, the denominators $ (1-\\beta) $ in the uncertainty terms are replaced by $ (1-\\beta\\rho) $, and the risk terms involve $ (1-\\beta\\rho^2) $:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-ts-compensations'></a>\n",
    "$$\n",
    "\\Delta c_0^{risk,ts} = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta\\rho^2)},\n",
    "\\qquad\n",
    "\\Delta c_0^{unc,ts,II} = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta\\rho)^2\\theta},\n",
    "\\qquad\n",
    "\\Delta c_0^{unc,ts,III} = \\frac{\\beta\\sigma_\\varepsilon^2}{(1-\\beta\\rho)^2\\theta}. \\tag{84.39}\n",
    "$$\n",
    "\n",
    "The qualitative message carries over: the risk component is negligible, and the model-uncertainty component dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc70da5",
   "metadata": {},
   "source": [
    "## Visualizing the welfare decomposition\n",
    "\n",
    "We set $ \\beta = 0.995 $ and calibrate $ \\theta $ so that $ p(\\theta^{-1}) = 0.10 $, a conservative detection-error level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be89bf1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p_star = 0.10\n",
    "θ_star = θ_from_detection_probability(p_star, \"rw\")\n",
    "γ_star = γ_from_θ(θ_star)\n",
    "w_star = w_from_θ(θ_star, \"rw\")\n",
    "\n",
    "# Type II compensations, random walk model\n",
    "comp_risk_only = β * rw[\"σ_ε\"]**2 / (2.0 * (1.0 - β))\n",
    "comp_risk_unc = comp_risk_only + β * rw[\"σ_ε\"]**2 / (2.0 * (1.0 - β)**2 * θ_star)\n",
    "\n",
    "# Two useful decompositions in levels\n",
    "risk_only_pct = 100.0 * (np.exp(comp_risk_only) - 1.0)\n",
    "risk_unc_pct = 100.0 * (np.exp(comp_risk_unc) - 1.0)\n",
    "uncertainty_only_pct = 100.0 * (np.exp(comp_risk_unc - comp_risk_only) - 1.0)\n",
    "\n",
    "print(f\"p*={p_star:.2f}, θ*={θ_star:.4f}, γ*={γ_star:.2f}, w*={w_star:.4f}\")\n",
    "print(f\"risk only compensation (log units): {comp_risk_only:.6f}\")\n",
    "print(f\"risk + uncertainty compensation (log units): {comp_risk_unc:.6f}\")\n",
    "print(f\"risk only compensation (percent): {risk_only_pct:.3f}%\")\n",
    "print(f\"risk + uncertainty compensation (percent): {risk_unc_pct:.3f}%\")\n",
    "print(f\"uncertainty component alone (percent): {uncertainty_only_pct:.3f}%\")\n",
    "\n",
    "h = 250\n",
    "t = np.arange(h + 1)\n",
    "\n",
    "# Baseline approximating model fan\n",
    "mean_base = rw[\"μ\"] * t\n",
    "std_base = rw[\"σ_ε\"] * np.sqrt(t)\n",
    "\n",
    "# Certainty equivalent line from Eq. (47), shifted by compensating variations\n",
    "certainty_slope = rw[\"μ\"] + 0.5 * rw[\"σ_ε\"]**2\n",
    "ce_risk = -comp_risk_only + certainty_slope * t\n",
    "ce_risk_unc = -comp_risk_unc + certainty_slope * t\n",
    "\n",
    "# Alternative models from the ambiguity set in panel B\n",
    "mean_low = (rw[\"μ\"] + rw[\"σ_ε\"] * w_star) * t\n",
    "mean_high = (rw[\"μ\"] - rw[\"σ_ε\"] * w_star) * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff564f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Panel A\n",
    "ax = axes[0]\n",
    "ax.fill_between(t, mean_base - std_base, mean_base + std_base, \n",
    "                alpha=0.25, color=\"tab:blue\")\n",
    "ax.plot(t, ce_risk_unc, lw=2, ls=\"--\", color=\"black\", \n",
    "                label=\"certainty equivalent: risk + uncertainty\")\n",
    "ax.plot(t, ce_risk, lw=2, color=\"tab:orange\", \n",
    "                label=\"certainty equivalent: risk only\")\n",
    "ax.plot(t, mean_base, lw=2, \n",
    "                color=\"tab:blue\", label=\"approximating-model mean\")\n",
    "ax.set_xlabel(\"quarters\")\n",
    "ax.set_ylabel(\"log consumption\")\n",
    "ax.legend(frameon=False, fontsize=8, loc=\"upper left\")\n",
    "\n",
    "# Panel B\n",
    "ax = axes[1]\n",
    "ax.fill_between(t, mean_base - std_base, mean_base + std_base, \n",
    "                            alpha=0.20, color=\"tab:blue\")\n",
    "ax.fill_between(t, mean_low - std_base, mean_low + std_base, \n",
    "                            alpha=0.20, color=\"tab:red\")\n",
    "ax.fill_between(t, mean_high - std_base, mean_high + std_base, \n",
    "                            alpha=0.20, color=\"tab:green\")\n",
    "ax.plot(t, ce_risk_unc, lw=2, ls=\"--\", color=\"black\", \n",
    "                        label=\"certainty equivalent: risk + uncertainty\")\n",
    "ax.plot(t, mean_base, lw=2, color=\"tab:blue\", label=\"approximating-model mean\")\n",
    "ax.plot(t, mean_low, lw=2, color=\"tab:red\", label=\"worst-case-leaning mean\")\n",
    "ax.plot(t, mean_high, lw=2, color=\"tab:green\", label=\"best-case-leaning mean\")\n",
    "ax.set_xlabel(\"quarters\")\n",
    "ax.set_ylabel(\"log consumption\")\n",
    "ax.legend(frameon=False, fontsize=8, loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e82fed",
   "metadata": {},
   "source": [
    "The left panel illustrates the elimination of model uncertainty and risk for a type II agent.\n",
    "\n",
    "The shaded fan shows a one-standard-deviation band for the $ j $-step-ahead conditional distribution of $ c_t $ under the calibrated random-walk model.\n",
    "\n",
    "The dashed line $ c^{II} $ shows the certainty-equivalent path whose date-zero consumption is reduced by $ c_0 - c_0^{II} $, making the type II agent indifferent between this deterministic trajectory and the stochastic plan.\n",
    "\n",
    "It compensates for bearing both risk and model ambiguity.\n",
    "\n",
    "The solid line $ c^r $ shows the certainty equivalent for a type II agent without model uncertainty ($ \\theta = \\infty $), initialized at $ c_0 - c_0^{II}(r) $.\n",
    "\n",
    "At postwar calibrated values this gap is small, so $ c^r $ sits just below the center of the fan.\n",
    "\n",
    "Consistent with Lucas [[2003](https://python.quantecon.org/zreferences.html#id242)], the welfare gains from eliminating well-understood risk are very small.\n",
    "\n",
    "The large welfare gains found by Tallarini [[2000](https://python.quantecon.org/zreferences.html#id132)] can be reinterpreted as arising not from reducing risk, but from reducing model uncertainty.\n",
    "\n",
    "The right panel shows the set of nearby models that the robust consumer guards against.\n",
    "\n",
    "Each shaded fan depicts a one-standard-deviation band for a different model in the ambiguity set.\n",
    "\n",
    "The models are statistically close to the baseline, with detection-error probability $ p = 0.10 $, but imply very different long-run consumption levels.\n",
    "\n",
    "The consumer’s caution against such alternatives accounts for the large certainty-equivalent gap in the left panel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91920ca8",
   "metadata": {},
   "source": [
    "## Welfare gains from removing model uncertainty\n",
    "\n",
    "A type III (constraint-preference) agent evaluates the worst model inside an entropy ball of radius $ \\eta $.\n",
    "\n",
    "As $ \\eta $ grows the set of plausible misspecifications expands, and with it the welfare cost of confronting model uncertainty.\n",
    "\n",
    "Since $ \\eta $ itself is not easy to interpret, we instead index these costs by the associated detection-error probability $ p(\\eta) $.\n",
    "\n",
    "The figure below plots the compensation for removing model uncertainty, measured as a proportion of consumption, against $ p(\\eta) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52133559",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "η_grid = np.linspace(0.0, 5.0, 300)\n",
    "\n",
    "# Use w and η relation, then convert to θ model by model\n",
    "w_abs_grid = np.sqrt(2.0 * (1.0 - β) * η_grid / β)\n",
    "\n",
    "θ_rw_from_η = np.full_like(w_abs_grid, np.inf)\n",
    "θ_ts_from_η = np.full_like(w_abs_grid, np.inf)\n",
    "mask_w = w_abs_grid > 0.0\n",
    "θ_rw_from_η[mask_w] = rw[\"σ_ε\"] / ((1.0 - β) * w_abs_grid[mask_w])\n",
    "θ_ts_from_η[mask_w] = ts[\"σ_ε\"] / ((1.0 - β * ts[\"ρ\"]) * w_abs_grid[mask_w])\n",
    "\n",
    "# Type III uncertainty terms from Table 3\n",
    "gain_rw = np.where(\n",
    "    np.isinf(θ_rw_from_η),\n",
    "    0.0,\n",
    "    β * rw[\"σ_ε\"]**2 / ((1.0 - β)**2 * θ_rw_from_η),\n",
    ")\n",
    "gain_ts = np.where(\n",
    "    np.isinf(θ_ts_from_η),\n",
    "    0.0,\n",
    "    β * ts[\"σ_ε\"]**2 / ((1.0 - β * ts[\"ρ\"])**2 * θ_ts_from_η),\n",
    ")\n",
    "\n",
    "# Convert log compensation to percent of initial consumption in levels\n",
    "gain_rw_pct = 100.0 * (np.exp(gain_rw) - 1.0)\n",
    "gain_ts_pct = 100.0 * (np.exp(gain_ts) - 1.0)\n",
    "\n",
    "# Detection error probabilities implied by η\n",
    "p_eta_pct = 100.0 * norm.cdf(-0.5 * w_abs_grid * np.sqrt(T))\n",
    "order = np.argsort(p_eta_pct)\n",
    "p_plot = p_eta_pct[order]\n",
    "gain_rw_plot = gain_rw_pct[order]\n",
    "gain_ts_plot = gain_ts_pct[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfde1c8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(p_plot, gain_rw_plot, lw=2, label=\"RW type III\")\n",
    "ax.plot(p_plot, gain_ts_plot, lw=2, label=\"TS type III\")\n",
    "ax.set_xlabel(r\"detection error probability $p(\\eta)$ (percent)\")\n",
    "ax.set_ylabel(\"proportion of consumption (percent)\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd934df5",
   "metadata": {},
   "source": [
    "The random-walk model implies somewhat larger costs than the trend-stationary model at the same detection-error probability, but both curves dwarf the classic Lucas cost of business cycles.\n",
    "\n",
    "To put the magnitudes in perspective, Lucas estimated that eliminating all aggregate consumption risk is worth roughly 0.05% of consumption.\n",
    "\n",
    "At detection-error probabilities of 10–20%, the model-uncertainty compensation alone runs to several percent, orders of magnitude larger.\n",
    "\n",
    "Under the robust reading, the large risk premia that Tallarini matched with high $ \\gamma $ are really compensations for bearing model uncertainty, and the implied welfare gains from resolving that uncertainty are correspondingly large.\n",
    "\n",
    "The following contour plot shows how type II (multiplier) compensation varies over two dimensions: the detection-error probability $ p $ and the consumption volatility $ \\sigma_\\varepsilon $.\n",
    "\n",
    "The cross marks the calibrated point ($ p = 0.10 $, $ \\sigma_\\varepsilon = 0.5\\% $).\n",
    "\n",
    "At the calibrated volatility, moving left (lower $ p $, stronger robustness concerns) increases compensation dramatically, while the classic risk-only cost (the $ p = 50\\% $ edge) remains negligible.\n",
    "\n",
    "A comparison of the two panels reveals that the random-walk model generates much larger welfare costs than the trend-stationary model at the same ($ p $, $ \\sigma_\\varepsilon $), because permanent shocks compound the worst-case drift indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2178f40",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p_grid = np.linspace(0.02, 0.49, 300)\n",
    "σ_grid = np.linspace(0.001, 0.015, 300)\n",
    "P, Σ = np.meshgrid(p_grid, σ_grid)\n",
    "\n",
    "W_abs = -2 * norm.ppf(P) / np.sqrt(T)\n",
    "\n",
    "# RW: total type II = βσ²γ / [2(1-β)] \n",
    "Γ_rw = 1 + W_abs / Σ\n",
    "comp_rw = 100 * (np.exp(β * Σ**2 * Γ_rw / (2 * (1 - β))) - 1)\n",
    "\n",
    "# TS: risk + uncertainty \n",
    "ρ_val = ts[\"ρ\"]\n",
    "risk_ts = β * Σ**2 / (2 * (1 - β * ρ_val**2))\n",
    "unc_ts = β * Σ * W_abs / (2 * (1 - β * ρ_val))\n",
    "comp_ts = 100 * (np.exp(risk_ts + unc_ts) - 1)\n",
    "\n",
    "levels = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5.5), sharey=True)\n",
    "\n",
    "for ax, comp, title in [(ax1, comp_rw, 'Random walk'),\n",
    "                         (ax2, comp_ts, 'Trend stationary')]:\n",
    "    cf = ax.contourf(100 * P, 100 * Σ, comp, levels=levels,\n",
    "                     cmap='Blues', extend='both')\n",
    "    cs = ax.contour(100 * P, 100 * Σ, comp, levels=levels,\n",
    "                    colors='k', linewidths=0.5)\n",
    "    ax.clabel(cs, fmt='%g%%', fontsize=8)\n",
    "    ax.plot(10, 0.5, 'x', markersize=14, color='w',\n",
    "            mec='k', mew=1, zorder=5)\n",
    "    ax.set_xlabel(r'Detection-error probability $p$ (%)')\n",
    "    ax.set_title(title)\n",
    "\n",
    "ax1.set_ylabel(r'Consumption volatility $\\sigma_\\varepsilon$ (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81be71e",
   "metadata": {},
   "source": [
    "## Learning doesn’t eliminate misspecification fears\n",
    "\n",
    "A reasonable question  arises: if the consumer has 235 quarters of data,  can’t she learn enough to dismiss the worst-case model?\n",
    "\n",
    "The answer is no.\n",
    "\n",
    "This is because   the drift is a low-frequency feature that is very  hard to pin down.\n",
    "\n",
    "Estimating the mean of a random walk to the precision needed to reject small but economically meaningful shifts requires far more data than estimating volatility precisely does.\n",
    "\n",
    "The following figure makes this point concrete.\n",
    "\n",
    "We measure consumption as real personal consumption expenditures on nondurable goods and services, deflated by its implicit chain price deflator and expressed in per-capita terms using the civilian noninstitutional population aged 16+.\n",
    "\n",
    "The construction uses four FRED series:\n",
    "\n",
    "````````|FRED series|Description|\n",
    "|:------------------------------------------------:|:------------------------------------------------:|\n",
    "|PCND|Nominal PCE: nondurable goods (billions of \\$, SAAR, quarterly)|\n",
    "|PCESV|Nominal PCE: services (billions of \\$, SAAR, quarterly)|\n",
    "|DPCERD3Q086SBEA|PCE implicit price deflator (index 2017 $ = 100 $, quarterly)|\n",
    "|CNP16OV|Civilian noninstitutional population, 16+ (thousands, monthly)|\n",
    "We use nominal rather than chained-dollar components because chained-dollar series are not additive.\n",
    "\n",
    "Chain-weighted indices update their base-period expenditure weights every period, so components deflated with different price changes do not sum to the separately chained aggregate.\n",
    "\n",
    "Adding nominal series and deflating the sum with a single price index avoids this problem.\n",
    "\n",
    "The processing pipeline is:\n",
    "\n",
    "1. Add nominal nondurables and services: $ C_t^{nom} = C_t^{nd} + C_t^{sv} $.  \n",
    "1. Deflate by the PCE price index: $ C_t^{real} = C_t^{nom} / (P_t / 100) $.  \n",
    "1. Convert to per-capita: divide by the quarterly average of the monthly population series.  \n",
    "1. Compute log consumption: $ c_t = \\log C_t^{real,pc} $.  \n",
    "\n",
    "\n",
    "When we plot *levels* of log consumption, we align the time index to 1948Q1–2006Q4, which yields $ T+1 = 236 $ quarterly observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865883f7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "start_date = dt.datetime(1947, 1, 1)\n",
    "end_date = dt.datetime(2007, 1, 1)\n",
    "\n",
    "\n",
    "def _read_fred_series(series_id, start_date, end_date):\n",
    "    series = web.DataReader(series_id, \"fred\", start_date, end_date)[series_id]\n",
    "    series = pd.to_numeric(series, errors=\"coerce\").dropna().sort_index()\n",
    "    if series.empty:\n",
    "        raise ValueError(f\"FRED series '{series_id}' returned no data in sample window\")\n",
    "    return series\n",
    "\n",
    "\n",
    "# Fetch nominal PCE components, deflator, and population from FRED\n",
    "nom_nd = _read_fred_series(\"PCND\", start_date, end_date)        # quarterly, 1947–\n",
    "nom_sv = _read_fred_series(\"PCESV\", start_date, end_date)       # quarterly, 1947–\n",
    "defl = _read_fred_series(\"DPCERD3Q086SBEA\", start_date, end_date)  # quarterly, 1947–\n",
    "pop_m = _read_fred_series(\"CNP16OV\", start_date, end_date)      # monthly, 1948–\n",
    "\n",
    "# Step 1: add nominal nondurables + services\n",
    "nom_total = nom_nd + nom_sv\n",
    "\n",
    "# Step 2: deflate by PCE implicit price deflator (index 2017=100)\n",
    "real_total = nom_total / (defl / 100.0)\n",
    "\n",
    "# Step 3: convert to per-capita (population is monthly, so average to quarterly)\n",
    "pop_q = pop_m.resample(\"QS\").mean()\n",
    "real_pc = (real_total / pop_q).dropna()\n",
    "\n",
    "# Restrict to sample period 1948Q1–2006Q4\n",
    "real_pc = real_pc.loc[\"1948-01-01\":\"2006-12-31\"].dropna()\n",
    "\n",
    "if real_pc.empty:\n",
    "    raise RuntimeError(\n",
    "        \"FRED returned no usable observations after alignment/filtering\")\n",
    "\n",
    "# Step 4: log consumption\n",
    "log_c_data = np.log(real_pc.to_numpy(dtype=float).reshape(-1))\n",
    "years_data = (\n",
    "    real_pc.index.year \n",
    "    + (real_pc.index.month - 1) / 12.0).to_numpy(dtype=float)\n",
    "\n",
    "print(f\"Fetched {len(log_c_data)} quarterly observations from FRED\")\n",
    "print(f\"Sample: {years_data[0]:.1f} – {years_data[-1] + 0.25:.1f}\")\n",
    "print(f\"Observations: {len(log_c_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac61be",
   "metadata": {},
   "source": [
    "We can verify Table 2 by computing sample moments of log consumption growth from our FRED data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e9346",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Growth rates: 1948Q2 to 2006Q4 (T = 235 quarters)\n",
    "diff_c = np.diff(log_c_data)\n",
    "\n",
    "μ_hat = diff_c.mean()\n",
    "σ_hat = diff_c.std(ddof=1)\n",
    "\n",
    "print(\"Sample estimates from FRED data vs Table 2:\")\n",
    "print(f\"  μ   = {μ_hat:.5f}   (Table 2 RW: {rw['μ']:.5f})\")\n",
    "print(f\"  σ_ε = {σ_hat:.4f}    (Table 2: {rw['σ_ε']:.4f})\")\n",
    "print(f\"  T   = {len(diff_c)} quarters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426769d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p_fig6 = 0.20\n",
    "\n",
    "rw_fig6 = dict(μ=μ_hat, σ_ε=σ_hat)\n",
    "w_fig6 = 2.0 * norm.ppf(p_fig6) / np.sqrt(T)\n",
    "\n",
    "c = log_c_data\n",
    "years = years_data\n",
    "\n",
    "t6 = np.arange(T + 1)\n",
    "μ_approx = rw_fig6[\"μ\"]\n",
    "μ_worst = rw_fig6[\"μ\"] + rw_fig6[\"σ_ε\"] * w_fig6\n",
    "\n",
    "a_approx = (c - μ_approx * t6).mean()\n",
    "a_worst = (c - μ_worst * t6).mean()\n",
    "line_approx = a_approx + μ_approx * t6\n",
    "line_worst = a_worst + μ_worst * t6\n",
    "\n",
    "p_right = np.linspace(0.01, 0.50, 500)\n",
    "w_right = 2.0 * norm.ppf(p_right) / np.sqrt(T)\n",
    "μ_worst_right = rw_fig6[\"μ\"] + rw_fig6[\"σ_ε\"] * w_right\n",
    "\n",
    "μ_se = rw_fig6[\"σ_ε\"] / np.sqrt(T)\n",
    "upper_band = rw_fig6[\"μ\"] + 2.0 * μ_se\n",
    "lower_band = rw_fig6[\"μ\"] - 2.0 * μ_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2ff7c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(years, c, lw=2, color=\"tab:blue\", label=\"log consumption\")\n",
    "ax.plot(years, line_approx, lw=2, ls=\"--\", \n",
    "            color=\"black\", label=\"approximating model\")\n",
    "ax.plot(\n",
    "    years,\n",
    "    line_worst,\n",
    "    lw=2,\n",
    "    ls=\":\",\n",
    "    color=\"black\",\n",
    "    label=rf\"wc model $p(\\theta^{{-1}})={p_fig6:.1f}$\",\n",
    ")\n",
    "ax.set_xlabel(\"year\")\n",
    "ax.set_ylabel(\"log consumption\")\n",
    "ax.legend(frameon=False, fontsize=8, loc=\"upper left\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    100.0 * p_right,\n",
    "    1_000.0 * μ_worst_right,\n",
    "    lw=2,\n",
    "    color=\"tab:red\",\n",
    "    label=r\"$\\mu + \\sigma_\\varepsilon w(\\theta)$\",\n",
    ")\n",
    "ax.axhline(1_000.0 * rw_fig6[\"μ\"], lw=2, color=\"black\", label=r\"$\\hat\\mu$\")\n",
    "ax.axhline(1_000.0 * upper_band, lw=2, ls=\"--\", \n",
    "                    color=\"gray\", label=r\"$\\hat\\mu \\pm 2\\hat s.e.$\")\n",
    "ax.axhline(1_000.0 * lower_band, lw=2, ls=\"--\", color=\"gray\")\n",
    "ax.set_xlabel(\"detection error probability (percent)\")\n",
    "ax.set_ylabel(r\"mean consumption growth ($\\times 10^{-3}$)\")\n",
    "ax.legend(frameon=False, fontsize=8)\n",
    "ax.set_xlim(0.0, 50.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da088f",
   "metadata": {},
   "source": [
    "In the left panel, postwar U.S. log consumption is shown alongside two deterministic trend lines: the approximating-model drift $ \\mu $ and the worst-case drift $ \\mu + \\sigma_\\varepsilon w(\\theta) $ for $ p(\\theta^{-1}) = 0.20 $.\n",
    "\n",
    "The two trends are close enough that, even with six decades of data, it is hard to distinguish them by eye.\n",
    "\n",
    "In the right panel, as the detection-error probability rises (the two models become harder to tell apart), the worst-case mean growth rate drifts back toward $ \\hat\\mu $.\n",
    "\n",
    "The dashed gray lines mark a two-standard-error band around the maximum-likelihood estimate of $ \\mu $.\n",
    "\n",
    "Even at detection probabilities in the 5–20% range, the worst-case drift remains inside (or very near) this confidence band.\n",
    "\n",
    "Drift distortions that are economically large, large enough to generate substantial model-uncertainty premia, are statistically small relative to sampling uncertainty in $ \\hat\\mu $.\n",
    "\n",
    "Robustness concerns persist despite long histories precisely because the low-frequency features that matter most for pricing are the hardest to estimate precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fb9d9",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "The title of this lecture poses a question: are large risk premia prices of *variability* (atemporal risk aversion) or prices of *doubts* (model uncertainty)?\n",
    "\n",
    "Asset-pricing data alone cannot settle the question, because the two interpretations are observationally equivalent.\n",
    "\n",
    "But the choice of interpretation matters for the conclusions we draw.\n",
    "\n",
    "Under the risk-aversion reading, high Sharpe ratios imply that consumers would pay a great deal to smooth known aggregate consumption fluctuations.\n",
    "\n",
    "Under the robustness reading, those same Sharpe ratios tell us that consumers would pay a great deal to resolve uncertainty about which probability model actually governs consumption growth.\n",
    "\n",
    "Three features of the analysis support the robustness reading:\n",
    "\n",
    "1. Detection-error probabilities provide a more stable calibration language than $ \\gamma $.  \n",
    "  - The two consumption models that required very different $ \\gamma $ values to match the data yield nearly identical pricing implications when indexed by detectability.  \n",
    "1. The welfare gains implied by asset prices decompose overwhelmingly into a model-uncertainty component, with the pure risk component remaining small, consistent with Lucas’s original finding.  \n",
    "1. The drift distortions that drive pricing are small enough to hide inside standard-error bands, so finite-sample learning cannot eliminate the consumer’s fears.  \n",
    "\n",
    "\n",
    "Whether one ultimately prefers the risk or the uncertainty interpretation, the framework clarifies that the question is not about the size of risk premia but about the economic object those premia measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40e169",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The following exercises ask you to fill in several derivation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb9ad0",
   "metadata": {},
   "source": [
    "## Exercise 84.1\n",
    "\n",
    "Let $ R_{t+1} $ be an $ n \\times 1 $ vector of gross returns with unconditional mean $ E(R) $ and covariance matrix $ \\Sigma_R $.\n",
    "\n",
    "Let $ m_{t+1} $ be a stochastic discount factor satisfying $ \\mathbf{1} = E[m_{t+1} R_{t+1}] $.\n",
    "\n",
    "1. Use the covariance decomposition $ E[mR] = E[m] E[R] + \\operatorname{cov}(m,R) $ to show that $ \\operatorname{cov}(m,R) = \\mathbf{1} - E[m] E[R] =: b $.  \n",
    "1. For a portfolio with weight vector $ \\alpha $ and return $ R^p = \\alpha^\\top R $, show that $ \\operatorname{cov}(m, R^p) = \\alpha^\\top b $.  \n",
    "1. Apply the Cauchy–Schwarz inequality to the pair $ (m, R^p) $ to obtain $ |\\alpha^\\top b| \\leq \\sigma(m)\\sqrt{\\alpha^\\top \\Sigma_R\\alpha} $.  \n",
    "1. Maximize the ratio $ |\\alpha^\\top b|/\\sqrt{\\alpha^\\top \\Sigma_R \\alpha} $ over $ \\alpha $ and show that the maximum is $ \\sqrt{b^\\top \\Sigma_R^{-1} b} $, attained at $ \\alpha^\\star = \\Sigma_R^{-1}b $.  \n",
    "1. Conclude that $ \\sigma(m) \\geq \\sqrt{b^\\top \\Sigma_R^{-1} b} $, which is [(84.5)](#equation-bhs-hj-unconditional).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745a8bb",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**Part 1.** From $ \\mathbf{1} = E[mR] = E[m] E[R] + \\operatorname{cov}(m,R) $, rearranging gives $ \\operatorname{cov}(m,R) = \\mathbf{1} - E[m] E[R]= b $.\n",
    "\n",
    "**Part 2.** The portfolio return is $ R^p = \\alpha^\\top R $, so\n",
    "\n",
    "$$\n",
    "\\operatorname{cov}(m, R^p) = \\operatorname{cov}(m, \\alpha^\\top R) = \\alpha^\\top \\operatorname{cov}(m, R) = \\alpha^\\top b.\n",
    "$$\n",
    "\n",
    "**Part 3.**\n",
    "Applying the Cauchy–Schwarz inequality to $ (m, R^p) $:\n",
    "\n",
    "$$\n",
    "|\\alpha^\\top b| = |\\operatorname{cov}(m, R^p)| \\leq \\sigma(m) \\sigma(R^p) = \\sigma(m) \\sqrt{\\alpha^\\top \\Sigma_R \\alpha}.\n",
    "$$\n",
    "\n",
    "**Part 4.** Rearranging Part 3 gives\n",
    "\n",
    "$$\n",
    "\\frac{|\\alpha^\\top b|}{\\sqrt{\\alpha^\\top \\Sigma_R \\alpha}} \\leq \\sigma(m).\n",
    "$$\n",
    "\n",
    "To maximize the left-hand side over $ \\alpha $, define the $ \\Sigma_R $-inner product $ \\langle u, v \\rangle_{\\Sigma} = u^\\top \\Sigma_R v $.\n",
    "\n",
    "Inserting $ I = \\Sigma_R \\Sigma_R^{-1} $ gives\n",
    "\n",
    "$$\n",
    "\\alpha^\\top b\n",
    "= \\alpha^\\top (\\Sigma_R \\Sigma_R^{-1}) b\n",
    "= (\\alpha^\\top \\Sigma_R)(\\Sigma_R^{-1} b)\n",
    "= \\langle \\alpha, \\Sigma_R^{-1}b \\rangle_{\\Sigma}.\n",
    "$$\n",
    "\n",
    "Cauchy–Schwarz in this inner product gives\n",
    "\n",
    "$$\n",
    "|\\langle \\alpha, \\Sigma_R^{-1}b \\rangle_{\\Sigma}|\n",
    "\\leq\n",
    "\\sqrt{\\langle \\alpha, \\alpha \\rangle_{\\Sigma}}\\sqrt{\\langle \\Sigma_R^{-1}b, \\Sigma_R^{-1}b \\rangle_{\\Sigma}}\n",
    "=\n",
    "\\sqrt{\\alpha^\\top \\Sigma_R \\alpha} \\sqrt{b^\\top \\Sigma_R^{-1} b},\n",
    "$$\n",
    "\n",
    "with equality when $ \\alpha \\propto \\Sigma_R^{-1} b $.\n",
    "\n",
    "Substituting $ \\alpha^\\star = \\Sigma_R^{-1} b $ verifies\n",
    "\n",
    "$$\n",
    "\\max_\\alpha \\frac{|\\alpha^\\top b|}{\\sqrt{\\alpha^\\top \\Sigma_R \\alpha}} = \\sqrt{b^\\top \\Sigma_R^{-1} b}.\n",
    "$$\n",
    "\n",
    "**Part 5.** Combining Parts 3 and 4 gives $ \\sigma(m) \\geq \\sqrt{b^\\top \\Sigma_R^{-1} b} $, which is [(84.5)](#equation-bhs-hj-unconditional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d514ca",
   "metadata": {},
   "source": [
    "## Exercise 84.2\n",
    "\n",
    "Combine the SDF representation [(84.18)](#equation-bhs-sdf) with the random-walk consumption dynamics and the Gaussian mean-shift distortion to derive closed-form SDF moments.\n",
    "\n",
    "1. Show that $ \\log m_{t+1} $ is normally distributed under the approximating model and compute its mean and variance in terms of $ (\\beta,\\mu,\\sigma_\\varepsilon,w) $.  \n",
    "1. Use lognormal moments to derive expressions for $ E[m] $ and $ \\sigma(m)/E[m] $.  \n",
    "1. Use the parameter mapping $ \\theta = [(1-\\beta)(\\gamma-1)]^{-1} $ and the associated $ w $ to obtain closed-form expressions for the random-walk model.  \n",
    "1. Explain why $ E[m] $ stays roughly constant while $ \\sigma(m)/E[m] $ grows linearly with $ \\gamma $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355249f",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Under the random walk,\n",
    "\n",
    "$$\n",
    "c_{t+1}-c_t=\\mu+\\sigma_\\varepsilon \\varepsilon_{t+1}\n",
    "$$\n",
    "\n",
    "with $ \\varepsilon_{t+1}\\sim\\mathcal{N}(0,1) $ under the approximating model.\n",
    "\n",
    "Using [(84.18)](#equation-bhs-sdf) and the Gaussian distortion\n",
    "\n",
    "$$\n",
    "\\hat g_{t+1}=\\exp \\left(w\\varepsilon_{t+1}-\\tfrac{1}{2}w^2\\right),\n",
    "$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$\n",
    "m_{t+1}\n",
    "=\n",
    "\\beta \\exp \\left(-(c_{t+1}-c_t)\\right)\\hat g_{t+1}\n",
    "=\n",
    "\\beta \\exp \\left(-\\mu-\\sigma_\\varepsilon\\varepsilon_{t+1}\\right)\\exp \\left(w\\varepsilon_{t+1}-\\frac{1}{2}w^2\\right).\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\log m_{t+1}\n",
    "=\n",
    "\\log\\beta-\\mu-\\frac{1}{2}w^2 + (w-\\sigma_\\varepsilon)\\varepsilon_{t+1},\n",
    "$$\n",
    "\n",
    "which is normal with mean\n",
    "\n",
    "$$\n",
    "E[\\log m]=\\log\\beta-\\mu-\\tfrac{1}{2}w^2\n",
    "$$\n",
    "\n",
    "and variance\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\log m)=(w-\\sigma_\\varepsilon)^2.\n",
    "$$\n",
    "\n",
    "For a lognormal random variable,\n",
    "\n",
    "$$\n",
    "E[m]=\\exp(E[\\log m]+\\tfrac{1}{2}\\operatorname{Var}(\\log m))\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sigma(m)/E[m]=\\sqrt{e^{\\operatorname{Var}(\\log m)}-1}.\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "E[m]\n",
    "=\n",
    "\\beta\\exp\\left(\n",
    "-\\mu-\\frac{1}{2}w^2+\\frac{1}{2}(w-\\sigma_\\varepsilon)^2\n",
    "\\right)\n",
    "=\n",
    "\\beta\\exp\\left(-\\mu+\\frac{\\sigma_\\varepsilon^2}{2}-\\sigma_\\varepsilon w\\right),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma(m)}{E[m]}\n",
    "=\n",
    "\\sqrt{\\exp\\left((w-\\sigma_\\varepsilon)^2\\right)-1}.\n",
    "$$\n",
    "\n",
    "Now use $ w_{\\text{RW}}(\\theta)=-\\sigma_\\varepsilon/[(1-\\beta)\\theta] $ from [(84.23)](#equation-bhs-w-formulas) and\n",
    "$ \\theta=[(1-\\beta)(\\gamma-1)]^{-1} $ to get $ w=-\\sigma_\\varepsilon(\\gamma-1) $.\n",
    "Then\n",
    "\n",
    "$$\n",
    "-\\sigma_\\varepsilon w=\\sigma_\\varepsilon^2(\\gamma-1)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "(w-\\sigma_\\varepsilon)^2 = (-\\sigma_\\varepsilon\\gamma)^2=\\sigma_\\varepsilon^2\\gamma^2.\n",
    "$$\n",
    "\n",
    "Substituting gives the closed-form expressions for the random-walk model:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-em-rw'></a>\n",
    "$$\n",
    "E[m] = \\beta \\exp\\left[-\\mu + \\frac{\\sigma_\\varepsilon^2}{2}(2\\gamma - 1)\\right], \\tag{84.40}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-bhs-sigma-rw'></a>\n",
    "$$\n",
    "\\frac{\\sigma(m)}{E[m]} = \\sqrt{\\exp\\left(\\sigma_\\varepsilon^2 \\gamma^2\\right) - 1}. \\tag{84.41}\n",
    "$$\n",
    "\n",
    "Notice that in [(84.40)](#equation-bhs-em-rw), because $ \\sigma_\\varepsilon $ is small ($ \\approx 0.005 $), the term $ \\frac{\\sigma_\\varepsilon^2}{2}(2\\gamma-1) $ grows slowly with $ \\gamma $, keeping $ E[m] $ roughly constant near $ 1/(1+r^f) $.\n",
    "\n",
    "Meanwhile [(84.41)](#equation-bhs-sigma-rw) shows that $ \\sigma(m)/E[m] \\approx \\sigma_\\varepsilon \\gamma $ grows linearly with $ \\gamma $.\n",
    "\n",
    "This is how Epstein–Zin preferences push volatility toward the HJ bound without distorting the risk-free rate.\n",
    "\n",
    "An analogous calculation for the trend-stationary model yields:\n",
    "\n",
    "\n",
    "<a id='equation-bhs-em-ts'></a>\n",
    "$$\n",
    "E[m] = \\beta \\exp\\left[-\\mu + \\frac{\\sigma_\\varepsilon^2}{2}\\left(1 - \\frac{2(1-\\beta)(1-\\gamma)}{1-\\beta\\rho} + \\frac{1-\\rho}{1+\\rho}\\right)\\right], \\tag{84.42}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-bhs-sigma-ts'></a>\n",
    "$$\n",
    "\\frac{\\sigma(m)}{E[m]} = \\sqrt{\\exp\\left[\\sigma_\\varepsilon^2\\left(\\left(\\frac{(1-\\beta)(1-\\gamma)}{1-\\beta\\rho} - 1\\right)^{2} + \\frac{1-\\rho}{1+\\rho}\\right)\\right] - 1}. \\tag{84.43}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f0ea5",
   "metadata": {},
   "source": [
    "## Exercise 84.3\n",
    "\n",
    "Starting from the type I recursion [(84.9)](#equation-bhs-type1-recursion) and the definitions of $ U_t $ and $ \\theta $ in [(84.10)](#equation-bhs-ut-def)–[(84.11)](#equation-bhs-theta-def), derive the risk-sensitive recursion [(84.12)](#equation-bhs-risk-sensitive).\n",
    "\n",
    "Verify that as $ \\gamma \\to 1 $ (equivalently $ \\theta \\to \\infty $), the recursion converges to standard discounted expected log utility $ U_t = c_t + \\beta E_t U_{t+1} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3ed03",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Start from the type I recursion [(84.9)](#equation-bhs-type1-recursion) and write\n",
    "\n",
    "$$\n",
    "(V_{t+1})^{1-\\gamma} = \\exp\\bigl((1-\\gamma)\\log V_{t+1}\\bigr).\n",
    "$$\n",
    "\n",
    "Using $ \\log V_t = (1-\\beta)U_t $ from [(84.10)](#equation-bhs-ut-def), we obtain\n",
    "\n",
    "$$\n",
    "(1-\\beta)U_t\n",
    "=\n",
    "(1-\\beta)c_t\n",
    "+\n",
    "\\frac{\\beta}{1-\\gamma}\\log E_t\\left[\\exp\\bigl((1-\\gamma)(1-\\beta)U_{t+1}\\bigr)\\right].\n",
    "$$\n",
    "\n",
    "Divide by $ (1-\\beta) $ and use [(84.11)](#equation-bhs-theta-def),\n",
    "\n",
    "$$\n",
    "\\theta = -\\bigl[(1-\\beta)(1-\\gamma)\\bigr]^{-1}.\n",
    "$$\n",
    "\n",
    "Then $ (1-\\gamma)(1-\\beta)=-1/\\theta $ and $ \\beta/[(1-\\beta)(1-\\gamma)]=-\\beta\\theta $, so\n",
    "\n",
    "$$\n",
    "U_t\n",
    "=\n",
    "c_t - \\beta\\theta \\log E_t \\left[\\exp \\left(-\\frac{U_{t+1}}{\\theta}\\right)\\right],\n",
    "$$\n",
    "\n",
    "which is [(84.12)](#equation-bhs-risk-sensitive).\n",
    "\n",
    "For $ \\theta\\to\\infty $ (equivalently $ \\gamma\\to 1 $), use the expansion\n",
    "\n",
    "$$\n",
    "\\exp(-U_{t+1}/\\theta)=1-U_{t+1}/\\theta+o(1/\\theta).\n",
    "$$\n",
    "\n",
    "Taking expectations,\n",
    "\n",
    "$$\n",
    "E_t[\\exp(-U_{t+1}/\\theta)] = 1 - E_t[U_{t+1}]/\\theta + o(1/\\theta).\n",
    "$$\n",
    "\n",
    "Applying $ \\log(1+x) = x + o(x) $ with $ x = -E_t[U_{t+1}]/\\theta + o(1/\\theta) $,\n",
    "\n",
    "$$\n",
    "\\log E_t[\\exp(-U_{t+1}/\\theta)]\n",
    "=\n",
    "-E_t[U_{t+1}]/\\theta + o(1/\\theta),\n",
    "$$\n",
    "\n",
    "so $ -\\theta\\log E_t[\\exp(-U_{t+1}/\\theta)] \\to E_t[U_{t+1}] $ as\n",
    "$ \\theta\\to\\infty $ and the recursion converges to\n",
    "\n",
    "$$\n",
    "U_t = c_t + \\beta E_t U_{t+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a07adb",
   "metadata": {},
   "source": [
    "## Exercise 84.4\n",
    "\n",
    "Consider the type II Bellman equation [(84.16)](#equation-bhs-bellman-type2).\n",
    "\n",
    "1. Use a Lagrange multiplier to impose the normalization constraint $ \\int g(\\varepsilon) \\pi(\\varepsilon) d\\varepsilon = 1 $.  \n",
    "1. Derive the first-order condition for $ g(\\varepsilon) $ and show that the minimizer is the exponential tilt in [(84.17)](#equation-bhs-ghat).  \n",
    "1. Substitute your minimizing $ g $ back into [(84.16)](#equation-bhs-bellman-type2) to recover the risk-sensitive Bellman equation [(84.13)](#equation-bhs-bellman-type1).  \n",
    "\n",
    "\n",
    "Conclude that $ W(x) \\equiv U(x) $ for consumption plans in $ \\mathcal{C}(A,B,H;x_0) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a53f8",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Fix $ x $ and write $ W'(\\varepsilon) := W(Ax + B\\varepsilon) $ for short.\n",
    "\n",
    "Form the Lagrangian\n",
    "\n",
    "$$\n",
    "\\mathcal{L}[g,\\lambda]\n",
    "=\n",
    "\\beta \\int \\Bigl[g(\\varepsilon)W'(\\varepsilon) + \\theta g(\\varepsilon)\\log g(\\varepsilon)\\Bigr]\\pi(\\varepsilon)d\\varepsilon\n",
    "+\n",
    "\\lambda\\left(\\int g(\\varepsilon)\\pi(\\varepsilon) d\\varepsilon - 1\\right).\n",
    "$$\n",
    "\n",
    "The pointwise first-order condition for $ g(\\varepsilon) $ is\n",
    "\n",
    "$$\n",
    "0\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial g(\\varepsilon)}\n",
    "=\n",
    "\\beta\\Bigl[W'(\\varepsilon) + \\theta(1+\\log g(\\varepsilon))\\Bigr]\\pi(\\varepsilon)\n",
    "+\n",
    "\\lambda\\pi(\\varepsilon),\n",
    "$$\n",
    "\n",
    "so (dividing by $ \\beta\\pi(\\varepsilon) $)\n",
    "\n",
    "$$\n",
    "\\log g(\\varepsilon)\n",
    "=\n",
    "-\\frac{W'(\\varepsilon)}{\\theta} - 1 - \\frac{\\lambda}{\\beta\\theta}.\n",
    "$$\n",
    "\n",
    "Exponentiating yields $ g(\\varepsilon)=K\\exp(-W'(\\varepsilon)/\\theta) $ where $ K = \\exp(-1 - \\lambda/(\\beta\\theta)) $ is a constant that does not depend on $ \\varepsilon $.\n",
    "\n",
    "To pin down $ K $, impose the normalization $ \\int g(\\varepsilon)\\pi(\\varepsilon)d\\varepsilon=1 $:\n",
    "\n",
    "$$\n",
    "1 = K \\int \\exp \\left(-\\frac{W(Ax+B\\varepsilon)}{\\theta}\\right)\\pi(\\varepsilon) d\\varepsilon,\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "K^{-1}\n",
    "=\n",
    "\\int \\exp\\left(-\\frac{W(Ax+B\\varepsilon)}{\\theta}\\right)\\pi(\\varepsilon) d\\varepsilon.\n",
    "$$\n",
    "\n",
    "Substituting $ K^{-1} $ into the denominator of $ g = K\\exp(-W'/\\theta) $ gives the minimizer:\n",
    "\n",
    "$$\n",
    "g^*(\\varepsilon)\n",
    "=\n",
    "\\frac{\\exp\\left(-W(Ax+B\\varepsilon)/\\theta\\right)}{\n",
    "    \\int \\exp\\left(-W(Ax+B\\tilde\\varepsilon)/\\theta\\right)\\pi(\\tilde\\varepsilon) d\\tilde\\varepsilon}.\n",
    "$$\n",
    "\n",
    "This has exactly the same form as the distortion $ \\hat g_{t+1} = \\exp(-U_{t+1}/\\theta)/E_t[\\exp(-U_{t+1}/\\theta)] $ that appears in the type I SDF [(84.14)](#equation-bhs-sdf-ut), with $ W $ in place of $ U $.\n",
    "\n",
    "Once we verify below that $ W \\equiv U $, the minimizer $ g^* $ and the SDF distortion $ \\hat g $ coincide, which is [(84.17)](#equation-bhs-ghat).\n",
    "\n",
    "To substitute back, define\n",
    "\n",
    "$$\n",
    "Z(x):=\\int \\exp(-W(Ax+B\\varepsilon)/\\theta)\\pi(\\varepsilon) d\\varepsilon.\n",
    "$$\n",
    "\n",
    "Then $ \\hat g(\\varepsilon)=\\exp(-W(Ax+B\\varepsilon)/\\theta)/Z(x) $ and\n",
    "\n",
    "$$\n",
    "\\log\\hat g(\\varepsilon)=-W(Ax+B\\varepsilon)/\\theta-\\log Z(x).\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "\\int \\Bigl[\\hat g(\\varepsilon)W(Ax+B\\varepsilon) + \\theta \\hat g(\\varepsilon)\\log \\hat g(\\varepsilon)\\Bigr]\\pi(\\varepsilon) d\\varepsilon\n",
    "=\n",
    "-\\theta\\log Z(x),\n",
    "$$\n",
    "\n",
    "because the $ W $ terms cancel and $ \\int \\hat g \\pi = 1 $.\n",
    "\n",
    "Plugging this into [(84.16)](#equation-bhs-bellman-type2) gives\n",
    "\n",
    "$$\n",
    "W(x)\n",
    "=\n",
    "c-\\beta\\theta\\log Z(x)\n",
    "=\n",
    "c-\\beta\\theta \\log \\int \\exp\\left(-\\frac{W(Ax+B\\varepsilon)}{\\theta}\\right)\\pi(\\varepsilon) d\\varepsilon,\n",
    "$$\n",
    "\n",
    "which is [(84.13)](#equation-bhs-bellman-type1). Therefore $ W(x)\\equiv U(x) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3e34c",
   "metadata": {},
   "source": [
    "## Exercise 84.5\n",
    "\n",
    "Let $ \\varepsilon \\sim \\mathcal{N}(0,1) $ under the approximating model and define\n",
    "\n",
    "$$\n",
    "\\hat g(\\varepsilon) = \\exp\\left(w\\varepsilon - \\frac{1}{2}w^2\\right)\n",
    "$$\n",
    "\n",
    "as in the Gaussian mean-shift section.\n",
    "\n",
    "1. Show that $ E[\\hat g(\\varepsilon)] = 1 $.  \n",
    "1. Show that for any bounded measurable function $ f $,  \n",
    "\n",
    "\n",
    "$$\n",
    "E[\\hat g(\\varepsilon) f(\\varepsilon)]\n",
    "$$\n",
    "\n",
    "equals the expectation of $ f $ under $ \\mathcal{N}(w,1) $.\n",
    "\n",
    "1. Compute the mean and variance of $ \\log \\hat g(\\varepsilon) $ and use these to derive  \n",
    "\n",
    "\n",
    "$$\n",
    "\\operatorname{std}(\\hat g) = \\sqrt{e^{w^2}-1}.\n",
    "$$\n",
    "\n",
    "1. Compute the conditional relative entropy $ E[\\hat g\\log \\hat g] $ and verify that it equals $ \\tfrac{1}{2}w^2 $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0ae2f",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "1. Using the moment generating function of a standard normal,  \n",
    "\n",
    "\n",
    "$$\n",
    "E[\\hat g(\\varepsilon)]\n",
    "=\n",
    "e^{-w^2/2}E[e^{w\\varepsilon}]\n",
    "=\n",
    "e^{-w^2/2}e^{w^2/2}\n",
    "=\n",
    "1.\n",
    "$$\n",
    "\n",
    "1. Let $ \\varphi(\\varepsilon) = (2\\pi)^{-1/2}e^{-\\varepsilon^2/2} $ be the $ \\mathcal{N}(0,1) $ density.  \n",
    "\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\hat g(\\varepsilon)\\varphi(\\varepsilon)\n",
    "=\n",
    "\\frac{1}{\\sqrt{2\\pi}}\n",
    "\\exp\\left(w\\varepsilon-\\frac{1}{2}w^2-\\frac{1}{2}\\varepsilon^2\\right)\n",
    "=\n",
    "\\frac{1}{\\sqrt{2\\pi}}\n",
    "\\exp\\left(-\\frac{1}{2}(\\varepsilon-w)^2\\right),\n",
    "$$\n",
    "\n",
    "which is the $ \\mathcal{N}(w,1) $ density.\n",
    "\n",
    "Therefore, for bounded measurable $ f $,\n",
    "\n",
    "$$\n",
    "E[\\hat g(\\varepsilon)f(\\varepsilon)]\n",
    "=\n",
    "\\int f(\\varepsilon)\\hat g(\\varepsilon)\\varphi(\\varepsilon)d\\varepsilon\n",
    "$$\n",
    "\n",
    "equals the expectation of $ f $ under $ \\mathcal{N}(w,1) $.\n",
    "\n",
    "1. Since $ \\log \\hat g(\\varepsilon) = w\\varepsilon - \\tfrac{1}{2}w^2 $ and $ \\varepsilon\\sim\\mathcal{N}(0,1) $,  \n",
    "\n",
    "\n",
    "$$\n",
    "E[\\log \\hat g] = -\\frac{1}{2}w^2,\n",
    "\\qquad\n",
    "\\operatorname{Var}(\\log \\hat g)=w^2.\n",
    "$$\n",
    "\n",
    "Moreover, $ \\operatorname{Var}(\\hat g)=E[\\hat g^2]-1 $ because $ E[\\hat g]=1 $.\n",
    "\n",
    "Now\n",
    "\n",
    "$$\n",
    "E[\\hat g^2]\n",
    "=\n",
    "E\\left[\\exp\\left(2w\\varepsilon - w^2\\right)\\right]\n",
    "=\n",
    "e^{-w^2}E[e^{2w\\varepsilon}]\n",
    "=\n",
    "e^{-w^2}e^{(2w)^2/2}\n",
    "=\n",
    "e^{w^2},\n",
    "$$\n",
    "\n",
    "so $ \\operatorname{std}(\\hat g)=\\sqrt{e^{w^2}-1} $.\n",
    "\n",
    "1. Using part 2 with $ f(\\varepsilon)=\\log \\hat g(\\varepsilon)=w\\varepsilon-\\tfrac{1}{2}w^2 $,  \n",
    "\n",
    "\n",
    "$$\n",
    "E[\\hat g\\log \\hat g]\n",
    "=\n",
    "E_{\\mathcal{N}(w,1)}\\left[w\\varepsilon-\\frac{1}{2}w^2\\right]\n",
    "=\n",
    "w\\cdot E_{\\mathcal{N}(w,1)}[\\varepsilon]-\\frac{1}{2}w^2\n",
    "=\n",
    "w^2-\\frac{1}{2}w^2\n",
    "=\n",
    "\\frac{1}{2}w^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf21f04",
   "metadata": {},
   "source": [
    "## Exercise 84.6\n",
    "\n",
    "Derive the worst-case mean shifts [(84.23)](#equation-bhs-w-formulas) for both consumption models.\n",
    "\n",
    "From [(84.17)](#equation-bhs-ghat), $ \\hat g_{t+1} \\propto \\exp(-W(x_{t+1})/\\theta) $.\n",
    "\n",
    "When $ W $ is linear in the state, the exponent is linear in $ \\varepsilon_{t+1} $, and the Gaussian mean shift is $ w = -\\lambda/\\theta $ where $ \\lambda $ is the coefficient on $ \\varepsilon_{t+1} $ in $ W(x_{t+1}) $.\n",
    "\n",
    "1. Random-walk model: Guess $ W(x_t) = \\frac{1}{1-\\beta}[c_t + d] $. Using $ c_{t+1} = c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} $, find $ \\lambda $ and show that $ w = -\\sigma_\\varepsilon/[(1-\\beta)\\theta] $.  \n",
    "1. Trend-stationary model: Write $ z_t = \\tilde c_t - \\zeta $ and guess $ W(x_t) = \\frac{1}{1-\\beta}[c_t + \\alpha_1 z_t + \\alpha_0] $. Show that:  \n",
    "  - The coefficient on $ \\varepsilon_{t+1} $ in $ W(x_{t+1}) $ is $ (1+\\alpha_1)\\sigma_\\varepsilon/(1-\\beta) $.  \n",
    "  - Matching coefficients on $ z_t $ in the Bellman equation gives $ \\alpha_1 = \\beta(\\rho-1)/(1-\\beta\\rho) $.  \n",
    "  - Therefore $ 1+\\alpha_1 = (1-\\beta)/(1-\\beta\\rho) $ and $ w = -\\sigma_\\varepsilon/[(1-\\beta\\rho)\\theta] $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303780c",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**Part 1.**\n",
    "Under the guess $ W(x_t) = \\frac{1}{1-\\beta}[c_t + d] $ and $ c_{t+1} = c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} $,\n",
    "\n",
    "$$\n",
    "W(x_{t+1}) = \\frac{1}{1-\\beta}[c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} + d].\n",
    "$$\n",
    "\n",
    "The coefficient on $ \\varepsilon_{t+1} $ is $ \\lambda = \\sigma_\\varepsilon/(1-\\beta) $, so $ w = -\\lambda/\\theta = -\\sigma_\\varepsilon/[(1-\\beta)\\theta] $.\n",
    "\n",
    "**Part 2.**\n",
    "Under the guess $ W(x_t) = \\frac{1}{1-\\beta}[c_t + \\alpha_1 z_t + \\alpha_0] $ with $ c_{t+1} = c_t + \\mu + (\\rho-1)z_t + \\sigma_\\varepsilon\\varepsilon_{t+1} $ and $ z_{t+1} = \\rho z_t + \\sigma_\\varepsilon\\varepsilon_{t+1} $,\n",
    "\n",
    "$$\n",
    "W(x_{t+1}) = \\tfrac{1}{1-\\beta}\\bigl[c_t + \\mu + (\\rho-1)z_t + \\sigma_\\varepsilon\\varepsilon_{t+1} + \\alpha_1(\\rho z_t + \\sigma_\\varepsilon\\varepsilon_{t+1}) + \\alpha_0\\bigr].\n",
    "$$\n",
    "\n",
    "The coefficient on $ \\varepsilon_{t+1} $ is $ (1+\\alpha_1)\\sigma_\\varepsilon/(1-\\beta) $.\n",
    "\n",
    "To find $ \\alpha_1 $, substitute the guess into the Bellman equation.\n",
    "\n",
    "The factors of $ \\frac{1}{1-\\beta} $ cancel on both sides, and matching coefficients on $ z_t $ gives\n",
    "\n",
    "$$\n",
    "\\alpha_1 = \\beta\\bigl[(\\rho-1) + \\alpha_1\\rho\\bigr]\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\alpha_1(1-\\beta\\rho) = \\beta(\\rho-1)\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\alpha_1 = \\frac{\\beta(\\rho-1)}{1-\\beta\\rho}.\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "1+\\alpha_1 = \\frac{1-\\beta\\rho + \\beta(\\rho-1)}{1-\\beta\\rho} = \\frac{1-\\beta}{1-\\beta\\rho},\n",
    "$$\n",
    "\n",
    "and the coefficient on $ \\varepsilon_{t+1} $ becomes $ (1+\\alpha_1)\\sigma_\\varepsilon/(1-\\beta) = \\sigma_\\varepsilon/(1-\\beta\\rho) $, giving $ w = -\\sigma_\\varepsilon/[(1-\\beta\\rho)\\theta] $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a8593",
   "metadata": {},
   "source": [
    "## Exercise 84.7\n",
    "\n",
    "Verify the closed-form value function [(84.27)](#equation-bhs-w-rw) for the random-walk model by substituting a guess of the form $ W(x_t) = \\frac{1}{1-\\beta}[c_t + d] $ into the risk-sensitive Bellman equation [(84.13)](#equation-bhs-bellman-type1).\n",
    "\n",
    "1. Under the random walk $ c_{t+1} = c_t + \\mu + \\sigma_\\varepsilon \\varepsilon_{t+1} $, show that $ W(Ax_t + B\\varepsilon) = \\frac{1}{1-\\beta}[c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} + d] $.  \n",
    "1. Substitute into the $ \\log E\\exp $ term, using the fact that for $ Z \\sim \\mathcal{N}(\\mu_Z, \\sigma_Z^2) $ we have $ \\log E[\\exp(Z)] = \\mu_Z + \\frac{1}{2}\\sigma_Z^2 $.  \n",
    "1. Solve for $ d $ and confirm that it matches [(84.27)](#equation-bhs-w-rw).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f309cf",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**Part 1.** Under the random walk, $ c_{t+1} = c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} $. Substituting the guess $ W(x) = \\frac{1}{1-\\beta}[Hx + d] $ with $ Hx_t = c_t $:\n",
    "\n",
    "$$\n",
    "W(Ax_t + B\\varepsilon_{t+1}) = \\frac{1}{1-\\beta}\\bigl[c_t + \\mu + \\sigma_\\varepsilon\\varepsilon_{t+1} + d\\bigr].\n",
    "$$\n",
    "\n",
    "**Part 2.** The Bellman equation [(84.13)](#equation-bhs-bellman-type1) requires computing\n",
    "\n",
    "$$\n",
    "-\\beta\\theta\\log E_t\\left[\\exp\\left(\\frac{-W(Ax_t + B\\varepsilon_{t+1})}{\\theta}\\right)\\right].\n",
    "$$\n",
    "\n",
    "Substituting the guess:\n",
    "\n",
    "$$\n",
    "\\frac{-W(Ax_t + B\\varepsilon_{t+1})}{\\theta}\n",
    "=\n",
    "\\frac{-1}{(1-\\beta)\\theta}\\bigl[c_t + \\mu + d + \\sigma_\\varepsilon\\varepsilon_{t+1}\\bigr].\n",
    "$$\n",
    "\n",
    "This is an affine function of the standard normal $ \\varepsilon_{t+1} $, so the argument of the $ \\log E\\exp $ is normal with\n",
    "\n",
    "$$\n",
    "\\mu_Z = \\frac{-(c_t + \\mu + d)}{(1-\\beta)\\theta},\n",
    "\\qquad\n",
    "\\sigma_Z^2 = \\frac{\\sigma_\\varepsilon^2}{(1-\\beta)^2\\theta^2}.\n",
    "$$\n",
    "\n",
    "Using $ \\log E[e^Z] = \\mu_Z + \\frac{1}{2}\\sigma_Z^2 $:\n",
    "\n",
    "$$\n",
    "-\\beta\\theta\\left[\\frac{-(c_t + \\mu + d)}{(1-\\beta)\\theta} + \\frac{\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta^2}\\right]\n",
    "=\n",
    "\\frac{\\beta}{1-\\beta}\\left[c_t + \\mu + d - \\frac{\\sigma_\\varepsilon^2}{2(1-\\beta)\\theta}\\right].\n",
    "$$\n",
    "\n",
    "**Part 3.** The Bellman equation becomes\n",
    "\n",
    "$$\n",
    "\\frac{1}{1-\\beta}[c_t + d]\n",
    "=\n",
    "c_t + \\frac{\\beta}{1-\\beta}\\left[c_t + \\mu + d - \\frac{\\sigma_\\varepsilon^2}{2(1-\\beta)\\theta}\\right].\n",
    "$$\n",
    "\n",
    "Expanding the right-hand side:\n",
    "\n",
    "$$\n",
    "c_t + \\frac{\\beta c_t}{1-\\beta} + \\frac{\\beta(\\mu + d)}{1-\\beta} - \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta}\n",
    "=\n",
    "\\frac{c_t}{1-\\beta} + \\frac{\\beta(\\mu + d)}{1-\\beta} - \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta}.\n",
    "$$\n",
    "\n",
    "Equating both sides and cancelling $ \\frac{c_t}{1-\\beta} $:\n",
    "\n",
    "$$\n",
    "\\frac{d}{1-\\beta} = \\frac{\\beta(\\mu + d)}{1-\\beta} - \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta}.\n",
    "$$\n",
    "\n",
    "Solving: $ d - \\beta d = \\beta\\mu - \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)\\theta} $, so\n",
    "\n",
    "$$\n",
    "d = \\frac{\\beta}{1-\\beta}\\left(\\mu - \\frac{\\sigma_\\varepsilon^2}{2(1-\\beta)\\theta}\\right),\n",
    "$$\n",
    "\n",
    "which matches [(84.27)](#equation-bhs-w-rw)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b1a9a",
   "metadata": {},
   "source": [
    "## Exercise 84.8\n",
    "\n",
    "In the Gaussian mean-shift setting of [Exercise 5](#dov_ex5), let $ L_T $ be the log likelihood ratio between the worst-case and approximating models based on $ T $ observations.\n",
    "\n",
    "1. Show that $ L_T $ is normal under each model.  \n",
    "1. Compute its mean and variance under the approximating and worst-case models.  \n",
    "1. Using the definition of detection-error probability in [(84.30)](#equation-bhs-detection-formula), derive the closed-form expression [(84.31)](#equation-bhs-detection-closed).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172bef9",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Let the approximating model be $ \\varepsilon_i \\sim \\mathcal{N}(0,1) $ and the worst-case model be $ \\varepsilon_i \\sim \\mathcal{N}(w,1) $, i.i.d. for $ i=1,\\ldots,T $.\n",
    "\n",
    "Take the log likelihood ratio in the direction that matches the definitions in the text:\n",
    "\n",
    "$$\n",
    "L_T\n",
    "=\n",
    "\\log \\frac{\\prod_{i=1}^T \\varphi(\\varepsilon_i)}{\\prod_{i=1}^T \\varphi(\\varepsilon_i-w)}\n",
    "=\n",
    "\\sum_{i=1}^T \\ell(\\varepsilon_i),\n",
    "$$\n",
    "\n",
    "where $ \\varphi $ is the $ \\mathcal{N}(0,1) $ density and\n",
    "\n",
    "$$\n",
    "\\ell(\\varepsilon)\n",
    "=\n",
    "\\log \\varphi(\\varepsilon) - \\log \\varphi(\\varepsilon-w)\n",
    "=\n",
    "-\\frac{1}{2}\\Bigl[\\varepsilon^2-(\\varepsilon-w)^2\\Bigr]\n",
    "=\n",
    "-w\\varepsilon + \\frac{1}{2}w^2.\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "L_T = -w\\sum_{i=1}^T \\varepsilon_i + \\tfrac{1}{2}w^2T.\n",
    "$$\n",
    "\n",
    "Under the approximating model, $ \\sum_{i=1}^T \\varepsilon_i \\sim \\mathcal{N}(0,T) $, so\n",
    "\n",
    "$$\n",
    "L_T \\sim \\mathcal{N}\\left(\\frac{1}{2}w^2T, w^2T\\right).\n",
    "$$\n",
    "\n",
    "Under the worst-case model, $ \\sum_{i=1}^T \\varepsilon_i \\sim \\mathcal{N}(wT,T) $, so\n",
    "\n",
    "$$\n",
    "L_T \\sim \\mathcal{N}\\left(-\\frac{1}{2}w^2T, w^2T\\right).\n",
    "$$\n",
    "\n",
    "Now\n",
    "\n",
    "$$\n",
    "p_A = \\Pr_A(L_T<0)\n",
    "=\n",
    "\\Phi\\left(\\frac{0-\\frac{1}{2}w^2T}{|w|\\sqrt{T}}\\right)\n",
    "=\n",
    "\\Phi\\left(-\\frac{|w|\\sqrt{T}}{2}\\right),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "p_B = \\Pr_B(L_T>0)\n",
    "=\n",
    "1-\\Phi\\left(\\frac{0-(-\\frac{1}{2}w^2T)}{|w|\\sqrt{T}}\\right)\n",
    "=\n",
    "1-\\Phi\\left(\\frac{|w|\\sqrt{T}}{2}\\right)\n",
    "=\n",
    "\\Phi\\left(-\\frac{|w|\\sqrt{T}}{2}\\right).\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "p(\\theta^{-1})=\\tfrac{1}{2}(p_A+p_B)=\\Phi\\left(-\\tfrac{|w|\\sqrt{T}}{2}\\right),\n",
    "$$\n",
    "\n",
    "which is [(84.31)](#equation-bhs-detection-closed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8e772",
   "metadata": {},
   "source": [
    "## Exercise 84.9\n",
    "\n",
    "Using the formulas for $ w(\\theta) $ in [(84.23)](#equation-bhs-w-formulas) and the definition of discounted entropy\n",
    "\n",
    "$$\n",
    "\\eta = \\frac{\\beta}{1-\\beta}\\cdot \\frac{w(\\theta)^2}{2},\n",
    "$$\n",
    "\n",
    "show that holding $ \\eta $ fixed across the random-walk and trend-stationary consumption specifications implies the mapping [(84.32)](#equation-bhs-theta-cross-model).\n",
    "\n",
    "Specialize your result to the case $ \\sigma_\\varepsilon^{\\text{TS}} = \\sigma_\\varepsilon^{\\text{RW}} $ and interpret the role of $ \\rho $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd90bb9",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Because $ \\eta $ depends on $ \\theta $ only through $ w(\\theta)^2 $, holding $ \\eta $ fixed across models is equivalent to holding $ |w(\\theta)| $ fixed.\n",
    "\n",
    "Using [(84.23)](#equation-bhs-w-formulas),\n",
    "\n",
    "$$\n",
    "|w_{\\text{RW}}(\\theta_{\\text{RW}})|\n",
    "=\n",
    "\\frac{\\sigma_\\varepsilon^{\\text{RW}}}{(1-\\beta)\\theta_{\\text{RW}}},\n",
    "\\qquad\n",
    "|w_{\\text{TS}}(\\theta_{\\text{TS}})|\n",
    "=\n",
    "\\frac{\\sigma_\\varepsilon^{\\text{TS}}}{(1-\\beta\\rho)\\theta_{\\text{TS}}}.\n",
    "$$\n",
    "\n",
    "Equating these magnitudes and solving for $ \\theta_{\\text{TS}} $ gives\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{TS}}\n",
    "=\n",
    "\\left(\\frac{\\sigma_\\varepsilon^{\\text{TS}}}{\\sigma_\\varepsilon^{\\text{RW}}}\\right)\n",
    "\\frac{1-\\beta}{1-\\beta\\rho}\\theta_{\\text{RW}},\n",
    "$$\n",
    "\n",
    "which is [(84.32)](#equation-bhs-theta-cross-model).\n",
    "\n",
    "If $ \\sigma_\\varepsilon^{\\text{TS}}=\\sigma_\\varepsilon^{\\text{RW}} $, then\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{TS}}=\\frac{1-\\beta}{1-\\beta\\rho}\\theta_{\\text{RW}}.\n",
    "$$\n",
    "\n",
    "Since $ \\rho\\in(0,1) $ implies $ 1-\\beta\\rho > 1-\\beta $, the ratio $ (1-\\beta)/(1-\\beta\\rho) $ is less than one.\n",
    "\n",
    "To hold entropy fixed, the trend-stationary model therefore requires a smaller $ \\theta $ (i.e., a cheaper distortion and stronger robustness) than the random-walk model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51463c0",
   "metadata": {},
   "source": [
    "## Exercise 84.10\n",
    "\n",
    "For type II (multiplier) preferences under random-walk consumption growth, derive the compensating-variation formulas in [(84.35)](#equation-bhs-type2-rw-decomp).\n",
    "\n",
    "In particular, derive\n",
    "\n",
    "1. the *risk* term by comparing the stochastic economy to a deterministic consumption path with the same mean level of consumption (Lucas’s thought experiment), and  \n",
    "1. the *uncertainty* term by comparing a type II agent with parameter $ \\theta $ to the expected-utility case $ \\theta=\\infty $, holding the stochastic environment fixed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b8738",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Write the random walk as\n",
    "\n",
    "$$\n",
    "c_t = c_0 + t\\mu + \\sigma_\\varepsilon\\sum_{j=1}^t \\varepsilon_j\n",
    "$$\n",
    "\n",
    "with $ \\varepsilon_j\\stackrel{iid}{\\sim}\\mathcal{N}(0,1) $.\n",
    "\n",
    "**Risk term:**\n",
    "\n",
    "The mean level of consumption is\n",
    "\n",
    "$$\n",
    "E[C_t]=E[e^{c_t}]=\\exp(c_0+t\\mu+\\tfrac{1}{2}t\\sigma_\\varepsilon^2),\n",
    "$$\n",
    "\n",
    "so the deterministic path with the same mean levels is\n",
    "\n",
    "$$\n",
    "\\bar c_t = c_0 + t(\\mu+\\tfrac{1}{2}\\sigma_\\varepsilon^2).\n",
    "$$\n",
    "\n",
    "Under expected log utility ($ \\theta=\\infty $), discounted expected utility is\n",
    "\n",
    "$$\n",
    "\\sum_{t\\geq 0}\\beta^t E[c_t]\n",
    "=\n",
    "\\frac{c_0}{1-\\beta} + \\frac{\\beta\\mu}{(1-\\beta)^2},\n",
    "$$\n",
    "\n",
    "while for the deterministic mean-level path it is\n",
    "\n",
    "$$\n",
    "\\sum_{t\\geq 0}\\beta^t \\bar c_t\n",
    "=\n",
    "\\frac{c_0}{1-\\beta} + \\frac{\\beta(\\mu+\\tfrac{1}{2}\\sigma_\\varepsilon^2)}{(1-\\beta)^2}.\n",
    "$$\n",
    "\n",
    "If we reduce initial consumption by $ \\Delta c_0^{risk} $ (so $ \\bar c_t $ shifts down by $ \\Delta c_0^{risk} $ for all $ t $), utility falls by $ \\Delta c_0^{risk}/(1-\\beta) $.\n",
    "\n",
    "Equating the two utilities gives\n",
    "\n",
    "$$\n",
    "\\frac{\\Delta c_0^{risk}}{1-\\beta}\n",
    "=\n",
    "\\frac{\\beta(\\tfrac{1}{2}\\sigma_\\varepsilon^2)}{(1-\\beta)^2}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\Delta c_0^{risk}=\\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)}.\n",
    "$$\n",
    "\n",
    "**Uncertainty term:**\n",
    "\n",
    "For type II multiplier preferences, the minimizing distortion is a Gaussian mean shift with parameter $ w $ and per-period relative entropy $ \\tfrac{1}{2}w^2 $.\n",
    "\n",
    "Under the distorted model, $ E[\\varepsilon]=w $, so\n",
    "\n",
    "$$\n",
    "E[c_t]=c_0+t(\\mu+\\sigma_\\varepsilon w).\n",
    "$$\n",
    "\n",
    "Plugging this into the type II objective (and using $ E_t[g\\log g]=\\tfrac{1}{2}w^2 $) gives the discounted objective as a function of $ w $:\n",
    "\n",
    "$$\n",
    "J(w)\n",
    "=\n",
    "\\sum_{t\\geq 0}\\beta^t\\Bigl(c_0+t(\\mu+\\sigma_\\varepsilon w)\\Bigr)\n",
    "+\n",
    "\\sum_{t\\geq 0}\\beta^{t+1}\\theta\\cdot\\frac{w^2}{2}.\n",
    "$$\n",
    "\n",
    "Using $ \\sum_{t\\geq0}\\beta^t=1/(1-\\beta) $ and $ \\sum_{t\\geq0}t\\beta^t=\\beta/(1-\\beta)^2 $,\n",
    "\n",
    "$$\n",
    "J(w)\n",
    "=\n",
    "\\frac{c_0}{1-\\beta}\n",
    "+\n",
    "\\frac{\\beta(\\mu+\\sigma_\\varepsilon w)}{(1-\\beta)^2}\n",
    "+\n",
    "\\frac{\\beta\\theta}{1-\\beta}\\cdot\\frac{w^2}{2}.\n",
    "$$\n",
    "\n",
    "Minimizing over $ w $ yields\n",
    "\n",
    "$$\n",
    "0=\\frac{\\partial J}{\\partial w}\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon}{(1-\\beta)^2}\n",
    "+\n",
    "\\frac{\\beta\\theta}{1-\\beta}w\n",
    "\\quad\\Rightarrow\\quad\n",
    "w^*=-\\frac{\\sigma_\\varepsilon}{(1-\\beta)\\theta},\n",
    "$$\n",
    "\n",
    "which matches [(84.23)](#equation-bhs-w-formulas).\n",
    "\n",
    "Substituting $ w^* $ back in gives\n",
    "\n",
    "$$\n",
    "J(w^*)\n",
    "=\n",
    "\\frac{c_0}{1-\\beta}\n",
    "+\n",
    "\\frac{\\beta\\mu}{(1-\\beta)^2}\n",
    "-\\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^3\\theta}.\n",
    "$$\n",
    "\n",
    "When $ \\theta=\\infty $ (no model uncertainty), the last term disappears.\n",
    "Thus the utility gain from removing model uncertainty at fixed $ (\\mu,\\sigma_\\varepsilon) $ is\n",
    "\n",
    "$$\n",
    "\\beta\\sigma_\\varepsilon^2/[2(1-\\beta)^3\\theta].\n",
    "$$\n",
    "\n",
    "To offset this by a permanent upward shift in initial log consumption, we need\n",
    "\n",
    "$$\n",
    "\\Delta c_0^{uncertainty}/(1-\\beta)=\\beta\\sigma_\\varepsilon^2/[2(1-\\beta)^3\\theta],\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\Delta c_0^{uncertainty}\n",
    "=\n",
    "\\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)^2\\theta}.\n",
    "$$\n",
    "\n",
    "Together these reproduce [(84.35)](#equation-bhs-type2-rw-decomp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135d600",
   "metadata": {},
   "source": [
    "## Exercise 84.11\n",
    "\n",
    "Derive the trend-stationary risk compensation $ \\Delta c_0^{risk,ts} $ in [(84.39)](#equation-bhs-ts-compensations).\n",
    "\n",
    "For the trend-stationary model with $ \\tilde c_{t+1} - \\zeta = \\rho(\\tilde c_t - \\zeta) + \\sigma_\\varepsilon\\varepsilon_{t+1} $, where $ \\tilde c_t = c_t - \\mu t $, compute the risk compensation $ \\Delta c_0^{risk,ts} $ by comparing expected log utility under the stochastic plan to the deterministic certainty-equivalent path, and show that\n",
    "\n",
    "$$\n",
    "\\Delta c_0^{risk,ts} = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta\\rho^2)}.\n",
    "$$\n",
    "\n",
    "*Hint:* You will need $ \\operatorname{Var}(z_t) = \\sigma_\\varepsilon^2(1 + \\rho^2 + \\cdots + \\rho^{2(t-1)}) $ and the formula $ \\sum_{t \\geq 1}\\beta^t \\sum_{j=0}^{t-1}\\rho^{2j} = \\frac{\\beta}{(1-\\beta)(1-\\beta\\rho^2)} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425daec",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Under the trend-stationary model with $ z_0 = 0 $, $ c_t = c_0 + \\mu t + z_t $ and $ E[c_t] = c_0 + \\mu t $ (since $ E[z_t] = 0 $).\n",
    "\n",
    "The deterministic certainty-equivalent path matches $ E[C_t] = \\exp(c_0 + \\mu t + \\frac{1}{2}\\operatorname{Var}(z_t)) $, so its log is $ c_0^{ce} + \\mu t + \\frac{1}{2}\\operatorname{Var}(z_t) $.\n",
    "\n",
    "Under expected log utility ($ \\theta = \\infty $), the value of the stochastic plan is\n",
    "\n",
    "$$\n",
    "\\sum_{t \\geq 0}\\beta^t E[c_t] = \\frac{c_0}{1-\\beta} + \\frac{\\beta\\mu}{(1-\\beta)^2}.\n",
    "$$\n",
    "\n",
    "The value of the certainty-equivalent path (matching mean levels) starting from $ c_0 - \\Delta c_0^{risk} $ is\n",
    "\n",
    "$$\n",
    "\\sum_{t \\geq 0}\\beta^t \\bigl[c_0 - \\Delta c_0^{risk} + \\mu t + \\tfrac{1}{2}\\operatorname{Var}(z_t)\\bigr].\n",
    "$$\n",
    "\n",
    "Since $ \\operatorname{Var}(z_t) = \\sigma_\\varepsilon^2 \\sum_{j=0}^{t-1}\\rho^{2j} $, the extra term sums to\n",
    "\n",
    "$$\n",
    "\\sum_{t \\geq 1}\\beta^t \\cdot \\frac{\\sigma_\\varepsilon^2}{2}\\sum_{j=0}^{t-1}\\rho^{2j}\n",
    "= \\frac{\\sigma_\\varepsilon^2}{2}\\cdot\\frac{\\beta}{(1-\\beta)(1-\\beta\\rho^2)}.\n",
    "$$\n",
    "\n",
    "Equating values and solving:\n",
    "\n",
    "$$\n",
    "\\frac{\\Delta c_0^{risk}}{1-\\beta} = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta)(1-\\beta\\rho^2)}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\Delta c_0^{risk,ts} = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta\\rho^2)}.\n",
    "$$\n",
    "\n",
    "The uncertainty compensation follows from the value function: $ \\Delta c_0^{unc,ts,II} = \\frac{\\beta\\sigma_\\varepsilon^2}{2(1-\\beta\\rho)^2\\theta} $, with the $ (1-\\beta) $ factors replaced by $ (1-\\beta\\rho) $ because the worst-case mean shift scales with $ 1/(1-\\beta\\rho) $ rather than $ 1/(1-\\beta) $."
   ]
  }
 ],
 "metadata": {
  "date": 1771374620.3000107,
  "filename": "doubts_or_variability.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Doubts or Variability?"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}