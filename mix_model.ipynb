{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77ff161",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506360f4",
   "metadata": {},
   "source": [
    "\n",
    "<a id='likelihood-ratio-process'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64de8f",
   "metadata": {},
   "source": [
    "# Incorrect Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92812408",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with access to a GPU — although it will also run without one.\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/) has a free tier with GPUs\n",
    "that you can access as follows:\n",
    "\n",
    "1. Click on the “play” icon top right  \n",
    "1. Select Colab  \n",
    "1. Set the runtime environment to include a GPU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cfeb7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install numpyro jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb32a50",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is a sequel to [this quantecon lecture](https://python.quantecon.org/likelihood_bayes.html).\n",
    "\n",
    "We discuss two ways to create a compound lottery and their consequences.\n",
    "\n",
    "A compound lottery can be said to create a *mixture distribution*.\n",
    "\n",
    "Our two ways of constructing a compound lottery will differ in their **timing**.\n",
    "\n",
    "- in one, mixing between two possible probability distributions  will occur once and all at the beginning of time  \n",
    "- in the other, mixing between the same two possible probability distributions will occur each period  \n",
    "\n",
    "\n",
    "The statistical setting is close but not identical to the problem studied in that quantecon lecture.\n",
    "\n",
    "In that lecture, there were two  i.i.d. processes that could possibly govern successive draws of a non-negative random variable $ W $.\n",
    "\n",
    "Nature decided  once and for all whether to make  a sequence of IID draws from either $ f $ or from $ g $.\n",
    "\n",
    "That lecture studied an agent who knew both $ f $ and $ g $ but  did not know which distribution nature chose at time $ -1 $.\n",
    "\n",
    "The agent represented that ignorance  by assuming that nature had chosen  $ f $ or $ g $ by  flipping an unfair coin that put probability  $ \\pi_{-1} $ on probability distribution $ f $.\n",
    "\n",
    "That assumption allowed the agent to construct a subjective joint probability distribution over the\n",
    "random sequence $ \\{W_t\\}_{t=0}^\\infty $.\n",
    "\n",
    "We studied how the agent would then use the laws of conditional probability and an observed history $ w^t =\\{w_s\\}_{s=0}^t $ to form\n",
    "\n",
    "$$\n",
    "\\pi_t = E [ \\textrm{nature chose distribution}  f | w^t] , \\quad  t = 0, 1, 2, \\ldots\n",
    "$$\n",
    "\n",
    "However, in the  setting of this lecture, that rule imputes to the agent an incorrect model.\n",
    "\n",
    "The reason is that  now the wage sequence is actually described by a different statistical model.\n",
    "\n",
    "Thus, we change the [quantecon lecture](https://python.quantecon.org/likelihood_bayes.html) specification in the following way.\n",
    "\n",
    "Now, **each period** $ t \\geq 0 $, nature flips a possibly unfair coin that comes up $ f $ with probability $ \\alpha $\n",
    "and $ g $ with probability $ 1 -\\alpha $.\n",
    "\n",
    "Thus, nature perpetually draws from the **mixture distribution** with c.d.f.\n",
    "\n",
    "$$\n",
    "H(w) = \\alpha F(w) + (1-\\alpha) G(w), \\quad \\alpha \\in (0,1)\n",
    "$$\n",
    "\n",
    "We’ll study two agents  who try to learn about the wage process, but who use different  statistical models.\n",
    "\n",
    "Both types of agent know $ f $ and $ g $ but neither knows $ \\alpha $.\n",
    "\n",
    "Our first type of agent erroneously thinks that at time $ -1 $ nature once and for all chose $ f $ or $ g $ and thereafter\n",
    "permanently draws from that distribution.\n",
    "\n",
    "Our second type of agent knows, correctly, that nature mixes $ f $ and $ g $ with mixing probability $ \\alpha \\in (0,1) $\n",
    "each period, though the agent doesn’t know the mixing parameter.\n",
    "\n",
    "Our first type of agent applies the learning algorithm described in [this  quantecon lecture](https://python.quantecon.org/likelihood_bayes.html).\n",
    "\n",
    "In the context of the statistical model that prevailed in that lecture, that was a good learning algorithm and it enabled the Bayesian learner\n",
    "eventually to learn the distribution that nature had drawn at time $ -1 $.\n",
    "\n",
    "This is because the agent’s statistical model was *correct* in the sense of being aligned with the data\n",
    "generating process.\n",
    "\n",
    "But in the present context, our type 1 decision maker’s model is incorrect because the model $ h $ that actually\n",
    "generates the data is neither $ f $ nor $ g $ and so is beyond the support of the models that the agent thinks are\n",
    "possible.\n",
    "\n",
    "Nevertheless, we’ll see that our first type of agent muddles through and eventually learns something  interesting and useful, even though it is not *true*.\n",
    "\n",
    "Instead, it turns out that our type 1 agent who is armed with a wrong statistical model ends up learning whichever probability distribution, $ f $ or $ g $,\n",
    "is in a special sense *closest* to the $ h $ that actually generates the data.\n",
    "\n",
    "We’ll tell the sense in which it is closest.\n",
    "\n",
    "Our second type of agent understands that nature mixes between $ f $ and $ g $ each period with a fixed mixing\n",
    "probability $ \\alpha $.\n",
    "\n",
    "But  the agent doesn’t know $ \\alpha $.\n",
    "\n",
    "The agent sets out to learn $ \\alpha $ using Bayes’ law applied to his model.\n",
    "\n",
    "His model is correct in the sense that\n",
    "it includes the actual data generating process $ h $ as a possible distribution.\n",
    "\n",
    "In this lecture, we’ll learn about\n",
    "\n",
    "- how nature can *mix* between two distributions $ f $ and $ g $ to create a new distribution $ h $.  \n",
    "- The Kullback-Leibler statistical divergence [https://en.wikipedia.org/wiki/Kullback–Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) that governs statistical learning under an incorrect statistical model  \n",
    "- A useful Python function `numpy.searchsorted` that,  in conjunction with a uniform random number generator, can be used to sample from an arbitrary distribution  \n",
    "\n",
    "\n",
    "As usual, we’ll start by importing some Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0a43e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "import pandas as pd\n",
    "import scipy.stats as sp\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette()\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "np.random.seed(142857)\n",
    "\n",
    "@jit\n",
    "def set_seed():\n",
    "    np.random.seed(142857)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab405fd8",
   "metadata": {},
   "source": [
    "Let’s use Python to generate two beta distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6725fe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters in the two beta distributions.\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# The two density functions.\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6917fc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate(a, b, T=50, N=500):\n",
    "    '''\n",
    "    Generate N sets of T observations of the likelihood ratio,\n",
    "    return as N x T matrix.\n",
    "\n",
    "    '''\n",
    "\n",
    "    l_arr = np.empty((N, T))\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        for j in range(T):\n",
    "            w = np.random.beta(a, b)\n",
    "            l_arr[i, j] = f(w) / g(w)\n",
    "\n",
    "    return l_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0e7b4",
   "metadata": {},
   "source": [
    "We’ll also use the following Python code to prepare some informative simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e7d45",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b, N=50000)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8571d0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_f = simulate(F_a, F_b, N=50000)\n",
    "l_seq_f = np.cumprod(l_arr_f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67653d",
   "metadata": {},
   "source": [
    "## Sampling from  Compound Lottery $ H $\n",
    "\n",
    "We implement two methods  to draw samples from\n",
    "our mixture model $ \\alpha F + (1-\\alpha) G $.\n",
    "\n",
    "We’ll generate samples using each of them and verify that they match well.\n",
    "\n",
    "Here is pseudo code for a direct “method 1” for drawing from our compound lottery:\n",
    "\n",
    "- Step one:  \n",
    "  - use the numpy.random.choice function to flip an unfair coin that selects distribution $ F $ with prob $ \\alpha $\n",
    "    and $ G $ with prob $ 1 -\\alpha $  \n",
    "- Step two:  \n",
    "  - draw from either $ F $ or $ G $, as determined by the coin flip.  \n",
    "- Step three:  \n",
    "  - put the first two steps in a big loop and do them for each realization of $ w $  \n",
    "\n",
    "\n",
    "Our second method uses a uniform distribution and the following fact that we also described and used in the quantecon lecture [https://python.quantecon.org/prob_matrix.html](https://python.quantecon.org/prob_matrix.html):\n",
    "\n",
    "- If a random variable $ X $ has c.d.f. $ F $, then a random variable $ F^{-1}(U) $ also has c.d.f. $ F $, where $ U $ is a uniform random variable on $ [0,1] $.  \n",
    "\n",
    "\n",
    "In other words, if $ X \\sim F(x) $ we can generate a random sample from $ F $ by drawing a random sample from\n",
    "a uniform distribution on $ [0,1] $ and computing $ F^{-1}(U) $.\n",
    "\n",
    "We’ll  use this  fact\n",
    "in conjunction with the `numpy.searchsorted` command to sample from $ H $ directly.\n",
    "\n",
    "See [https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html) for the\n",
    "`searchsorted` function.\n",
    "\n",
    "See the [Mr. P Solver video on Monte Carlo simulation](https://www.google.com/search?q=Mr.+P+Solver+video+on+Monte+Carlo+simulation&amp;oq=Mr.+P+Solver+video+on+Monte+Carlo+simulation) to see other applications of this powerful trick.\n",
    "\n",
    "In the Python code below, we’ll use both of our methods and confirm that each of them does a good job of sampling\n",
    "from our target mixture distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6625e3d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def draw_lottery(p, N):\n",
    "    \"Draw from the compound lottery directly.\"\n",
    "\n",
    "    draws = []\n",
    "    for i in range(0, N):\n",
    "        if np.random.rand()<=p:\n",
    "            draws.append(np.random.beta(F_a, F_b))\n",
    "        else:\n",
    "            draws.append(np.random.beta(G_a, G_b))\n",
    "    return np.array(draws)\n",
    "\n",
    "def draw_lottery_MC(p, N):\n",
    "    \"Draw from the compound lottery using the Monte Carlo trick.\"\n",
    "\n",
    "    xs = np.linspace(1e-8,1-(1e-8),10000)\n",
    "    CDF = p*sp.beta.cdf(xs, F_a, F_b) + (1-p)*sp.beta.cdf(xs, G_a, G_b)\n",
    "\n",
    "    Us = np.random.rand(N)\n",
    "    draws = xs[np.searchsorted(CDF[:-1], Us)]\n",
    "    return draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9072c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# verify\n",
    "N = 100000\n",
    "α = 0.0\n",
    "\n",
    "sample1 = draw_lottery(α, N)\n",
    "sample2 = draw_lottery_MC(α, N)\n",
    "\n",
    "# plot draws and density function\n",
    "plt.hist(sample1, 50, density=True, alpha=0.5, label='direct draws')\n",
    "plt.hist(sample2, 50, density=True, alpha=0.5, label='MC draws')\n",
    "\n",
    "xs = np.linspace(0,1,1000)\n",
    "plt.plot(xs, α*f(xs)+(1-α)*g(xs), color='red', label='density')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829bdcd",
   "metadata": {},
   "source": [
    "## Type 1 Agent\n",
    "\n",
    "We’ll now study what our type 1 agent learns\n",
    "\n",
    "Remember that our type 1 agent uses the wrong statistical model, thinking that nature mixed between $ f $ and $ g $ once and for all at time $ -1 $.\n",
    "\n",
    "The type 1 agent thus uses the learning algorithm studied in [this  quantecon lecture](https://python.quantecon.org/likelihood_bayes.html).\n",
    "\n",
    "We’ll briefly review that learning algorithm now.\n",
    "\n",
    "Let $ \\pi_t $ be a Bayesian posterior defined as\n",
    "\n",
    "$$\n",
    "\\pi_t = {\\rm Prob}(q=f|w^t)\n",
    "$$\n",
    "\n",
    "The likelihood ratio process plays a principal role  in the formula that governs the evolution\n",
    "of the posterior probability $ \\pi_t $, an instance of **Bayes’ Law**.\n",
    "\n",
    "Bayes’ law implies that $ \\{\\pi_t\\} $ obeys the recursion\n",
    "\n",
    "\n",
    "<a id='equation-eq-recur1'></a>\n",
    "$$\n",
    "\\pi_t=\\frac{\\pi_{t-1} l_t(w_t)}{\\pi_{t-1} l_t(w_t)+1-\\pi_{t-1}} \\tag{30.1}\n",
    "$$\n",
    "\n",
    "with $ \\pi_{0} $ being a Bayesian prior probability that $ q = f $,\n",
    "i.e., a personal or subjective belief about $ q $ based on our having seen no data.\n",
    "\n",
    "Below we define a Python function that updates belief $ \\pi $ using\n",
    "likelihood ratio $ \\ell $ according to recursion [(30.1)](#equation-eq-recur1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f66211",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(π, l):\n",
    "    \"Update π using likelihood l\"\n",
    "\n",
    "    # Update belief\n",
    "    π = π * l / (π * l + 1 - π)\n",
    "\n",
    "    return π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c471d10",
   "metadata": {},
   "source": [
    "Formula [(30.1)](#equation-eq-recur1) can be generalized  by iterating on it and thereby deriving an\n",
    "expression for  the time $ t $ posterior $ \\pi_{t+1} $ as a function\n",
    "of the time $ 0 $ prior $ \\pi_0 $ and the likelihood ratio process\n",
    "$ L(w^{t+1}) $ at time $ t $.\n",
    "\n",
    "To begin, notice that the updating rule\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}\n",
    "=\\frac{\\pi_{t}\\ell \\left(w_{t+1}\\right)}\n",
    "{\\pi_{t}\\ell \\left(w_{t+1}\\right)+\\left(1-\\pi_{t}\\right)}\n",
    "$$\n",
    "\n",
    "implies\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{\\pi_{t+1}}\n",
    "    &=\\frac{\\pi_{t}\\ell \\left(w_{t+1}\\right)\n",
    "        +\\left(1-\\pi_{t}\\right)}{\\pi_{t}\\ell \\left(w_{t+1}\\right)} \\\\\n",
    "    &=1-\\frac{1}{\\ell \\left(w_{t+1}\\right)}\n",
    "        +\\frac{1}{\\ell \\left(w_{t+1}\\right)}\\frac{1}{\\pi_{t}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\frac{1}{\\pi_{t+1}}-1\n",
    "=\\frac{1}{\\ell \\left(w_{t+1}\\right)}\\left(\\frac{1}{\\pi_{t}}-1\\right).\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{1}{\\pi_{t+1}}-1\n",
    "    =\\frac{1}{\\prod_{i=1}^{t+1}\\ell \\left(w_{i}\\right)}\n",
    "        \\left(\\frac{1}{\\pi_{0}}-1\\right)\n",
    "    =\\frac{1}{L\\left(w^{t+1}\\right)}\\left(\\frac{1}{\\pi_{0}}-1\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $ \\pi_{0}\\in\\left(0,1\\right) $ and\n",
    "$ L\\left(w^{t+1}\\right)>0 $, we can verify that\n",
    "$ \\pi_{t+1}\\in\\left(0,1\\right) $.\n",
    "\n",
    "After rearranging the preceding equation, we can express $ \\pi_{t+1} $ as a\n",
    "function of  $ L\\left(w^{t+1}\\right) $, the  likelihood ratio process at $ t+1 $,\n",
    "and the initial prior $ \\pi_{0} $\n",
    "\n",
    "\n",
    "<a id='equation-eq-bayeslaw103'></a>\n",
    "$$\n",
    "\\pi_{t+1}=\\frac{\\pi_{0}L\\left(w^{t+1}\\right)}{\\pi_{0}L\\left(w^{t+1}\\right)+1-\\pi_{0}}. \\tag{30.2}\n",
    "$$\n",
    "\n",
    "Formula [(30.2)](#equation-eq-bayeslaw103) generalizes formula [(30.1)](#equation-eq-recur1).\n",
    "\n",
    "Formula [(30.2)](#equation-eq-bayeslaw103) can be regarded as a one step revision of prior probability $ \\pi_0 $ after seeing\n",
    "the batch of data $ \\left\\{ w_{i}\\right\\} _{i=1}^{t+1} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ae6c2",
   "metadata": {},
   "source": [
    "## What a type 1 Agent Learns when Mixture $ H $ Generates Data\n",
    "\n",
    "We now study what happens when the mixture distribution $ h;\\alpha $ truly generated the data each period.\n",
    "\n",
    "The sequence $ \\pi_t $ continues to converge, despite the agent’s misspecified model, and the limit is either $ 0 $ or $ 1 $.\n",
    "\n",
    "This is true even though in truth nature always mixes between $ f $ and $ g $.\n",
    "\n",
    "After verifying that claim about possible limit points of $ \\pi_t $ sequences, we’ll drill down and study\n",
    "what fundamental force determines the limiting value of $ \\pi_t $.\n",
    "\n",
    "Let’s set a value of $ \\alpha $ and then watch how $ \\pi_t $ evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029fa8e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_mixed(α, T=50, N=500):\n",
    "    \"\"\"\n",
    "    Generate N sets of T observations of the likelihood ratio,\n",
    "    return as N x T matrix, when the true density is mixed h;α\n",
    "    \"\"\"\n",
    "\n",
    "    w_s = draw_lottery(α, N*T).reshape(N, T)\n",
    "    l_arr = f(w_s) / g(w_s)\n",
    "\n",
    "    return l_arr\n",
    "\n",
    "def plot_π_seq(α, π1=0.2, π2=0.8, T=200):\n",
    "    \"\"\"\n",
    "    Compute and plot π_seq and the log likelihood ratio process\n",
    "    when the mixed distribution governs the data.\n",
    "    \"\"\"\n",
    "\n",
    "    l_arr_mixed = simulate_mixed(α, T=T, N=50)\n",
    "    l_seq_mixed = np.cumprod(l_arr_mixed, axis=1)\n",
    "\n",
    "    T = l_arr_mixed.shape[1]\n",
    "    π_seq_mixed = np.empty((2, T+1))\n",
    "    π_seq_mixed[:, 0] = π1, π2\n",
    "\n",
    "    for t in range(T):\n",
    "        for i in range(2):\n",
    "            π_seq_mixed[i, t+1] = update(π_seq_mixed[i, t], l_arr_mixed[0, t])\n",
    "\n",
    "    # plot\n",
    "    fig, ax1 = plt.subplots()\n",
    "    for i in range(2):\n",
    "        ax1.plot(range(T+1), π_seq_mixed[i, :], label=rf\"$\\pi_0$={π_seq_mixed[i, 0]}\")\n",
    "\n",
    "    ax1.plot(np.nan, np.nan,  '--', color='b', label='Log likelihood ratio process')\n",
    "    ax1.set_ylabel(r\"$\\pi_t$\")\n",
    "    ax1.set_xlabel(\"t\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\"when $\\\\alpha F + (1-\\\\alpha)G$ governs data\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(1, T+1), np.log(l_seq_mixed[0, :]), '--', color='b')\n",
    "    ax2.set_ylabel(\"$log(L(w^{t}))$\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664aed11",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_π_seq(α = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a31ec",
   "metadata": {},
   "source": [
    "The above graph shows a sample path of the log likelihood ratio process as the blue dotted line, together with\n",
    "sample paths of $ \\pi_t $ that start from two distinct initial conditions.\n",
    "\n",
    "Let’s see what happens when we change $ \\alpha $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf13e2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_π_seq(α = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f558a4",
   "metadata": {},
   "source": [
    "Evidently, $ \\alpha $ is having a big effect on the destination of $ \\pi_t $ as $ t \\rightarrow + \\infty $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ecb29",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence Governs Limit of $ \\pi_t $\n",
    "\n",
    "To understand what determines whether the limit point of  $ \\pi_t $ is  $ 0 $ or $ 1 $  and how the answer depends on the true value of the mixing probability  $ \\alpha \\in (0,1) $ that generates\n",
    "\n",
    "$$\n",
    "h(w) \\equiv h(w | \\alpha) = \\alpha f(w) + (1-\\alpha) g(w)\n",
    "$$\n",
    "\n",
    "we shall compute the following two Kullback-Leibler divergences\n",
    "\n",
    "$$\n",
    "KL_g (\\alpha) = \\int \\log\\left(\\frac{h(w)}{g(w)}\\right) h(w) d w\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "KL_f (\\alpha) = \\int \\log\\left(\\frac{h(w)}{f(w)}\\right) h(w) d w\n",
    "$$\n",
    "\n",
    "We shall plot both of these functions against $ \\alpha $ as we use $ \\alpha $ to vary\n",
    "$ h(w) = h(w|\\alpha) $.\n",
    "\n",
    "The limit of $ \\pi_t $ is  determined by\n",
    "\n",
    "$$\n",
    "\\min_{f,g} \\{KL_g, KL_f\\}\n",
    "$$\n",
    "\n",
    "The only possible limits are $ 0 $ and $ 1 $.\n",
    "\n",
    "As $ t \\rightarrow +\\infty $, $ \\pi_t $ goes to one if and only if  $ KL_f < KL_g $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6f28",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@vectorize\n",
    "def KL_g(α):\n",
    "    \"Compute the KL divergence KL(h, g).\"\n",
    "    err = 1e-8                          # to avoid 0 at end points\n",
    "    ws = np.linspace(err, 1-err, 10000)\n",
    "    gs, fs = g(ws), f(ws)\n",
    "    hs = α*fs + (1-α)*gs\n",
    "    return np.sum(np.log(hs/gs)*hs)/10000\n",
    "\n",
    "@vectorize\n",
    "def KL_f(α):\n",
    "    \"Compute the KL divergence KL(h, f).\"\n",
    "    err = 1e-8                          # to avoid 0 at end points\n",
    "    ws = np.linspace(err, 1-err, 10000)\n",
    "    gs, fs = g(ws), f(ws)\n",
    "    hs = α*fs + (1-α)*gs\n",
    "    return np.sum(np.log(hs/fs)*hs)/10000\n",
    "\n",
    "\n",
    "# compute KL using quad in Scipy\n",
    "def KL_g_quad(α):\n",
    "    \"Compute the KL divergence KL(h, g) using scipy.integrate.\"\n",
    "    h = lambda x: α*f(x) + (1-α)*g(x)\n",
    "    return quad(lambda x: h(x) * np.log(h(x)/g(x)), 0, 1)[0]\n",
    "\n",
    "def KL_f_quad(α):\n",
    "    \"Compute the KL divergence KL(h, f) using scipy.integrate.\"\n",
    "    h = lambda x: α*f(x) + (1-α)*g(x)\n",
    "    return quad(lambda x: h(x) * np.log(h(x)/f(x)), 0, 1)[0]\n",
    "\n",
    "# vectorize\n",
    "KL_g_quad_v = np.vectorize(KL_g_quad)\n",
    "KL_f_quad_v = np.vectorize(KL_f_quad)\n",
    "\n",
    "\n",
    "# Let us find the limit point\n",
    "def π_lim(α, T=5000, π_0=0.4):\n",
    "    \"Find limit of π sequence.\"\n",
    "    π_seq = np.zeros(T+1)\n",
    "    π_seq[0] = π_0\n",
    "    l_arr = simulate_mixed(α, T, N=1)[0]\n",
    "\n",
    "    for t in range(T):\n",
    "        π_seq[t+1] = update(π_seq[t], l_arr[t])\n",
    "    return π_seq[-1]\n",
    "\n",
    "π_lim_v = np.vectorize(π_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712994d",
   "metadata": {},
   "source": [
    "Let us first plot the KL divergences $ KL_g\\left(\\alpha\\right), KL_f\\left(\\alpha\\right) $ for each $ \\alpha $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825eca6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α_arr = np.linspace(0, 1, 100)\n",
    "KL_g_arr = KL_g(α_arr)\n",
    "KL_f_arr = KL_f(α_arr)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=[10, 6])\n",
    "\n",
    "ax.plot(α_arr, KL_g_arr, label='KL(h, g)')\n",
    "ax.plot(α_arr, KL_f_arr, label='KL(h, f)')\n",
    "ax.set_ylabel('KL divergence')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628393b4",
   "metadata": {},
   "source": [
    "Let’s compute an $ \\alpha $ for which  the KL divergence  between $ h $ and $ g $ is the same as that between $ h $ and $ f $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9e800",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# where KL_f = KL_g\n",
    "discretion = α_arr[np.argmin(np.abs(KL_g_arr-KL_f_arr))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ada93",
   "metadata": {},
   "source": [
    "We can compute and plot the convergence point $ \\pi_{\\infty} $ for each $ \\alpha $ to verify that the convergence is indeed governed by the KL divergence.\n",
    "\n",
    "The blue circles show the limiting values of $ \\pi_t $ that simulations discover for different values of $ \\alpha $\n",
    "recorded on the $ x $ axis.\n",
    "\n",
    "Thus, the graph below confirms how a minimum  KL divergence governs what our type 1 agent eventually learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe5f93",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α_arr_x = α_arr[(α_arr<discretion)|(α_arr>discretion)]\n",
    "π_lim_arr = π_lim_v(α_arr_x)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, figsize=[10, 6])\n",
    "\n",
    "ax.plot(α_arr, KL_g_arr, label='KL(h, g)')\n",
    "ax.plot(α_arr, KL_f_arr, label='KL(h, f)')\n",
    "ax.set_ylabel('KL divergence')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "\n",
    "# plot KL\n",
    "ax2 = ax.twinx()\n",
    "# plot limit point\n",
    "ax2.scatter(α_arr_x, π_lim_arr, \n",
    "            facecolors='none', \n",
    "            edgecolors='tab:blue', \n",
    "            label=r'$\\pi$ lim')\n",
    "ax2.set_ylabel('π lim')\n",
    "\n",
    "ax.legend(loc=[0.85, 0.8])\n",
    "ax2.legend(loc=[0.85, 0.73])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20857a69",
   "metadata": {},
   "source": [
    "Evidently, our type 1 learner who applies Bayes’ law to his misspecified set of statistical models eventually learns an approximating model that is as close as possible to the true model, as measured by its\n",
    "Kullback-Leibler divergence:\n",
    "\n",
    "- When $ \\alpha $ is small, $ KL_g < KL_f $ meaning the divergence of $ g $ from $ h $ is smaller than that of $ f $ and so the limit point of $ \\pi_t $ is close to $ 0 $.  \n",
    "- When $ \\alpha $ is large, $ KL_f < KL_g $ meaning the divergence of $ f $ from $ h $ is smaller than that of $ g $ and so the limit point of $ \\pi_t $ is close to $ 1 $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4cb15",
   "metadata": {},
   "source": [
    "## Type 2 Agent\n",
    "\n",
    "We now describe how our type 2 agent formulates his learning problem and what he eventually learns.\n",
    "\n",
    "Our type 2 agent understands the correct statistical model but does not know $ \\alpha $.\n",
    "\n",
    "We apply Bayes law to deduce an algorithm for  learning $ \\alpha $ under the assumption\n",
    "that the agent knows that\n",
    "\n",
    "$$\n",
    "h(w) = h(w| \\alpha)\n",
    "$$\n",
    "\n",
    "but does not know $ \\alpha $.\n",
    "\n",
    "We’ll assume that the person starts out with a prior probability $ \\pi_0(\\alpha) $ on\n",
    "$ \\alpha \\in (0,1) $ where the prior has one of the forms that we deployed in [this quantecon lecture](https://python.quantecon.org/bayes_nonconj.html).\n",
    "\n",
    "We’ll fire up `numpyro` and  apply it  to the present situation.\n",
    "\n",
    "Bayes’ law now takes the form\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}(\\alpha) = \\frac {h(w_{t+1} | \\alpha) \\pi_t(\\alpha)}\n",
    "       { \\int h(w_{t+1} | \\hat \\alpha) \\pi_t(\\hat \\alpha) d \\hat \\alpha }\n",
    "$$\n",
    "\n",
    "We’ll use numpyro  to approximate this equation.\n",
    "\n",
    "We’ll create  graphs of the posterior $ \\pi_t(\\alpha) $ as\n",
    "$ t \\rightarrow +\\infty $ corresponding to ones presented in the quantecon lecture [https://python.quantecon.org/bayes_nonconj.html](https://python.quantecon.org/bayes_nonconj.html).\n",
    "\n",
    "We anticipate that a posterior  distribution will collapse around  the true $ \\alpha $ as\n",
    "$ t \\rightarrow + \\infty $.\n",
    "\n",
    "Let us try a uniform prior first.\n",
    "\n",
    "We use the `Mixture` class in numpyro to construct the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615a6f6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α = 0.8\n",
    "\n",
    "# simulate data with true α\n",
    "data = draw_lottery(α, 1000)\n",
    "sizes = [5, 20, 50, 200, 1000, 25000]\n",
    "\n",
    "def model(w):\n",
    "    α = numpyro.sample('α', dist.Uniform(low=0.0, high=1.0))\n",
    "\n",
    "    y_samp = numpyro.sample('w',\n",
    "        dist.Mixture(dist.Categorical(jnp.array([α, 1-α])), [dist.Beta(F_a, F_b), dist.Beta(G_a, G_b)]), obs=w)\n",
    "\n",
    "def MCMC_run(ws):\n",
    "    \"Compute posterior using MCMC with observed ws\"\n",
    "\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(kernel, num_samples=5000, num_warmup=1000, progress_bar=False)\n",
    "\n",
    "    mcmc.run(rng_key=random.PRNGKey(142857), w=jnp.array(ws))\n",
    "    sample = mcmc.get_samples()\n",
    "    return sample['α']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ceefa",
   "metadata": {},
   "source": [
    "The following code generates the graph below that displays Bayesian posteriors for $ \\alpha $ at various history lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62d29b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    sample = MCMC_run(data[:sizes[i]])\n",
    "    sns.histplot(\n",
    "        data=sample, kde=True, stat='density', alpha=0.2, ax=ax,\n",
    "        color=colors[i], binwidth=0.02, linewidth=0.05, label=f't={sizes[i]}'\n",
    "    )\n",
    "ax.set_title(r'$\\pi_t(\\alpha)$ as $t$ increases')\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092e9ef",
   "metadata": {},
   "source": [
    "Evidently,  the Bayesian posterior  narrows in on the true value  $ \\alpha = .8 $ of the mixing parameter as the length of a history of observations grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd03c5a",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "Our type 1 person  deploys an incorrect statistical  model.\n",
    "\n",
    "He believes\n",
    "that either $ f $ or $ g $ generated the $ w $ process, but just doesn’t know which one.\n",
    "\n",
    "That is wrong because nature is actually mixing each period with mixing probability $ \\alpha $.\n",
    "\n",
    "Our type 1 agent  eventually believes that either $ f $ or $ g $ generated the $ w $ sequence, the outcome being determined by the model, either $ f $ or $ g $, whose  KL divergence relative to $ h $ is smaller.\n",
    "\n",
    "Our type 2 agent has a different statistical model, one that is correctly specified.\n",
    "\n",
    "He knows the parametric form of the statistical model but not the mixing parameter $ \\alpha $.\n",
    "\n",
    "He knows that he does not know it.\n",
    "\n",
    "But by using Bayes’ law in conjunction with his statistical model and a history of data,  he eventually acquires a more and more accurate inference about $ \\alpha $.\n",
    "\n",
    "This little laboratory  exhibits some important general principles that govern outcomes of Bayesian learning of misspecified models.\n",
    "\n",
    "Thus, the  following situation prevails quite generally in empirical work.\n",
    "\n",
    "A scientist approaches the data with a manifold $ S $ of statistical models $ s (X | \\theta) $ , where $ s $ is a probability distribution over a random vector $ X $, $ \\theta \\in \\Theta $\n",
    "is a vector of parameters, and $ \\Theta $ indexes the manifold of models.\n",
    "\n",
    "The scientist with observations that he interprets as realizations $ x $ of the random vector $ X $ wants to solve an **inverse problem** of somehow *inverting*\n",
    "$ s(x | \\theta) $ to infer $ \\theta $ from $ x $.\n",
    "\n",
    "But the scientist’s model is misspecified, being only an approximation to an unknown  model $ h $ that nature uses to generate $ X $.\n",
    "\n",
    "If the scientist uses Bayes’ law or a related  likelihood-based  method to infer $ \\theta $, it occurs quite generally that for large sample sizes the inverse problem infers a  $ \\theta $ that minimizes  the KL divergence of the scientist’s model $ s $ relative to nature’s   model $ h $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b0ae1",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c7204",
   "metadata": {},
   "source": [
    "## Exercise 30.1\n",
    "\n",
    "In [Likelihood Ratio Processes and Bayesian Learning](https://python.quantecon.org/likelihood_bayes.html), we studied the consequence of applying likelihood ratio\n",
    "and Bayes’ law to a misspecified statistical model.\n",
    "\n",
    "In that lecture, we used a model selection algorithm to study the case where the true data generating process is a mixture.\n",
    "\n",
    "In this lecture, we studied how to correctly “learn” a model generated by a mixing process using a Bayesian approach.\n",
    "\n",
    "To fix the algorithm we used in [Likelihood Ratio Processes and Bayesian Learning](https://python.quantecon.org/likelihood_bayes.html), a correct Bayesian approach should directly model the uncertainty about $ x $ and update beliefs about it as new data arrives.\n",
    "\n",
    "Here is the algorithm:\n",
    "\n",
    "First we specify a prior distribution for $ x $ given by $ x \\sim \\text{Beta}(\\alpha_0, \\beta_0) $ with expectation $ \\mathbb{E}[x] = \\frac{\\alpha_0}{\\alpha_0 + \\beta_0} $.\n",
    "\n",
    "The likelihood for a single observation $ w_t $ is $ p(w_t|x) = x f(w_t) + (1-x) g(w_t) $.\n",
    "\n",
    "For a sequence $ w^t = (w_1, \\dots, w_t) $, the likelihood is $ p(w^t|x) = \\prod_{i=1}^t p(w_i|x) $.\n",
    "\n",
    "The posterior distribution is updated using $ p(x|w^t) \\propto p(w^t|x) p(x) $.\n",
    "\n",
    "Recursively, the posterior after $ w_t $ is $ p(x|w^t) \\propto p(w_t|x) p(x|w^{t-1}) $.\n",
    "\n",
    "Without a conjugate prior, we can approximate the posterior by discretizing $ x $ into a grid.\n",
    "\n",
    "Your task is to implement this algorithm in Python.\n",
    "\n",
    "You can verify your implementation by checking that the posterior mean converges to the true value of $ x $ as $ t $ increases in [Likelihood Ratio Processes and Bayesian Learning](https://python.quantecon.org/likelihood_bayes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f6403",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Here is one solution:\n",
    "\n",
    "First we define the mixture probability\n",
    "and parameters of prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a99850",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_true = 0.5\n",
    "T_mix = 200\n",
    "\n",
    "# Three different priors with means 0.25, 0.5, 0.75\n",
    "prior_params = [(1, 3), (1, 1), (3, 1)]\n",
    "prior_means = [a/(a+b) for a, b in prior_params]\n",
    "\n",
    "w_mix = draw_lottery(x_true, T_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49b724",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def learn_x_bayesian(observations, α0, β0, grid_size=2000):\n",
    "    \"\"\"\n",
    "    Sequential Bayesian learning of the mixing probability x\n",
    "    using a grid approximation.\n",
    "    \"\"\"\n",
    "    w = np.asarray(observations)\n",
    "    T = w.size\n",
    "\n",
    "    x_grid = np.linspace(1e-3, 1 - 1e-3, grid_size)\n",
    "\n",
    "    # Log prior\n",
    "    log_prior = (α0 - 1) * np.log(x_grid) + (β0 - 1) * np.log1p(-x_grid)\n",
    "\n",
    "    μ_path = np.empty(T + 1)\n",
    "    μ_path[0] = α0 / (α0 + β0)\n",
    "\n",
    "    log_post = log_prior.copy()\n",
    "\n",
    "    for t in range(T):\n",
    "        wt = w[t]\n",
    "        # P(w_t | x) = x f(w_t) + (1 - x) g(w_t)\n",
    "        like = x_grid * f(wt) + (1 - x_grid) * g(wt)\n",
    "        log_post += np.log(like)\n",
    "\n",
    "        # normalize\n",
    "        log_post -= log_post.max()\n",
    "        post = np.exp(log_post)\n",
    "        post /= post.sum()\n",
    "\n",
    "        μ_path[t + 1] = x_grid @ post\n",
    "\n",
    "    return μ_path\n",
    "\n",
    "x_posterior_means = [learn_x_bayesian(w_mix, α0, β0) for α0, β0 in prior_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203aefe4",
   "metadata": {},
   "source": [
    "Let’s visualize how the posterior mean of $ x $ evolves over time, starting from three different prior beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6090a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (x_means, mean0) in enumerate(zip(x_posterior_means, prior_means)):\n",
    "    ax.plot(range(T_mix + 1), x_means, \n",
    "            label=fr'Prior mean = ${mean0:.2f}$', \n",
    "            color=colors[i], linewidth=2)\n",
    "\n",
    "ax.axhline(y=x_true, color='black', linestyle='--', \n",
    "           label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('Posterior mean of $x$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605e764",
   "metadata": {},
   "source": [
    "The plot shows that regardless of the initial prior belief, all three posterior means eventually converge towards the true value of $ x=0.5 $.\n",
    "\n",
    "Next, let’s look at multiple simulations with a longer time horizon, all starting from a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21621fff",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "n_paths = 20\n",
    "T_long = 10_000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for j in range(n_paths):\n",
    "    w_path = draw_lottery(x_true, T_long) \n",
    "    x_means = learn_x_bayesian(w_path, 1, 1)  # Uniform prior\n",
    "    ax.plot(range(T_long + 1), x_means, alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.axhline(y=x_true, color='red', linestyle='--', \n",
    "            label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_ylabel('Posterior mean of $x$')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801118d",
   "metadata": {},
   "source": [
    "We can see that the posterior mean of $ x $ converges to the true value $ x=0.5 $."
   ]
  }
 ],
 "metadata": {
  "date": 1772080452.8297532,
  "filename": "mix_model.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Incorrect Models"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}