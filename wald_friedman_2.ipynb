{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6225f3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='wald-friedman-2'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c00fb",
   "metadata": {},
   "source": [
    "# A Bayesian Formulation of Friedman and Wald’s Problem\n",
    "\n",
    "\n",
    "<a id='index-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e593d",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [A Bayesian Formulation of Friedman and Wald’s Problem](#A-Bayesian-Formulation-of-Friedman-and-Wald’s-Problem)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [A Dynamic Programming Approach](#A-Dynamic-Programming-Approach)  \n",
    "  - [Implementation](#Implementation)  \n",
    "  - [Analysis](#Analysis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4611c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture revisits the  statistical decision problem presented to Milton\n",
    "Friedman and W. Allen Wallis during World War II when they were analysts at\n",
    "the U.S. Government’s  Statistical Research Group at Columbia University.\n",
    "\n",
    "In  [an earlier lecture](https://python.quantecon.org/wald_friedman.html), we described how  Abraham Wald [[Wald, 1947](https://python.quantecon.org/zreferences.html#id125)]  solved the problem by  extending frequentist hypothesis testing techniques and formulating the problem sequentially.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Wald’s idea of formulating the problem sequentially created links to the **dynamic programming** that Richard Bellman developed in the 1950s.\n",
    "\n",
    "As we learned in [Elementary Probability with Matrices](https://python.quantecon.org/prob_matrix.html) and  [Two Meanings of Probability](https://python.quantecon.org/prob_meaning.html), a frequentist statistician views a probability distribution as measuring relative frequencies of a statistic that he anticipates constructing  from a very long sequence of i.i.d. draws from a known probability distribution.\n",
    "\n",
    "That known probability distribution is his ‘hypothesis’.\n",
    "\n",
    "A frequentist statistician studies the distribution of that statistic under that known probability distribution\n",
    "\n",
    "- when the distribution is a member of a set of parameterized probability distributions, his hypothesis takes the form of a particular parameter vector.  \n",
    "- this is what we mean when we say that the frequentist statistician ‘conditions on the parameters’  \n",
    "- he regards the parameters as fixed numbers that are known to nature, but not to him.  \n",
    "- the statistician copes with his ignorance of those parameters by constructing type I and type II errors associated with frequentist hypothesis testing.  \n",
    "\n",
    "\n",
    "In this lecture, we reformulate Friedman and Wald’s  problem  by transforming our point of view from the ‘objective’ frequentist perspective of [the lecture on Wald’s sequential analysis](https://python.quantecon.org/wald_friedman.html) to an explicitly ‘subjective’ perspective taken by a Bayesian decision maker who regards parameters not as fixed numbers but as (hidden) random variables that are jointly distributed with the random variables that he can observe by  sampling from that joint distribution.\n",
    "\n",
    "To form that joint distribution, the Bayesian statistician supplements the conditional distributions used by the frequentist statistician with\n",
    "a prior probability distribution over the parameters that represents his personal, subjective opinion about them.\n",
    "\n",
    "That lets the Bayesian statistician calculate the joint distribution that he requires to calculate the conditional distributions that he wants.\n",
    "\n",
    "To proceed in this way, we endow our decision maker with\n",
    "\n",
    "- an initial prior subjective probability $ \\pi_{-1} \\in (0,1) $  that nature uses to  generate  $ \\{z_k\\} $ as a sequence of i.i.d. draws from $ f_1 $ rather than $ f_0 $.  \n",
    "- faith in Bayes’ law as a way to revise his subjective beliefs as observations on $ \\{z_k\\} $ sequence arrive.  \n",
    "- a loss function that tells how the decision maker values type I and type II errors.  \n",
    "\n",
    "\n",
    "In our [previous frequentist version](https://python.quantecon.org/wald_friedman.html), key ideas in play were:\n",
    "\n",
    "- Type I and type II statistical errors  \n",
    "  - a type I error occurs when you reject a null hypothesis that is true  \n",
    "  - a type II error occurs when you accept a null hypothesis that is false  \n",
    "- Abraham Wald’s **sequential probability ratio test**  \n",
    "- The **power** of a statistical test  \n",
    "- The **critical region** of a statistical test  \n",
    "- A **uniformly most powerful test**  \n",
    "\n",
    "\n",
    "In this lecture about a Bayesian reformulation of the problem, additional  ideas at work are\n",
    "\n",
    "- an initial prior probability $ \\pi_{-1} $ that model $ f_1 $ generates the data  \n",
    "- Bayes’ Law  \n",
    "- a sequence of posterior probabilities that model $ f_1 $ is  generating the data  \n",
    "- dynamic programming  \n",
    "\n",
    "\n",
    "This lecture uses ideas studied in the lectures on [likelihood ratio processes](https://python.quantecon.org/likelihood_ratio_process.html), [their roles in Bayesian learning](https://python.quantecon.org/likelihood_bayes.html), and [this lecture on exchangeability](https://python.quantecon.org/exchangeable.html).\n",
    "\n",
    "We’ll begin with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f34ef",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, prange, float64, int64\n",
    "from numba.experimental import jitclass\n",
    "from math import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea96406",
   "metadata": {},
   "source": [
    "## A Dynamic Programming Approach\n",
    "\n",
    "The following presentation of the problem closely follows Dmitri\n",
    "Bertsekas’s treatment in **Dynamic Programming and Stochastic Control** [[Bertsekas, 1975](https://python.quantecon.org/zreferences.html#id124)].\n",
    "\n",
    "A decision-maker can observe a sequence of draws of a random variable $ z $.\n",
    "\n",
    "He (or she) wants to know which of two probability distributions $ f_0 $ or $ f_1 $ governs $ z $.\n",
    "\n",
    "Conditional on knowing that successive observations are drawn from distribution $ f_0 $, the sequence of\n",
    "random variables is independently and identically distributed (IID).\n",
    "\n",
    "Conditional on knowing that successive observations are drawn from distribution $ f_1 $, the sequence of\n",
    "random variables is also independently and identically distributed (IID).\n",
    "\n",
    "But the observer does not know which of the two distributions generated the sequence.\n",
    "\n",
    "For reasons explained in  [Exchangeability and Bayesian Updating](https://python.quantecon.org/exchangeable.html), this means that the sequence is not\n",
    "IID.\n",
    "\n",
    "The observer has something to learn, namely, whether the observations are drawn from  $ f_0 $ or from $ f_1 $.\n",
    "\n",
    "The decision maker   wants  to decide\n",
    "which of the  two distributions is generating outcomes.\n",
    "\n",
    "We adopt a Bayesian formulation.\n",
    "\n",
    "The decision maker begins  with a prior probability\n",
    "\n",
    "$$\n",
    "\\pi_{-1} =\n",
    "\\mathbb P \\{ f = f_1 \\mid \\textrm{ no observations} \\} \\in (0, 1)\n",
    "$$\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">In Bertsekas [[1975](https://python.quantecon.org/zreferences.html#id124)], the belief is associated with the distribution $ f_0 $, but here\n",
    "we associate the belief with the distribution $ f_1 $ to match the discussions in [the lecture on Wald’s sequential analysis](https://python.quantecon.org/wald_friedman.html).\n",
    "\n",
    "After observing $ k+1 $ observations $ z_k, z_{k-1}, \\ldots, z_0 $, he updates his personal probability that the observations are described by distribution $ f_1 $  to\n",
    "\n",
    "$$\n",
    "\\pi_k = \\mathbb P \\{ f = f_1 \\mid z_k, z_{k-1}, \\ldots, z_0 \\}\n",
    "$$\n",
    "\n",
    "which is calculated recursively by applying Bayes’ law:\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} = \\frac{ \\pi_k f_1(z_{k+1})}{ (1-\\pi_k) f_0(z_{k+1}) + \\pi_k f_1 (z_{k+1}) },\n",
    "\\quad k = -1, 0, 1, \\ldots\n",
    "$$\n",
    "\n",
    "After observing $ z_k, z_{k-1}, \\ldots, z_0 $, the decision-maker believes\n",
    "that $ z_{k+1} $ has probability distribution\n",
    "\n",
    "$$\n",
    "f_{{\\pi}_k} (v) = (1-\\pi_k) f_0(v) + \\pi_k f_1 (v) ,\n",
    "$$\n",
    "\n",
    "which  is a mixture of distributions $ f_0 $ and $ f_1 $, with the weight\n",
    "on $ f_1 $ being the posterior probability that $ f = f_1 $ <sup><a href=#f1 id=f1-link>[1]</a></sup>.\n",
    "\n",
    "To  illustrate such a distribution, let’s inspect some mixtures of beta distributions.\n",
    "\n",
    "The density of a beta probability distribution with parameters $ a $ and $ b $ is\n",
    "\n",
    "$$\n",
    "f(z; a, b) = \\frac{\\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\\Gamma(a) \\Gamma(b)}\n",
    "\\quad \\text{where} \\quad\n",
    "\\Gamma(t) := \\int_{0}^{\\infty} x^{t-1} e^{-x} dx\n",
    "$$\n",
    "\n",
    "The next figure shows two beta distributions in the top panel.\n",
    "\n",
    "The bottom panel presents mixtures of these distributions, with various mixing probabilities $ \\pi_k $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9288f9f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x**(a-1) * (1 - x)**(b-1)\n",
    "\n",
    "f0 = lambda x: p(x, 1, 1)\n",
    "f1 = lambda x: p(x, 9, 9)\n",
    "grid = np.linspace(0, 1, 50)\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 8))\n",
    "\n",
    "axes[0].set_title(\"Original Distributions\")\n",
    "axes[0].plot(grid, f0(grid), lw=2, label=\"$f_0$\")\n",
    "axes[0].plot(grid, f1(grid), lw=2, label=\"$f_1$\")\n",
    "\n",
    "axes[1].set_title(\"Mixtures\")\n",
    "for π in 0.25, 0.5, 0.75:\n",
    "    y = (1 - π) * f0(grid) + π * f1(grid)\n",
    "    axes[1].plot(grid, y, lw=2, label=fr\"$\\pi_k$ = {π}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend()\n",
    "    ax.set(xlabel=\"$z$ values\", ylabel=\"probability of $z_k$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d023743",
   "metadata": {},
   "source": [
    "### Losses and Costs\n",
    "\n",
    "After observing $ z_k, z_{k-1}, \\ldots, z_0 $, the decision-maker\n",
    "chooses among three distinct actions:\n",
    "\n",
    "- He decides that $ f = f_0 $ and draws no more $ z $’s  \n",
    "- He decides that $ f = f_1 $ and draws no more $ z $’s  \n",
    "- He postpones deciding now and instead chooses to draw a\n",
    "  $ z_{k+1} $  \n",
    "\n",
    "\n",
    "Associated with these three actions, the decision-maker can suffer three\n",
    "kinds of losses:\n",
    "\n",
    "- A loss $ L_0 $ if he decides $ f = f_0 $ when actually\n",
    "  $ f=f_1 $  \n",
    "- A loss $ L_1 $ if he decides $ f = f_1 $ when actually\n",
    "  $ f=f_0 $  \n",
    "- A cost $ c $ if he postpones deciding and chooses instead to draw\n",
    "  another $ z $  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f51d4f",
   "metadata": {},
   "source": [
    "### Digression on Type I and Type II Errors\n",
    "\n",
    "If we regard  $ f=f_0 $ as a null hypothesis and $ f=f_1 $ as an alternative hypothesis,\n",
    "then $ L_1 $ and $ L_0 $ are losses associated with two types of statistical errors\n",
    "\n",
    "- a type I error is an incorrect rejection of a true null hypothesis (a “false positive”)  \n",
    "- a type II error is a failure to reject a false null hypothesis (a “false negative”)  \n",
    "\n",
    "\n",
    "So when we treat $ f=f_0 $ as the null hypothesis\n",
    "\n",
    "- We can think of $ L_1 $ as the loss associated with a type I\n",
    "  error.  \n",
    "- We can think of $ L_0 $ as the loss associated with a type II\n",
    "  error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd52e33",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "Before proceeding,  let’s try to guess what an optimal decision rule might look like.\n",
    "\n",
    "Suppose at some given point in time that $ \\pi $ is close to 1.\n",
    "\n",
    "Then our prior beliefs and the evidence so far point strongly to $ f = f_1 $.\n",
    "\n",
    "If, on the other hand, $ \\pi $ is close to 0, then $ f = f_0 $ is strongly favored.\n",
    "\n",
    "Finally, if $ \\pi $ is in the middle of the interval $ [0, 1] $, then we are confronted with more uncertainty.\n",
    "\n",
    "This reasoning suggests a sequential  decision rule that we illustrate  in the following figure:\n",
    "\n",
    "![https://python.quantecon.org/_static/lecture_specific/wald_friedman_2/wald_dec_rule.png](https://python.quantecon.org/_static/lecture_specific/wald_friedman_2/wald_dec_rule.png)\n",
    "\n",
    "  \n",
    "As we’ll see, this is indeed the correct form of the decision rule.\n",
    "\n",
    "Our problem is to determine threshold values $ A, B $ that somehow depend on the parameters described  above.\n",
    "\n",
    "You might like to pause at this point and try to predict the impact of a\n",
    "parameter such as $ c $ or $ L_0 $ on $ A $ or $ B $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d3a49",
   "metadata": {},
   "source": [
    "### A Bellman Equation\n",
    "\n",
    "Let $ J(\\pi) $ be the total loss for a decision-maker with current belief $ \\pi $ who chooses optimally.\n",
    "\n",
    "Principles of **dynamic programming** teach  us that an optimal loss function  $ J $ satisfies the following  the Bellman functional  equation\n",
    "\n",
    "\n",
    "<a id='equation-new1'></a>\n",
    "$$\n",
    "J(\\pi) =\n",
    "    \\min\n",
    "    \\left\\{\n",
    "        \\underbrace{\\pi L_0}_{ \\text{accept } f_0 } \\; , \\; \\underbrace{(1-\\pi) L_1}_{ \\text{accept } f_1 } \\; , \\;\n",
    "        \\underbrace{c + \\mathbb E [ J (\\pi') ]}_{ \\text{draw again} }\n",
    "    \\right\\} \\tag{27.1}\n",
    "$$\n",
    "\n",
    "where $ \\pi' $ is the random variable defined by Bayes’ Law\n",
    "\n",
    "$$\n",
    "\\pi' = \\kappa(z', \\pi) = \\frac{ \\pi f_1(z')}{ (1-\\pi) f_0(z') + \\pi f_1 (z') }\n",
    "$$\n",
    "\n",
    "when $ \\pi $ is fixed and $ z' $ is drawn from the current best guess, which is the distribution $ f $ defined by\n",
    "\n",
    "$$\n",
    "f_{\\pi}(v) = (1-\\pi) f_0(v) + \\pi f_1 (v)\n",
    "$$\n",
    "\n",
    "In the Bellman equation, minimization is over three actions:\n",
    "\n",
    "1. Accept the hypothesis that $ f = f_0 $  \n",
    "1. Accept the hypothesis that $ f = f_1 $  \n",
    "1. Postpone deciding and draw again  \n",
    "\n",
    "\n",
    "We can represent the  Bellman equation as\n",
    "\n",
    "\n",
    "<a id='equation-optdec'></a>\n",
    "$$\n",
    "J(\\pi) =\n",
    "\\min \\left\\{ \\pi L_0, \\; (1-\\pi) L_1, \\; h(\\pi) \\right\\} \\tag{27.2}\n",
    "$$\n",
    "\n",
    "where $ \\pi \\in [0,1] $ and\n",
    "\n",
    "- $ \\pi L_0 $ is the expected loss associated with accepting\n",
    "  $ f_0 $ (i.e., the cost of making a type II error).  \n",
    "- $ (1-\\pi) L_1 $ is the expected loss associated with accepting\n",
    "  $ f_1 $ (i.e., the cost of making a type I error).  \n",
    "- $ h(\\pi) :=  c + \\mathbb E [J(\\pi')] $; this is the continuation value; i.e.,\n",
    "  the expected cost associated with drawing one more $ z $.  \n",
    "\n",
    "\n",
    "The optimal decision rule is characterized by two numbers $ A, B \\in (0,1) \\times (0,1) $ that satisfy\n",
    "\n",
    "$$\n",
    "\\pi L_0 < \\min \\{ (1-\\pi) L_1, c + \\mathbb E [J(\\pi')] \\}  \\textrm { if } \\pi \\leq B\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "(1- \\pi) L_1 < \\min \\{ \\pi L_0,  c + \\mathbb E [J(\\pi')] \\} \\textrm { if } \\pi \\geq A\n",
    "$$\n",
    "\n",
    "The optimal decision rule is then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm { accept } f=f_1 \\textrm{ if } \\pi \\geq A \\\\\n",
    "\\textrm { accept } f=f_0 \\textrm{ if } \\pi \\leq B \\\\\n",
    "\\textrm { draw another }  z \\textrm{ if }  B < \\pi < A\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our aim is to compute the cost function $ J $ as well as  the associated cutoffs $ A $\n",
    "and $ B $.\n",
    "\n",
    "To help make our computations more manageable,  we can use [(27.2)](#equation-optdec) to  write the continuation cost $ h(\\pi) $ as\n",
    "\n",
    "\n",
    "<a id='equation-optdec2'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(\\pi) &= c + \\mathbb E [J(\\pi')] \\\\\n",
    "&= c + \\mathbb E_{\\pi'} \\min \\{ \\pi' L_0, (1 - \\pi') L_1, h(\\pi') \\} \\\\\n",
    "&= c + \\int \\min \\{ \\kappa(z', \\pi) L_0, (1 - \\kappa(z', \\pi) ) L_1, h(\\kappa(z', \\pi) ) \\} f_\\pi (z') dz'\n",
    "\\end{aligned} \\tag{27.3}\n",
    "$$\n",
    "\n",
    "The equality\n",
    "\n",
    "\n",
    "<a id='equation-funceq'></a>\n",
    "$$\n",
    "h(\\pi) =\n",
    "c + \\int \\min \\{ \\kappa(z', \\pi) L_0, (1 - \\kappa(z', \\pi) ) L_1, h(\\kappa(z', \\pi) ) \\} f_\\pi (z') dz' \\tag{27.4}\n",
    "$$\n",
    "\n",
    "is an equation  in an unknown function  $ h $.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Such an equation is called a **functional equation**.\n",
    "\n",
    "Using the functional equation, [(27.4)](#equation-funceq), for the continuation cost, we can back out\n",
    "optimal choices using the right side of [(27.2)](#equation-optdec).\n",
    "\n",
    "This functional equation can be solved by taking an initial guess and iterating\n",
    "to find a fixed point.\n",
    "\n",
    "Thus, we iterate with an operator $ Q $, where\n",
    "\n",
    "$$\n",
    "Q h(\\pi) =\n",
    "c + \\int \\min \\{ \\kappa(z', \\pi) L_0, (1 - \\kappa(z', \\pi) ) L_1, h(\\kappa(z', \\pi) ) \\} f_\\pi (z') dz'\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b9a81",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "First, we will construct a `jitclass` to store the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b671d2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "wf_data = [('a0', float64),          # Parameters of beta distributions\n",
    "           ('b0', float64),\n",
    "           ('a1', float64),\n",
    "           ('b1', float64),\n",
    "           ('c', float64),           # Cost of another draw\n",
    "           ('π_grid_size', int64),\n",
    "           ('L0', float64),          # Cost of selecting f0 when f1 is true\n",
    "           ('L1', float64),          # Cost of selecting f1 when f0 is true\n",
    "           ('π_grid', float64[:]),\n",
    "           ('mc_size', int64),\n",
    "           ('z0', float64[:]),\n",
    "           ('z1', float64[:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959ec4c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jitclass(wf_data)\n",
    "class WaldFriedman:\n",
    "\n",
    "    def __init__(self,\n",
    "                 c=1.25,\n",
    "                 a0=1,\n",
    "                 b0=1,\n",
    "                 a1=3,\n",
    "                 b1=1.2,\n",
    "                 L0=25,\n",
    "                 L1=25,\n",
    "                 π_grid_size=200,\n",
    "                 mc_size=1000):\n",
    "\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        self.a1, self.b1 = a1, b1\n",
    "        self.c, self.π_grid_size = c, π_grid_size\n",
    "        self.L0, self.L1 = L0, L1\n",
    "        self.π_grid = np.linspace(0, 1, π_grid_size)\n",
    "        self.mc_size = mc_size\n",
    "\n",
    "        self.z0 = np.random.beta(a0, b0, mc_size)\n",
    "        self.z1 = np.random.beta(a1, b1, mc_size)\n",
    "\n",
    "    def f0(self, x):\n",
    "\n",
    "        return p(x, self.a0, self.b0)\n",
    "\n",
    "    def f1(self, x):\n",
    "\n",
    "        return p(x, self.a1, self.b1)\n",
    "\n",
    "    def f0_rvs(self):\n",
    "        return np.random.beta(self.a0, self.b0)\n",
    "\n",
    "    def f1_rvs(self):\n",
    "        return np.random.beta(self.a1, self.b1)\n",
    "\n",
    "    def κ(self, z, π):\n",
    "        \"\"\"\n",
    "        Updates π using Bayes' rule and the current observation z\n",
    "        \"\"\"\n",
    "\n",
    "        f0, f1 = self.f0, self.f1\n",
    "\n",
    "        π_f0, π_f1 = (1 - π) * f0(z), π * f1(z)\n",
    "        π_new = π_f1 / (π_f0 + π_f1)\n",
    "\n",
    "        return π_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997fe16e",
   "metadata": {},
   "source": [
    "As in [Cake Eating III: Stochastic Dynamics](https://python.quantecon.org/cake_eating_stochastic.html), to approximate a continuous value function\n",
    "\n",
    "- We iterate at a finite grid of possible values of $ \\pi $.  \n",
    "- When we evaluate $ \\mathbb E[J(\\pi')] $ between grid points, we use linear interpolation.  \n",
    "\n",
    "\n",
    "We define the operator function `Q` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cd754",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def Q(h, wf):\n",
    "\n",
    "    c, π_grid = wf.c, wf.π_grid\n",
    "    L0, L1 = wf.L0, wf.L1\n",
    "    z0, z1 = wf.z0, wf.z1\n",
    "    mc_size = wf.mc_size\n",
    "\n",
    "    κ = wf.κ\n",
    "\n",
    "    h_new = np.empty_like(π_grid)\n",
    "    h_func = lambda p: np.interp(p, π_grid, h)\n",
    "\n",
    "    for i in prange(len(π_grid)):\n",
    "        π = π_grid[i]\n",
    "\n",
    "        # Find the expected value of J by integrating over z\n",
    "        integral_f0, integral_f1 = 0, 0\n",
    "        for m in range(mc_size):\n",
    "            π_0 = κ(z0[m], π)  # Draw z from f0 and update π\n",
    "            integral_f0 += min(π_0 * L0, (1 - π_0) * L1, h_func(π_0))\n",
    "\n",
    "            π_1 = κ(z1[m], π)  # Draw z from f1 and update π\n",
    "            integral_f1 += min(π_1 * L0, (1 - π_1) * L1, h_func(π_1))\n",
    "\n",
    "        integral = ((1 - π) * integral_f0 + π * integral_f1) / mc_size\n",
    "\n",
    "        h_new[i] = c + integral\n",
    "\n",
    "    return h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b2b8f",
   "metadata": {},
   "source": [
    "To solve the key functional equation, we will iterate using `Q` to find the fixed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb2670",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def solve_model(wf, tol=1e-4, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Compute the continuation cost function\n",
    "\n",
    "    * wf is an instance of WaldFriedman\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up loop\n",
    "    h = np.zeros(len(wf.π_grid))\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        h_new = Q(h, wf)\n",
    "        error = np.max(np.abs(h - h_new))\n",
    "        i += 1\n",
    "        h = h_new\n",
    "\n",
    "    if error > tol:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    return h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148df798",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let’s inspect outcomes.\n",
    "\n",
    "We will be using the default parameterization with distributions like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0538977",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "wf = WaldFriedman()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(wf.f0(wf.π_grid), label=\"$f_0$\")\n",
    "ax.plot(wf.f1(wf.π_grid), label=\"$f_1$\")\n",
    "ax.set(ylabel=\"probability of $z_k$\", xlabel=\"$z_k$\", title=\"Distributions\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331a393",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "To solve the model, we will call our `solve_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199aadb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "h_star = solve_model(wf)    # Solve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce58cc",
   "metadata": {},
   "source": [
    "We will also set up a function to compute the cutoffs $ A $ and $ B $\n",
    "and plot these on our cost function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ed7f1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def find_cutoff_rule(wf, h):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a continuation cost function and returns the\n",
    "    corresponding cutoffs of where you transition between continuing and\n",
    "    choosing a specific model\n",
    "    \"\"\"\n",
    "\n",
    "    π_grid = wf.π_grid\n",
    "    L0, L1 = wf.L0, wf.L1\n",
    "\n",
    "    # Evaluate cost at all points on grid for choosing a model\n",
    "    cost_f0 = π_grid * L0\n",
    "    cost_f1 = (1 - π_grid) * L1\n",
    "    \n",
    "    # Find B: largest π where cost_f0 <= min(cost_f1, h)\n",
    "    optimal_cost = np.minimum(np.minimum(cost_f0, cost_f1), h)\n",
    "    choose_f0 = (cost_f0 <= cost_f1) & (cost_f0 <= h)\n",
    "    \n",
    "    if np.any(choose_f0):\n",
    "        B = π_grid[choose_f0][-1]  # Last point where we choose f0\n",
    "    else:\n",
    "        assert False, \"No point where we choose f0\"\n",
    "    \n",
    "    # Find A: smallest π where cost_f1 <= min(cost_f0, h)  \n",
    "    choose_f1 = (cost_f1 <= cost_f0) & (cost_f1 <= h)\n",
    "    \n",
    "    if np.any(choose_f1):\n",
    "        A = π_grid[choose_f1][0]  # First point where we choose f1\n",
    "    else:\n",
    "        assert False, \"No point where we choose f1\"\n",
    "\n",
    "    return (B, A)\n",
    "\n",
    "B, A = find_cutoff_rule(wf, h_star)\n",
    "cost_L0 = wf.π_grid * wf.L0\n",
    "cost_L1 = (1 - wf.π_grid) * wf.L1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(wf.π_grid, h_star, label='sample again')\n",
    "ax.plot(wf.π_grid, cost_L1, label='choose f1')\n",
    "ax.plot(wf.π_grid, cost_L0, label='choose f0')\n",
    "ax.plot(wf.π_grid,\n",
    "        np.amin(np.column_stack([h_star, cost_L0, cost_L1]),axis=1),\n",
    "        lw=15, alpha=0.1, color='b', label=r'$J(\\pi)$')\n",
    "\n",
    "ax.annotate(r\"$B$\", xy=(B + 0.01, 0.5), fontsize=14)\n",
    "ax.annotate(r\"$A$\", xy=(A + 0.01, 0.5), fontsize=14)\n",
    "\n",
    "plt.vlines(B, 0, (1 - B) * wf.L1, linestyle=\"--\")\n",
    "plt.vlines(A, 0, A * wf.L0, linestyle=\"--\")\n",
    "\n",
    "ax.set(xlim=(0, 1), ylim=(0, 0.5 * max(wf.L0, wf.L1)), ylabel=\"cost\",\n",
    "       xlabel=r\"$\\pi$\", title=r\"Cost function $J(\\pi)$\")\n",
    "\n",
    "plt.legend(borderpad=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6be5aa",
   "metadata": {},
   "source": [
    "The cost function $ J $ equals $ \\pi L_0 $ for $ \\pi \\leq B $, and $ (1-\\pi) L_1 $ for $ \\pi\n",
    "\\geq A $.\n",
    "\n",
    "The slopes of the two linear pieces of the cost   function $ J(\\pi) $ are determined by $ L_0 $\n",
    "and $ -L_1 $.\n",
    "\n",
    "The cost function $ J $ is smooth in the interior region, where the posterior\n",
    "probability assigned to $ f_1 $ is in the indecisive region $ \\pi \\in (B, A) $.\n",
    "\n",
    "The decision-maker continues to sample until the probability that he attaches to\n",
    "model $ f_1 $ falls below $ B $ or above $ A $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e417d8",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "The next figure shows the outcomes of 500 simulations of the decision process.\n",
    "\n",
    "On the left is a histogram of **stopping times**, i.e.,  the number of draws of $ z_k $ required to make a decision.\n",
    "\n",
    "The average number of draws is around 6.6.\n",
    "\n",
    "On the right is the fraction of correct decisions at the stopping time.\n",
    "\n",
    "In this case, the decision-maker is correct 80% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49d3d5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate(wf, true_dist, h_star, π_0=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes an initial condition and simulates until it\n",
    "    stops (when a decision is made)\n",
    "    \"\"\"\n",
    "\n",
    "    f0, f1 = wf.f0, wf.f1\n",
    "    f0_rvs, f1_rvs = wf.f0_rvs, wf.f1_rvs\n",
    "    π_grid = wf.π_grid\n",
    "    κ = wf.κ\n",
    "\n",
    "    if true_dist == \"f0\":\n",
    "        f, f_rvs = wf.f0, wf.f0_rvs\n",
    "    elif true_dist == \"f1\":\n",
    "        f, f_rvs = wf.f1, wf.f1_rvs\n",
    "\n",
    "    # Find cutoffs\n",
    "    B, A = find_cutoff_rule(wf, h_star)\n",
    "\n",
    "    # Initialize a couple of useful variables\n",
    "    decision_made = False\n",
    "    π = π_0\n",
    "    t = 0\n",
    "\n",
    "    while decision_made is False:\n",
    "        z = f_rvs()\n",
    "        t = t + 1\n",
    "        π = κ(z, π)\n",
    "        if π < B:\n",
    "            decision_made = True\n",
    "            decision = 0\n",
    "        elif π > A:\n",
    "            decision_made = True\n",
    "            decision = 1\n",
    "\n",
    "    if true_dist == \"f0\":\n",
    "        if decision == 0:\n",
    "            correct = True\n",
    "        else:\n",
    "            correct = False\n",
    "\n",
    "    elif true_dist == \"f1\":\n",
    "        if decision == 1:\n",
    "            correct = True\n",
    "        else:\n",
    "            correct = False\n",
    "\n",
    "    return correct, π, t\n",
    "\n",
    "def stopping_dist(wf, h_star, ndraws=250, true_dist=\"f0\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Simulates repeatedly to get distributions of time needed to make a\n",
    "    decision and how often they are correct\n",
    "    \"\"\"\n",
    "\n",
    "    tdist = np.empty(ndraws, int)\n",
    "    cdist = np.empty(ndraws, bool)\n",
    "\n",
    "    for i in range(ndraws):\n",
    "        correct, π, t = simulate(wf, true_dist, h_star)\n",
    "        tdist[i] = t\n",
    "        cdist[i] = correct\n",
    "\n",
    "    return cdist, tdist\n",
    "\n",
    "def simulation_plot(wf):\n",
    "    h_star = solve_model(wf)\n",
    "    ndraws = 500\n",
    "    cdist, tdist = stopping_dist(wf, h_star, ndraws)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    ax[0].hist(tdist, bins=np.max(tdist))\n",
    "    ax[0].set_title(f\"Stopping times over {ndraws} replications\")\n",
    "    ax[0].set(xlabel=\"time\", ylabel=\"number of stops\")\n",
    "    ax[0].annotate(f\"mean = {np.mean(tdist)}\", xy=(max(tdist) / 2,\n",
    "                   max(np.histogram(tdist, bins=max(tdist))[0]) / 2))\n",
    "\n",
    "    ax[1].hist(cdist.astype(int), bins=2)\n",
    "    ax[1].set_title(f\"Correct decisions over {ndraws} replications\")\n",
    "    ax[1].annotate(f\"% correct = {np.mean(cdist)}\",\n",
    "                   xy=(0.05, ndraws / 2))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "simulation_plot(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f286f",
   "metadata": {},
   "source": [
    "### Comparative Statics\n",
    "\n",
    "Now let’s consider the following exercise.\n",
    "\n",
    "We double the cost of drawing an additional observation.\n",
    "\n",
    "Before you look, think about what will happen:\n",
    "\n",
    "- Will the decision-maker be correct more or less often?  \n",
    "- Will he make decisions sooner or later?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf81e2d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "wf = WaldFriedman(c=2.5)\n",
    "simulation_plot(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f0e67",
   "metadata": {},
   "source": [
    "Increased cost per draw has induced the decision-maker to take fewer draws before deciding.\n",
    "\n",
    "Because he decides with fewer draws, the percentage of time he is correct drops.\n",
    "\n",
    "This leads to him having a higher expected loss when he puts equal weight on both models.\n",
    "\n",
    "To facilitate comparative statics, we invite you to adjust the parameters of the model\n",
    "and investigate\n",
    "\n",
    "- effects on the smoothness of the value function in the indecisive middle range\n",
    "  as we increase the number of grid points in the piecewise linear  approximation.  \n",
    "- effects of different settings for the cost parameters $ L_0, L_1, c $, the\n",
    "  parameters of two beta distributions $ f_0 $ and $ f_1 $, and the number\n",
    "  of points and linear functions $ m $ to use in the piecewise continuous approximation to the value function.  \n",
    "- various simulations from $ f_0 $ and associated distributions of waiting times to making a decision.  \n",
    "- associated histograms of correct and incorrect decisions.  \n",
    "\n",
    "\n",
    "<p><a id=f1 href=#f1-link><strong>[1]</strong></a> The decision maker acts as if he believes that the sequence of random variables\n",
    "$ [z_{0}, z_{1}, \\ldots] $ is *exchangeable*. See [Exchangeability and Bayesian Updating](https://python.quantecon.org/exchangeable.html) and\n",
    "[[Kreps, 1988](https://python.quantecon.org/zreferences.html#id123)] chapter 11, for discussions of exchangeability."
   ]
  }
 ],
 "metadata": {
  "date": 1763027072.948124,
  "filename": "wald_friedman_2.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "A Bayesian Formulation of Friedman and Wald’s Problem"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}