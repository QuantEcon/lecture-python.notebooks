{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e0d83a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2256625",
   "metadata": {},
   "source": [
    "\n",
    "<a id='sargent-measurement-models'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123ebfb",
   "metadata": {},
   "source": [
    "# Two Models of Measurements and the Investment Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd2ef6",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Two Models of Measurements and the Investment Accelerator](#Two-Models-of-Measurements-and-the-Investment-Accelerator)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [The economic model](#The-economic-model)  \n",
    "  - [Measurement errors](#Measurement-errors)  \n",
    "  - [A classical model of measurements initially collected by an agency](#A-classical-model-of-measurements-initially-collected-by-an-agency)  \n",
    "  - [A model of optimal estimates reported by an agency](#A-model-of-optimal-estimates-reported-by-an-agency)  \n",
    "  - [Simulation](#Simulation)  \n",
    "  - [Summary](#Summary)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858c8b1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "“Rational expectations econometrics” aims to interpret economic time\n",
    "series in terms of objects that are meaningful to economists, namely,\n",
    "parameters describing preferences, technologies, information sets,\n",
    "endowments, and equilibrium concepts.\n",
    "\n",
    "When fully worked out, rational expectations models typically deliver\n",
    "a well-defined mapping from these economically interpretable parameters\n",
    "to the moments of the time series determined by the model.\n",
    "\n",
    "If accurate observations on these time series are available, one can\n",
    "use that mapping to implement parameter estimation methods based\n",
    "either on the likelihood function or on the method of moments.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">This is why econometrics estimation is often called an ‘‘inverse’’ problem, while\n",
    "simulating a model for given parameter values is called a ‘‘direct problem’’. The direct problem\n",
    "refers to the mapping we have just described, while the inverse problem involves somehow applying an ‘‘inverse’’ of that mapping to a data set that is treated as if it were one draw from the joint probability distribution described by the mapping.\n",
    "\n",
    "However, if only error-ridden data exist for the variables of interest,\n",
    "then more steps are needed to extract parameter estimates.\n",
    "\n",
    "In effect, we require a model of the data reporting agency, one that\n",
    "is workable enough that we can determine the mapping induced jointly\n",
    "by the dynamic economic model and the measurement process to the\n",
    "probability law for the measured data.\n",
    "\n",
    "The model chosen for the data collection agency is an aspect of an\n",
    "econometric specification that can make big differences in inferences\n",
    "about the economic structure.\n",
    "\n",
    "Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] describes two alternative models of data generation\n",
    "in a [permanent income](https://python.quantecon.org/perm_income.html) economy in which the\n",
    "investment accelerator, the mechanism studied in these two quantecon lectures –  [Samuelson Multiplier-Accelerator](https://python.quantecon.org/samuelson.html) and\n",
    "[The Acceleration Principle and the Nature of Business Cycles](https://python.quantecon.org/chow_business_cycles.html) –  shapes  business cycle fluctuations.\n",
    "\n",
    "- In Model 1, the data collecting agency simply reports the\n",
    "  error-ridden data that it collects.  \n",
    "- In Model 2, the data collection agents first collects error-ridden data that satisfy\n",
    "  a classical errors-in-variables model, then filters the data,  and reports the filtered objects.  \n",
    "\n",
    "\n",
    "Although the two models have the same “deep parameters,” they produce\n",
    "quite different sets of restrictions on the data.\n",
    "\n",
    "In this lecture we follow Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] and study how these\n",
    "alternative measurement schemes affect  empirical implications.\n",
    "\n",
    "We start with imports and helper functions to be used throughout this lecture to generate LaTeX output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2721f15",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from IPython.display import Latex\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "def df_to_latex_matrix(df, label=''):\n",
    "    \"\"\"Convert DataFrame to LaTeX matrix.\"\"\"\n",
    "    lines = [r'\\begin{bmatrix}']\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_str = ' & '.join(\n",
    "          [f'{v:.4f}' if isinstance(v, (int, float)) \n",
    "            else str(v) for v in row]) + r' \\\\'\n",
    "        lines.append(row_str)\n",
    "\n",
    "    lines.append(r'\\end{bmatrix}')\n",
    "\n",
    "    if label:\n",
    "        return '$' + label + ' = ' + '\\n'.join(lines) + '$'\n",
    "    else:\n",
    "        return '$' + '\\n'.join(lines) + '$'\n",
    "\n",
    "def df_to_latex_array(df):\n",
    "    \"\"\"Convert DataFrame to LaTeX array.\"\"\"\n",
    "    n_rows, n_cols = df.shape\n",
    "\n",
    "    # Build column format (centered columns)\n",
    "    col_format = 'c' * (n_cols + 1)  # +1 for index\n",
    "\n",
    "    # Start array\n",
    "    lines = [r'\\begin{array}{' + col_format + '}']\n",
    "\n",
    "    # Header row\n",
    "    header = ' & '.join([''] + [str(c) for c in df.columns]) + r' \\\\'\n",
    "    lines.append(header)\n",
    "    lines.append(r'\\hline')\n",
    "\n",
    "    # Data rows\n",
    "    for idx, row in df.iterrows():\n",
    "        row_str = str(idx) + ' & ' + ' & '.join(\n",
    "          [f'{v:.3f}' if isinstance(v, (int, float)) else str(v) \n",
    "          for v in row]) + r' \\\\'\n",
    "        lines.append(row_str)\n",
    "\n",
    "    lines.append(r'\\end{array}')\n",
    "\n",
    "    return '$' + '\\n'.join(lines) + '$'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485316e7",
   "metadata": {},
   "source": [
    "## The economic model\n",
    "\n",
    "The data are generated by  a linear-quadratic version of a stochastic\n",
    "optimal growth model that is an instance of models described in this quantecon lecture: [The Permanent Income Model](https://python.quantecon.org/perm_income.html).\n",
    "\n",
    "A social planner chooses a stochastic process for $ \\{c_t, k_{t+1}\\}_{t=0}^\\infty $ that  maximizes\n",
    "\n",
    "\n",
    "<a id='equation-planner-obj'></a>\n",
    "$$\n",
    "E \\sum_{t=0}^{\\infty} \\beta^t \\left( u_0 + u_1 c_t - \\frac{u_2}{2} c_t^2 \\right) \\tag{43.1}\n",
    "$$\n",
    "\n",
    "subject to the restrictions imposed by the technology\n",
    "\n",
    "\n",
    "<a id='equation-tech-constraint'></a>\n",
    "$$\n",
    "c_t + k_{t+1} = f k_t + \\theta_t, \\qquad \\beta f^2 > 1. \\tag{43.2}\n",
    "$$\n",
    "\n",
    "Here $ c_t $ is consumption, $ k_t $ is the capital stock,\n",
    "$ f $ is the gross rate of return on capital,\n",
    "and $ \\theta_t $ is an endowment or technology shock following\n",
    "\n",
    "\n",
    "<a id='equation-shock-process'></a>\n",
    "$$\n",
    "a(L)\\,\\theta_t = \\varepsilon_t, \\tag{43.3}\n",
    "$$\n",
    "\n",
    "where $ L $ is the backward shift (or ‘lag’) operator and  $ a(z) = 1 - a_1 z - a_2 z^2 - \\cdots - a_r z^r $ having all its zeroes\n",
    "outside the unit circle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d41571",
   "metadata": {},
   "source": [
    "### Optimal decision rule\n",
    "\n",
    "The  optimal decision rule for $ c_t $ is\n",
    "\n",
    "\n",
    "<a id='equation-opt-decision'></a>\n",
    "$$\n",
    "c_t = \\frac{-\\alpha}{f-1}\n",
    "      + \\left(1 - \\frac{1}{\\beta f^2}\\right)\n",
    "        \\frac{L - f^{-1} a(f^{-1})^{-1} a(L)}{L - f^{-1}}\\,\\theta_t\n",
    "      + f k_t,\n",
    "\\qquad\n",
    "k_{t+1} = f k_t + \\theta_t - c_t, \\tag{43.4}\n",
    "$$\n",
    "\n",
    "where $ \\alpha = u_1[1-(\\beta f)^{-1}]/u_2 $.\n",
    "\n",
    "Equations [(43.3)](#equation-shock-process) and [(43.4)](#equation-opt-decision) exhibit the\n",
    "cross-equation restrictions characteristic of rational expectations\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66562",
   "metadata": {},
   "source": [
    "### Net income and the accelerator\n",
    "\n",
    "Define net output or national income as\n",
    "\n",
    "\n",
    "<a id='equation-net-income'></a>\n",
    "$$\n",
    "y_{nt} = (f-1)k_t + \\theta_t. \\tag{43.5}\n",
    "$$\n",
    "\n",
    "Note that [(43.2)](#equation-tech-constraint) and [(43.5)](#equation-net-income) imply\n",
    "$ (k_{t+1} - k_t) + c_t = y_{nt} $.\n",
    "\n",
    "To obtain both a version of Friedman [[1956](https://python.quantecon.org/zreferences.html#id185)]’s geometric\n",
    "distributed lag consumption function and a distributed lag\n",
    "accelerator, we impose two assumptions:\n",
    "\n",
    "1. $ a(L) = 1 $, so that $ \\theta_t $ is white noise.  \n",
    "1. $ \\beta f = 1 $, so the rate of return on capital equals the rate\n",
    "  of time preference.  \n",
    "\n",
    "\n",
    "Assumption 1 is crucial for the strict form of the accelerator.\n",
    "\n",
    "Relaxing it to allow serially correlated $ \\theta_t $ preserves an\n",
    "accelerator in a broad sense but loses the sharp geometric-lag\n",
    "form of [(43.8)](#equation-mm-accelerator).\n",
    "\n",
    "Adding a second shock breaks the one-index structure entirely\n",
    "and can generate nontrivial Granger causality even without\n",
    "measurement error.\n",
    "\n",
    "Assumption 2 is less important, affecting only various constants.\n",
    "\n",
    "Under both assumptions, [(43.4)](#equation-opt-decision) simplifies to\n",
    "\n",
    "\n",
    "<a id='equation-simple-crule'></a>\n",
    "$$\n",
    "c_t = (1-f^{-1})\\,\\theta_t + (f-1)\\,k_t. \\tag{43.6}\n",
    "$$\n",
    "\n",
    "When [(43.6)](#equation-simple-crule), [(43.5)](#equation-net-income), and\n",
    "[(43.2)](#equation-tech-constraint) are combined, the optimal plan satisfies\n",
    "\n",
    "\n",
    "<a id='equation-friedman-consumption'></a>\n",
    "$$\n",
    "c_t = \\left(\\frac{1-\\beta}{1-\\beta L}\\right) y_{nt}, \\tag{43.7}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-mm-accelerator'></a>\n",
    "$$\n",
    "k_{t+1} - k_t = f^{-1} \\left(\\frac{1-L}{1-\\beta L}\\right) y_{nt}, \\tag{43.8}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-income-process'></a>\n",
    "$$\n",
    "y_{nt} = \\theta_t + (1-\\beta)(\\theta_{t-1} + \\theta_{t-2} + \\cdots). \\tag{43.9}\n",
    "$$\n",
    "\n",
    "Equation [(43.7)](#equation-friedman-consumption) is Friedman’s consumption\n",
    "model: consumption is a geometric distributed lag of income,\n",
    "with the decay coefficient $ \\beta $ equal to the discount factor.\n",
    "\n",
    "Equation [(43.8)](#equation-mm-accelerator) is the distributed lag accelerator:\n",
    "investment is a geometric distributed lag of the first difference\n",
    "of income.\n",
    "\n",
    "This is the same mechanism that Chow [[1968](https://python.quantecon.org/zreferences.html#id294)] documented\n",
    "empirically (see [The Acceleration Principle and the Nature of Business Cycles](https://python.quantecon.org/chow_business_cycles.html)).\n",
    "\n",
    "Equation [(43.9)](#equation-income-process) states that the first difference of disposable income is a\n",
    "first-order moving average process with innovation equal to the innovation of the endowment shock $ \\theta_t $.\n",
    "\n",
    "As Muth [[1960](https://python.quantecon.org/zreferences.html#id143)] showed, such a process is optimally forecast\n",
    "via a geometric distributed lag or “adaptive expectations” scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe3e88",
   "metadata": {},
   "source": [
    "### The accelerator puzzle\n",
    "\n",
    "When all variables are measured accurately and are driven by\n",
    "the single shock $ \\theta_t $, the spectral density matrix of\n",
    "$ (c_t,\\, k_{t+1}-k_t,\\, y_{nt}) $ has rank one at all frequencies.\n",
    "\n",
    "Each variable is an invertible one-sided distributed lag of the\n",
    "same white noise, so no variable Granger-causes any other.\n",
    "\n",
    "Empirically, however, measures of output Granger-cause investment\n",
    "but not vice versa.\n",
    "\n",
    "Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] shows that measurement error can resolve\n",
    "this puzzle.\n",
    "\n",
    "To illustrate, suppose first that output $ y_{nt} $ is measured\n",
    "perfectly while consumption and capital are each polluted by\n",
    "serially correlated measurement errors $ v_{ct} $ and $ v_{kt} $\n",
    "orthogonal to $ \\theta_t $.\n",
    "\n",
    "Let $ \\bar c_t $ and $ \\bar k_{t+1} - \\bar k_t $ denote the measured\n",
    "series.  Then\n",
    "\n",
    "\n",
    "<a id='equation-meas-consumption'></a>\n",
    "$$\n",
    "\\bar c_t = \\left(\\frac{1-\\beta}{1-\\beta L}\\right) y_{nt} + v_{ct}, \\tag{43.10}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-meas-investment'></a>\n",
    "$$\n",
    "\\bar k_{t+1} - \\bar k_t\n",
    "  = \\beta\\left(\\frac{1-L}{1-\\beta L}\\right) y_{nt}\n",
    "  + (v_{k,t+1} - v_{kt}), \\tag{43.11}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-income-process-ma'></a>\n",
    "$$\n",
    "y_{nt} = \\theta_t + (1-\\beta)(\\theta_{t-1} + \\theta_{t-2} + \\cdots). \\tag{43.12}\n",
    "$$\n",
    "\n",
    "In this case income Granger-causes consumption and investment\n",
    "but is not Granger-caused by them.\n",
    "\n",
    "When each measured series is corrupted by measurement error, every\n",
    "measured variable will in general Granger-cause every other.\n",
    "\n",
    "The strength of this Granger causality, as measured by decompositions\n",
    "of $ j $-step-ahead prediction error variances, depends on the relative\n",
    "variances of the measurement errors.\n",
    "\n",
    "In this case, each observed series mixes the common signal $ \\theta_t $\n",
    "with idiosyncratic measurement noise.\n",
    "\n",
    "A series with lower measurement\n",
    "error variance tracks $ \\theta_t $ more closely, so its innovations\n",
    "contain more information about future values of the other series.\n",
    "\n",
    "Accordingly, in a forecast-error-variance decomposition, shocks to\n",
    "better-measured series account for a larger share of other variables’\n",
    "$ j $-step-ahead prediction errors.\n",
    "\n",
    "In a one-common-index model like this one ($ \\theta_t $ is the common\n",
    "index), better-measured variables extend more Granger causality to\n",
    "less well measured series than vice versa.\n",
    "\n",
    "This asymmetry drives the numerical results we observe soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5be97",
   "metadata": {},
   "source": [
    "### State-space formulation\n",
    "\n",
    "Let’s map the economic model and the measurement process into\n",
    "a linear  state-space framework.\n",
    "\n",
    "Set $ f = 1.05 $ and $ \\theta_t \\sim \\mathcal{N}(0, 1) $.\n",
    "\n",
    "Define the state and observation vectors\n",
    "\n",
    "$$\n",
    "x_t = \\begin{bmatrix} k_t \\\\ \\theta_t \\end{bmatrix},\n",
    "\\qquad\n",
    "z_t = \\begin{bmatrix} y_{nt} \\\\ c_t \\\\ \\Delta k_t \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "so that the error-free data are described by  the state-space system\n",
    "\n",
    "\n",
    "<a id='equation-true-ss'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{t+1} &= A x_t + \\varepsilon_t, \\\\\n",
    "z_t &= C x_t.\n",
    "\\end{aligned} \\tag{43.13}\n",
    "$$\n",
    "\n",
    "where $ \\varepsilon_t = \\begin{bmatrix} 0 \\\\ \\theta_t \\end{bmatrix} $ has\n",
    "covariance $ E \\varepsilon_t \\varepsilon_t^\\top = Q $ and the matrices are\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & f^{-1} \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "C = \\begin{bmatrix}\n",
    "f-1 & 1 \\\\\n",
    "f-1 & 1-f^{-1} \\\\\n",
    "0   & f^{-1}\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "Q = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$ Q $ is singular because there is only one source of randomness\n",
    "$ \\theta_t $; the capital stock $ k_t $ evolves deterministically\n",
    "given $ \\theta_t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130d773",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Baseline structural matrices for the true economy\n",
    "f = 1.05\n",
    "β = 1 / f\n",
    "\n",
    "A = np.array([\n",
    "    [1.0, 1.0 / f],\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "\n",
    "C = np.array([\n",
    "    [f - 1.0, 1.0],\n",
    "    [f - 1.0, 1.0 - 1.0 / f],\n",
    "    [0.0, 1.0 / f]\n",
    "])\n",
    "\n",
    "Q = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96872b",
   "metadata": {},
   "source": [
    "\n",
    "<a id='true-impulse-responses'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08132a9e",
   "metadata": {},
   "source": [
    "### True impulse responses\n",
    "\n",
    "Before introducing measurement error, we compute the impulse response of\n",
    "the error-free variables  to a unit shock $ \\theta_0 = 1 $.\n",
    "\n",
    "This benchmark clarifies what changes when we later switch from\n",
    "error-free  variables to  variables reported by the statistical agency.\n",
    "\n",
    "The response shows the investment accelerator clearly: the full impact on\n",
    "net income $ y_n $ occurs at lag 0, while consumption adjusts by only\n",
    "$ 1 - f^{-1} \\approx 0.048 $ and investment absorbs the remainder.\n",
    "\n",
    "From lag 1 onward the economy is in its new steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854b139",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def table2_irf(A, C, n_lags=6):\n",
    "    x = np.array([0.0, 1.0])  # k_0 = 0, theta_0 = 1\n",
    "    rows = []\n",
    "    for j in range(n_lags):\n",
    "        y_n, c, d_k = C @ x\n",
    "        rows.append([y_n, c, d_k])\n",
    "        x = A @ x\n",
    "    return pd.DataFrame(rows, columns=[r'y_n', r'c', r'\\Delta k'],\n",
    "                         index=pd.Index(range(n_lags), name='lag'))\n",
    "\n",
    "table2 = table2_irf(A, C, n_lags=6)\n",
    "display(Latex(df_to_latex_array(table2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafa243",
   "metadata": {},
   "source": [
    "## Measurement errors\n",
    "\n",
    "Let’s add the measurement layer that generates reported data.\n",
    "\n",
    "The econometrician does not observe $ z_t $ directly but instead\n",
    "sees $ \\bar z_t = z_t + v_t $, where $ v_t $ is a vector of measurement\n",
    "errors.\n",
    "\n",
    "Measurement errors follow an AR(1) process\n",
    "\n",
    "\n",
    "<a id='equation-meas-error-ar1'></a>\n",
    "$$\n",
    "v_{t+1} = D v_t + \\eta_t, \\tag{43.14}\n",
    "$$\n",
    "\n",
    "where $ \\eta_t $ is a vector white noise with\n",
    "$ E \\eta_t \\eta_t^\\top = \\Sigma_\\eta $ and\n",
    "$ E \\varepsilon_t v_s^\\top = 0 $ for all $ t, s $.\n",
    "\n",
    "The parameters are\n",
    "\n",
    "$$\n",
    "D = \\operatorname{diag}(0.6, 0.7, 0.3),\n",
    "\\qquad\n",
    "\\sigma_\\eta = (0.05, 0.035, 0.65),\n",
    "$$\n",
    "\n",
    "so the unconditional covariance of $ v_t $ is\n",
    "\n",
    "$$\n",
    "R = \\operatorname{diag}\\!\\left(\\frac{\\sigma_{\\eta,i}^2}{1 - \\rho_i^2}\\right).\n",
    "$$\n",
    "\n",
    "The innovation variances are smallest for consumption\n",
    "($ \\sigma_\\eta = 0.035 $), next for income ($ \\sigma_\\eta = 0.05 $),\n",
    "and largest for investment ($ \\sigma_\\eta = 0.65 $).\n",
    "\n",
    "As in Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] and our discussion above, what matters for Granger-causality\n",
    "asymmetries is the overall measurement quality in the full system:\n",
    "output is relatively well measured while investment is relatively\n",
    "poorly measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc2c05",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ρ = np.array([0.6, 0.7, 0.3])\n",
    "D = np.diag(ρ)\n",
    "\n",
    "# Innovation std. devs of η_t\n",
    "σ_η = np.array([0.05, 0.035, 0.65])\n",
    "Σ_η = np.diag(σ_η**2)\n",
    "\n",
    "# Unconditional covariance of measurement errors v_t\n",
    "R = np.diag((σ_η / np.sqrt(1.0 - ρ**2))**2)\n",
    "\n",
    "print(f\"f = {f},  β = 1/f = {β:.6f}\")\n",
    "print()\n",
    "display(Latex(df_to_latex_matrix(pd.DataFrame(A), 'A')))\n",
    "display(Latex(df_to_latex_matrix(pd.DataFrame(C), 'C')))\n",
    "display(Latex(df_to_latex_matrix(pd.DataFrame(D), 'D')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1e568",
   "metadata": {},
   "source": [
    "We will analyze the two reporting schemes separately, but first we need a solver for the steady-state Kalman gain and error covariances.\n",
    "\n",
    "The function below iterates on the Riccati equation until convergence,\n",
    "returning the Kalman gain $ K $, the state covariance $ S $, and the\n",
    "innovation covariance $ V $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f16ccd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def steady_state_kalman(A, C_obs, Q, R, W=None, tol=1e-13, max_iter=200_000):\n",
    "    \"\"\"\n",
    "    Solve steady-state Kalman equations for\n",
    "        x_{t+1} = A x_t + w_{t+1}\n",
    "        y_t     = C_obs x_t + v_t\n",
    "    with cov(w)=Q, cov(v)=R, cov(w,v)=W.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    m = C_obs.shape[0]\n",
    "    if W is None:\n",
    "        W = np.zeros((n, m))\n",
    "\n",
    "    S = Q.copy()\n",
    "    for _ in range(max_iter):\n",
    "        V = C_obs @ S @ C_obs.T + R\n",
    "        K = (A @ S @ C_obs.T + W) @ np.linalg.inv(V)\n",
    "        S_new = Q + A @ S @ A.T - K @ V @ K.T\n",
    "\n",
    "        if np.max(np.abs(S_new - S)) < tol:\n",
    "            S = S_new\n",
    "            break\n",
    "        S = S_new\n",
    "\n",
    "    V = C_obs @ S @ C_obs.T + R\n",
    "    K = (A @ S @ C_obs.T + W) @ np.linalg.inv(V)\n",
    "    return K, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876684ed",
   "metadata": {},
   "source": [
    "With structural matrices and tools we need in place, we now follow\n",
    "Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)]’s two reporting schemes in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa54321",
   "metadata": {},
   "source": [
    "## A classical model of measurements initially collected by an agency\n",
    "\n",
    "A data collecting agency observes a noise-corrupted version of $ z_t $, namely\n",
    "\n",
    "\n",
    "<a id='equation-model1-obs'></a>\n",
    "$$\n",
    "\\bar z_t = C x_t + v_t. \\tag{43.15}\n",
    "$$\n",
    "\n",
    "We refer to this as *Model 1*: the agency collects noisy\n",
    "data and reports them without filtering.\n",
    "\n",
    "To represent the second moments of the $ \\bar z_t $ process, it is\n",
    "convenient to obtain its population vector autoregression.\n",
    "\n",
    "The error vector in the vector autoregression is the\n",
    "innovation to $ \\bar z_t $ and can be taken to be the white noise in a Wold\n",
    "moving average representation, which can be obtained by “inverting”\n",
    "the autoregressive representation.\n",
    "\n",
    "The population vector autoregression, and how it depends on the\n",
    "parameters of the state-space system and the measurement error process,\n",
    "carries insights about how to interpret estimated vector\n",
    "autoregressions for $ \\bar z_t $.\n",
    "\n",
    "Constructing the vector autoregression is also useful as an\n",
    "intermediate step in computing the likelihood of a sample of\n",
    "$ \\bar z_t $’s as a function of the free parameters\n",
    "$ \\{A, C, D, Q, R\\} $.\n",
    "\n",
    "The particular method that will be used to construct the vector\n",
    "autoregressive representation also proves useful as an intermediate\n",
    "step in constructing a model of an optimal reporting agency.\n",
    "\n",
    "We use recursive (Kalman filtering) methods to obtain the\n",
    "vector autoregression for $ \\bar z_t $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473e957",
   "metadata": {},
   "source": [
    "### Quasi-differencing\n",
    "\n",
    "Because the measurement errors $ v_t $ are serially correlated,\n",
    "the standard Kalman filter with white-noise measurement error\n",
    "cannot be applied directly to $ \\bar z_t = C x_t + v_t $.\n",
    "\n",
    "An alternative is to augment the state vector with the\n",
    "measurement-error AR components (see Appendix B of\n",
    "Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)]).\n",
    "\n",
    "Here we take the quasi-differencing route described in\n",
    "Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)], which reduces the\n",
    "system to one with serially uncorrelated observation noise.\n",
    "\n",
    "Define\n",
    "\n",
    "\n",
    "<a id='equation-model1-qd'></a>\n",
    "$$\n",
    "\\tilde z_t = \\bar z_{t+1} - D \\bar z_t, \\qquad\n",
    "\\bar\\nu_t = C \\varepsilon_t + \\eta_t, \\qquad\n",
    "\\bar C = CA - DC. \\tag{43.16}\n",
    "$$\n",
    "\n",
    "Then the state-space system [(43.13)](#equation-true-ss), the measurement error\n",
    "process [(43.14)](#equation-meas-error-ar1), and the observation equation [(43.15)](#equation-model1-obs)\n",
    "imply the state-space system\n",
    "\n",
    "\n",
    "<a id='equation-model1-transformed'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{t+1} &= A x_t + \\varepsilon_t, \\\\\n",
    "\\tilde z_t &= \\bar C\\, x_t + \\bar\\nu_t,\n",
    "\\end{aligned} \\tag{43.17}\n",
    "$$\n",
    "\n",
    "where $ (\\varepsilon_t, \\bar\\nu_t) $ is a white noise process with\n",
    "\n",
    "\n",
    "<a id='equation-model1-covs'></a>\n",
    "$$\n",
    "E \\begin{bmatrix} \\varepsilon_t \\end{bmatrix}\n",
    "\\begin{bmatrix} \\varepsilon_t^\\top & \\bar\\nu_t^\\top \\end{bmatrix}\n",
    "= \\begin{bmatrix} Q & W_1 \\\\ W_1^\\top & R_1 \\end{bmatrix},\n",
    "\\qquad\n",
    "R_1 = C Q C^\\top + R, \\quad W_1 = Q C^\\top. \\tag{43.18}\n",
    "$$\n",
    "\n",
    "System [(43.17)](#equation-model1-transformed) with covariances [(43.18)](#equation-model1-covs) is\n",
    "characterized by the five matrices\n",
    "$ [A, \\bar C, Q, R_1, W_1] $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48121a21",
   "metadata": {},
   "source": [
    "### Innovations representation\n",
    "\n",
    "Associated with [(43.17)](#equation-model1-transformed) and [(43.18)](#equation-model1-covs) is the\n",
    "**innovations representation** for $ \\tilde z_t $,\n",
    "\n",
    "\n",
    "<a id='equation-model1-innov'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat x_{t+1} &= A \\hat x_t + K_1 u_t, \\\\\n",
    "\\tilde z_t &= \\bar C \\hat x_t + u_t,\n",
    "\\end{aligned} \\tag{43.19}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-model1-innov-defs'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat x_t &= E[x_t \\mid \\tilde z_{t-1}, \\tilde z_{t-2}, \\ldots, \\hat x_0]\n",
    "         = E[x_t \\mid \\bar z_t, \\bar z_{t-1}, \\ldots], \\\\\n",
    "u_t &= \\tilde z_t - E[\\tilde z_t \\mid \\tilde z_{t-1}, \\tilde z_{t-2}, \\ldots]\n",
    "     = \\bar z_{t+1} - E[\\bar z_{t+1} \\mid \\bar z_t, \\bar z_{t-1}, \\ldots],\n",
    "\\end{aligned} \\tag{43.20}\n",
    "$$\n",
    "\n",
    "$ [K_1, S_1] $ are computed from the steady-state Kalman filter applied to\n",
    "$ [A, \\bar C, Q, R_1, W_1] $, and\n",
    "\n",
    "\n",
    "<a id='equation-model1-s1'></a>\n",
    "$$\n",
    "S_1 = E[(x_t - \\hat x_t)(x_t - \\hat x_t)^\\top]. \\tag{43.21}\n",
    "$$\n",
    "\n",
    "From [(43.20)](#equation-model1-innov-defs), $ u_t $ is the innovation process for the\n",
    "$ \\bar z_t $ process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fa4b5",
   "metadata": {},
   "source": [
    "### Wold representation\n",
    "\n",
    "System [(43.19)](#equation-model1-innov) and definition [(43.16)](#equation-model1-qd) can be used to\n",
    "obtain a Wold vector moving average representation for the $ \\bar z_t $ process:\n",
    "\n",
    "\n",
    "<a id='equation-model1-wold'></a>\n",
    "$$\n",
    "\\bar z_{t+1} = (I - DL)^{-1}\\bigl[\\bar C(I - AL)^{-1}K_1 L + I\\bigr] u_t, \\tag{43.22}\n",
    "$$\n",
    "\n",
    "where $ L $ is the lag operator.\n",
    "\n",
    "From [(43.17)](#equation-model1-transformed) and [(43.19)](#equation-model1-innov) the innovation\n",
    "covariance is\n",
    "\n",
    "\n",
    "<a id='equation-model1-v1'></a>\n",
    "$$\n",
    "V_1 = E\\, u_t u_t^\\top = \\bar C\\, S_1\\, \\bar C^\\top + R_1. \\tag{43.23}\n",
    "$$\n",
    "\n",
    "Below we compute $ K_1 $, $ S_1 $, and $ V_1 $ numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2fe19",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "C_bar = C @ A - D @ C\n",
    "R1 = C @ Q @ C.T + R\n",
    "W1 = Q @ C.T\n",
    "\n",
    "K1, S1, V1 = steady_state_kalman(A, C_bar, Q, R1, W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb40466",
   "metadata": {},
   "source": [
    "### Computing  coefficients in a Wold moving average representation\n",
    "\n",
    "To compute the moving average  coefficients in [(43.22)](#equation-model1-wold) numerically,\n",
    "define the augmented state\n",
    "\n",
    "$$\n",
    "r_t = \\begin{bmatrix} \\hat x_{t-1} \\\\ \\bar z_{t-1} \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "with dynamics\n",
    "\n",
    "$$\n",
    "r_{t+1} = F_1 r_t + G_1 u_t,\n",
    "\\qquad\n",
    "\\bar z_t = H_1 r_t + u_t,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "F_1 =\n",
    "\\begin{bmatrix}\n",
    "A & 0 \\\\\n",
    "\\bar C & D\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "G_1 =\n",
    "\\begin{bmatrix}\n",
    "K_1 \\\\\n",
    "I\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "H_1 = [\\bar C \\;\\; D].\n",
    "$$\n",
    "\n",
    "The moving average  coefficients are then $ \\psi_0 = I $ and\n",
    "$ \\psi_j = H_1 F_1^{j-1} G_1 $ for $ j \\geq 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a6d4c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "F1 = np.block([\n",
    "    [A, np.zeros((2, 3))],\n",
    "    [C_bar, D]\n",
    "])\n",
    "G1 = np.vstack([K1, np.eye(3)])\n",
    "H1 = np.hstack([C_bar, D])\n",
    "\n",
    "\n",
    "def measured_wold_coeffs(F, G, H, n_terms=25):\n",
    "    psi = [np.eye(3)]\n",
    "    Fpow = np.eye(F.shape[0])\n",
    "    for _ in range(1, n_terms):\n",
    "        psi.append(H @ Fpow @ G)\n",
    "        Fpow = Fpow @ F\n",
    "    return psi\n",
    "\n",
    "\n",
    "def fev_contributions(psi, V, n_horizons=20):\n",
    "    \"\"\"\n",
    "    Returns contrib[var, shock, h-1] = contribution at horizon h.\n",
    "    \"\"\"\n",
    "    P = linalg.cholesky(V, lower=True)\n",
    "    out = np.zeros((3, 3, n_horizons))\n",
    "    for h in range(1, n_horizons + 1):\n",
    "        acc = np.zeros((3, 3))\n",
    "        for j in range(h):\n",
    "            T = psi[j] @ P\n",
    "            acc += T**2\n",
    "        out[:, :, h - 1] = acc\n",
    "    return out\n",
    "\n",
    "\n",
    "psi1 = measured_wold_coeffs(F1, G1, H1, n_terms=40)\n",
    "resp1 = np.array(\n",
    "      [psi1[j] @ linalg.cholesky(V1, lower=True) for j in range(14)])\n",
    "decomp1 = fev_contributions(psi1, V1, n_horizons=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d020a33",
   "metadata": {},
   "source": [
    "### Gaussian likelihood\n",
    "\n",
    "The Gaussian log-likelihood function for a sample\n",
    "$ \\{\\bar z_t,\\, t=0,\\ldots,T\\} $, conditioned on an initial state estimate\n",
    "$ \\hat x_0 $, can be represented as\n",
    "\n",
    "\n",
    "<a id='equation-model1-loglik'></a>\n",
    "$$\n",
    "\\mathcal{L}^* = -T\\ln 2\\pi - \\tfrac{1}{2}T\\ln|V_1|\n",
    "  - \\tfrac{1}{2}\\sum_{t=0}^{T-1} u_t^\\top V_1^{-1} u_t, \\tag{43.24}\n",
    "$$\n",
    "\n",
    "where $ u_t $ is a function of $ \\{\\bar z_t\\} $ defined by\n",
    "[(43.25)](#equation-model1-recursion) below.\n",
    "\n",
    "To use [(43.19)](#equation-model1-innov) to compute $ \\{u_t\\} $, it is useful to\n",
    "represent it as\n",
    "\n",
    "\n",
    "<a id='equation-model1-recursion'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat x_{t+1} &= (A - K_1 \\bar C)\\,\\hat x_t + K_1 \\tilde z_t, \\\\\n",
    "u_t &= -\\bar C\\,\\hat x_t + \\tilde z_t,\n",
    "\\end{aligned} \\tag{43.25}\n",
    "$$\n",
    "\n",
    "where $ \\tilde z_t = \\bar z_{t+1} - D\\bar z_t $ is the quasi-differenced\n",
    "observation.\n",
    "\n",
    "Given $ \\hat x_0 $, equation [(43.25)](#equation-model1-recursion) can be used recursively\n",
    "to compute a $ \\{u_t\\} $ process.\n",
    "\n",
    "Equations [(43.24)](#equation-model1-loglik) and [(43.25)](#equation-model1-recursion) give the\n",
    "likelihood function of a sample of error-corrupted data\n",
    "$ \\{\\bar z_t\\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29e11a",
   "metadata": {},
   "source": [
    "### Forecast-error-variance decomposition\n",
    "\n",
    "To measure the relative importance of each innovation, we decompose\n",
    "the $ j $-step-ahead forecast-error variance of each measured variable.\n",
    "\n",
    "Write $ \\bar z_{t+j} - E_t \\bar z_{t+j} = \\sum_{i=0}^{j-1} \\psi_i u_{t+j-i} $.\n",
    "\n",
    "Let $ P $ be the lower-triangular Cholesky factor of $ V_1 $ so that the\n",
    "orthogonalized innovations are $ e_t = P^{-1} u_t $.\n",
    "\n",
    "Then the contribution of orthogonalized innovation $ k $ to the\n",
    "$ j $-step-ahead variance of variable $ m $ is\n",
    "$ \\sum_{i=0}^{j-1} (\\psi_i P)_{mk}^2 $.\n",
    "\n",
    "The table below shows the cumulative contribution of each orthogonalized\n",
    "innovation to the forecast-error variance of $ y_n $, $ c $, and $ \\Delta k $\n",
    "at horizons 1 through 20.\n",
    "\n",
    "Each panel fixes one orthogonalized innovation and reports its\n",
    "cumulative contribution to each variable’s forecast-error variance.\n",
    "\n",
    "Rows are forecast horizons and columns are forecasted variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e1693",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "horizons = np.arange(1, 21)\n",
    "labels = [r'y_n', r'c', r'\\Delta k']\n",
    "\n",
    "def fev_table(decomp, shock_idx, horizons):\n",
    "    return pd.DataFrame(\n",
    "        np.round(decomp[:, shock_idx, :].T, 4),\n",
    "        columns=labels,\n",
    "        index=pd.Index(horizons, name='j')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc053ce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "shock_titles = [r'\\text{A. Innovation in } y_n',\n",
    "                r'\\text{B. Innovation in } c',\n",
    "                r'\\text{C. Innovation in } \\Delta k']\n",
    "\n",
    "parts = []\n",
    "for i, title in enumerate(shock_titles):\n",
    "    arr = df_to_latex_array(fev_table(decomp1, i, horizons)).strip('$')\n",
    "    parts.append(\n",
    "      r'\\begin{array}{c} ' + title + r' \\\\ ' + arr + r' \\end{array}')\n",
    "\n",
    "display(Latex('$' + r' \\quad '.join(parts) + '$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529f31d",
   "metadata": {},
   "source": [
    "The income innovation accounts for substantial proportions of\n",
    "forecast-error variance in all three variables, while the consumption and\n",
    "investment innovations contribute mainly to their own variances.\n",
    "\n",
    "This is a **Granger causality** pattern: income appears to\n",
    "Granger-cause consumption and investment, but not vice versa.\n",
    "\n",
    "This matches the paper’s message that, in a one-common-index model,\n",
    "the relatively best measured series has the strongest predictive content.\n",
    "\n",
    "Let’s look at the covariance matrix of the innovations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed77526",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Covariance matrix of innovations:')\n",
    "df_v1 = pd.DataFrame(np.round(V1, 4), index=labels, columns=labels)\n",
    "display(Latex(df_to_latex_matrix(df_v1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd8c91",
   "metadata": {},
   "source": [
    "The covariance matrix of the innovations is not diagonal, but the\n",
    "eigenvalues are well separated as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf1f6f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Eigenvalues of covariance matrix:')\n",
    "print(np.sort(np.linalg.eigvalsh(V1))[::-1].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1bf65",
   "metadata": {},
   "source": [
    "The first eigenvalue is much larger than the others, consistent with\n",
    "the presence of a dominant common shock $ \\theta_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb96e7",
   "metadata": {},
   "source": [
    "### Wold impulse responses\n",
    "\n",
    "Impulse responses in the Wold representation are reported using orthogonalized\n",
    "innovations (Cholesky factorization of $ V_1 $ with ordering\n",
    "$ y_n $, $ c $, $ \\Delta k $).\n",
    "\n",
    "Under this method, lag-0 responses reflect both\n",
    "contemporaneous covariance and the Cholesky ordering.\n",
    "\n",
    "We first define a helper function to format the  response coefficients as a LaTeX array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d9e19",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lags = np.arange(14)\n",
    "\n",
    "def wold_response_table(resp, shock_idx, lags):\n",
    "    return pd.DataFrame(\n",
    "        np.round(resp[:, :, shock_idx], 4),\n",
    "        columns=labels,\n",
    "        index=pd.Index(lags, name='j')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9485dbe7",
   "metadata": {},
   "source": [
    "Now we report the impulse  responses to each orthogonalized innovation in a single table with three panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae80f7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "wold_titles = [r'\\text{A. Response to } y_n \\text{ innovation}',\n",
    "               r'\\text{B. Response to } c \\text{ innovation}',\n",
    "               r'\\text{C. Response to } \\Delta k \\text{ innovation}']\n",
    "\n",
    "parts = []\n",
    "for i, title in enumerate(wold_titles):\n",
    "    arr = df_to_latex_array(wold_response_table(resp1, i, lags)).strip('$')\n",
    "    parts.append(\n",
    "      r'\\begin{array}{c} ' + title + r' \\\\ ' + arr + r' \\end{array}')\n",
    "\n",
    "display(Latex('$' + r' \\quad '.join(parts) + '$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96d76f",
   "metadata": {},
   "source": [
    "At impact, the first orthogonalized innovation\n",
    "loads on all three measured variables.\n",
    "\n",
    "At subsequent lags the income innovation generates persistent\n",
    "responses in all three variables because, being the best-measured\n",
    "series, its innovation is dominated by the true permanent shock\n",
    "$ \\theta_t $.\n",
    "\n",
    "The consumption and investment innovations produce responses that\n",
    "decay according to the AR(1) structure of their respective\n",
    "measurement errors ($ \\rho_c = 0.7 $, $ \\rho_{\\Delta k} = 0.3 $),\n",
    "with little spillover to other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48639a06",
   "metadata": {},
   "source": [
    "## A model of optimal estimates reported by an agency\n",
    "\n",
    "Suppose that instead of reporting the error-corrupted data $ \\bar z_t $,\n",
    "the data collecting agency reports linear least-squares projections of\n",
    "the true data on a history of the error-corrupted data.\n",
    "\n",
    "This model provides a possible way of interpreting two features of\n",
    "the data-reporting process.\n",
    "\n",
    "- *seasonal adjustment*: if the components of $ v_t $ have\n",
    "  strong seasonals, the optimal filter will assume a shape that can be\n",
    "  interpreted partly in terms of a seasonal adjustment filter, one that\n",
    "  is one-sided in current and past $ \\bar z_t $’s.  \n",
    "- *data revisions*: if $ z_t $ contains current and lagged\n",
    "  values of some variable of interest, then the model simultaneously\n",
    "  determines “preliminary,” “revised,” and “final” estimates as\n",
    "  successive conditional expectations based on progressively longer\n",
    "  histories of error-ridden observations.  \n",
    "\n",
    "\n",
    "To make this operational, we impute to the reporting agency a model of\n",
    "the joint process generating the true data and the measurement errors.\n",
    "\n",
    "We assume that the reporting agency has “rational expectations”: it\n",
    "knows the economic and measurement structure leading to\n",
    "[(43.17)](#equation-model1-transformed)–[(43.18)](#equation-model1-covs).\n",
    "\n",
    "To prepare its estimates, the reporting agency itself computes the\n",
    "Kalman filter to obtain the innovations representation [(43.19)](#equation-model1-innov).\n",
    "\n",
    "Rather than reporting the error-corrupted data $ \\bar z_t $, the agency\n",
    "reports $ \\tilde z_t = G \\hat x_t $, where $ G $ is a “selection matrix,”\n",
    "possibly equal to $ C $, for the data reported by the agency.\n",
    "\n",
    "The data $ G \\hat x_t = E[G x_t \\mid \\bar z_t, \\bar z_{t-1}, \\ldots, \\hat x_0] $.\n",
    "\n",
    "The state-space representation for the reported data is then\n",
    "\n",
    "\n",
    "<a id='equation-model2-state'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat x_{t+1} &= A \\hat x_t + K_1 u_t, \\\\\n",
    "\\tilde z_t &= G \\hat x_t,\n",
    "\\end{aligned} \\tag{43.26}\n",
    "$$\n",
    "\n",
    "where the first line of [(43.26)](#equation-model2-state) is from the innovations\n",
    "representation [(43.19)](#equation-model1-innov).\n",
    "\n",
    "Note that $ u_t $ is the innovation to $ \\bar z_{t+1} $ and is *not* the\n",
    "innovation to $ \\tilde z_t $.\n",
    "\n",
    "To obtain a Wold representation for $ \\tilde z_t $ and the likelihood\n",
    "function for a sample of $ \\tilde z_t $ requires that we obtain an\n",
    "innovations representation for [(43.26)](#equation-model2-state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f20ee",
   "metadata": {},
   "source": [
    "### Innovations representation for filtered data\n",
    "\n",
    "To add a little generality to [(43.26)](#equation-model2-state) we amend it to the system\n",
    "\n",
    "\n",
    "<a id='equation-model2-obs'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat x_{t+1} &= A \\hat x_t + K_1 u_t, \\\\\n",
    "\\tilde z_t &= G \\hat x_t + \\eta_t,\n",
    "\\end{aligned} \\tag{43.27}\n",
    "$$\n",
    "\n",
    "where $ \\eta_t $ is a type 2 white-noise measurement error process\n",
    "(“typos”) with presumably very small covariance matrix $ R_2 $.\n",
    "\n",
    "The covariance matrix of the joint noise is\n",
    "\n",
    "\n",
    "<a id='equation-model2-q'></a>\n",
    "$$\n",
    "E \\begin{bmatrix} K_1 u_t \\\\ \\eta_t \\end{bmatrix}\n",
    "  \\begin{bmatrix} K_1 u_t \\\\ \\eta_t \\end{bmatrix}^\\top\n",
    "= \\begin{bmatrix} Q_2 & 0 \\\\ 0 & R_2 \\end{bmatrix}, \\tag{43.28}\n",
    "$$\n",
    "\n",
    "where $ Q_2 = K_1 V_1 K_1^\\top $.\n",
    "\n",
    "If $ R_2 $ is singular, it is necessary to adjust the Kalman filtering\n",
    "formulas by using transformations that induce a “reduced order observer.”\n",
    "\n",
    "In practice, we approximate a zero $ R_2 $ matrix with the matrix\n",
    "$ \\epsilon I $ for a small $ \\epsilon > 0 $ to keep the Kalman filter\n",
    "numerically well-conditioned.\n",
    "\n",
    "For system [(43.27)](#equation-model2-obs) and [(43.28)](#equation-model2-q), an innovations\n",
    "representation is\n",
    "\n",
    "\n",
    "<a id='equation-model2-innov'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\check{x}_{t+1} &= A \\check{x}_t + K_2 a_t, \\\\\n",
    "\\tilde z_t &= G \\check{x}_t + a_t,\n",
    "\\end{aligned} \\tag{43.29}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-model2-innov-defs'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_t &= \\tilde z_t - E[\\tilde z_t \\mid \\tilde z_{t-1}, \\tilde z_{t-2}, \\ldots], \\\\\n",
    "\\check{x}_t &= E[\\hat x_t \\mid \\tilde z_{t-1}, \\tilde z_{t-2}, \\ldots, \\check{x}_0], \\\\\n",
    "S_2 &= E[(\\hat x_t - \\check{x}_t)(\\hat x_t - \\check{x}_t)^\\top], \\\\\n",
    "[K_2, S_2] &= \\text{kalmanfilter}(A, G, Q_2, R_2, 0).\n",
    "\\end{aligned} \\tag{43.30}\n",
    "$$\n",
    "\n",
    "Thus $ \\{a_t\\} $ is the innovation process for the reported data\n",
    "$ \\tilde z_t $, with innovation covariance\n",
    "\n",
    "\n",
    "<a id='equation-model2-v2'></a>\n",
    "$$\n",
    "V_2 = E\\, a_t a_t^\\top = G\\, S_2\\, G^\\top + R_2. \\tag{43.31}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197000b8",
   "metadata": {},
   "source": [
    "### Wold representation\n",
    "\n",
    "A Wold moving average representation for $ \\tilde z_t $ is found from\n",
    "[(43.29)](#equation-model2-innov) to be\n",
    "\n",
    "\n",
    "<a id='equation-model2-wold'></a>\n",
    "$$\n",
    "\\tilde z_t = \\bigl[G(I - AL)^{-1} K_2 L + I\\bigr] a_t, \\tag{43.32}\n",
    "$$\n",
    "\n",
    "with coefficients $ \\psi_0 = I $ and $ \\psi_j = G A^{j-1} K_2 $ for\n",
    "$ j \\geq 1 $.\n",
    "\n",
    "Note that this is simpler than the Model 1 Wold\n",
    "representation [(43.22)](#equation-model1-wold) because there is no quasi-differencing\n",
    "to undo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd781f",
   "metadata": {},
   "source": [
    "### Gaussian likelihood\n",
    "\n",
    "When a method analogous to Model 1 is used, a Gaussian log-likelihood\n",
    "for $ \\tilde z_t $ can be computed by first computing an $ \\{a_t\\} $ sequence\n",
    "from observations on $ \\tilde z_t $ by using\n",
    "\n",
    "\n",
    "<a id='equation-model2-recursion'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\check{x}_{t+1} &= (A - K_2 G)\\,\\check{x}_t + K_2 \\tilde z_t, \\\\\n",
    "a_t &= -G\\,\\check{x}_t + \\tilde z_t.\n",
    "\\end{aligned} \\tag{43.33}\n",
    "$$\n",
    "\n",
    "The likelihood function for a sample of $ T $ observations\n",
    "$ \\{\\tilde z_t\\} $ is then\n",
    "\n",
    "\n",
    "<a id='equation-model2-loglik'></a>\n",
    "$$\n",
    "\\mathcal{L}^{**} = -T\\ln 2\\pi - \\tfrac{1}{2}T\\ln|V_2|\n",
    "  - \\tfrac{1}{2}\\sum_{t=0}^{T-1} a_t^\\top V_2^{-1} a_t. \\tag{43.34}\n",
    "$$\n",
    "\n",
    "Note that relative to computing the likelihood function\n",
    "[(43.24)](#equation-model1-loglik) for the error-corrupted data, computing the\n",
    "likelihood function for the optimally filtered data requires more\n",
    "calculations.\n",
    "\n",
    "Both likelihood functions require that the Kalman filter\n",
    "[(43.20)](#equation-model1-innov-defs) be computed, while the likelihood function for\n",
    "the filtered data requires that the Kalman filter\n",
    "[(43.30)](#equation-model2-innov-defs) also be computed.\n",
    "\n",
    "In effect, in order to interpret and use the filtered data reported by\n",
    "the agency, it is necessary to retrace the steps that the agency used\n",
    "to synthesize those data.\n",
    "\n",
    "The Kalman filter [(43.20)](#equation-model1-innov-defs) is supposed to be formed by\n",
    "the agency.\n",
    "\n",
    "The agency need not use Kalman filter [(43.30)](#equation-model2-innov-defs) because\n",
    "it does not need the Wold representation for the filtered data.\n",
    "\n",
    "In our parameterization $ G = C $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c93633",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Q2 = K1 @ V1 @ K1.T\n",
    "ε = 1e-6\n",
    "\n",
    "K2, S2, V2 = steady_state_kalman(A, C, Q2, ε * np.eye(3))\n",
    "\n",
    "\n",
    "def filtered_wold_coeffs(A, C, K, n_terms=25):\n",
    "    psi = [np.eye(3)]\n",
    "    Apow = np.eye(2)\n",
    "    for _ in range(1, n_terms):\n",
    "        psi.append(C @ Apow @ K)\n",
    "        Apow = Apow @ A\n",
    "    return psi\n",
    "\n",
    "\n",
    "psi2 = filtered_wold_coeffs(A, C, K2, n_terms=40)\n",
    "resp2 = np.array(\n",
    "  [psi2[j] @ linalg.cholesky(V2, lower=True) for j in range(14)])\n",
    "decomp2 = fev_contributions(psi2, V2, n_horizons=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435f6b9",
   "metadata": {},
   "source": [
    "### Forecast-error-variance decomposition\n",
    "\n",
    "Because the filtered data are nearly noiseless, the innovation\n",
    "covariance $ V_2 $ is close to singular with one dominant eigenvalue.\n",
    "\n",
    "This means the filtered economy is driven by essentially one shock,\n",
    "just like the true economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13537f02",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "parts = []\n",
    "for i, title in enumerate(shock_titles):\n",
    "    arr = df_to_latex_array(fev_table(decomp2, i, horizons)).strip('$')\n",
    "    parts.append(\n",
    "      r'\\begin{array}{c} ' + title + r' \\\\ ' + arr + r' \\end{array}')\n",
    "\n",
    "display(Latex('$' + r' \\quad '.join(parts) + '$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17cf3e",
   "metadata": {},
   "source": [
    "In Model 2, the first innovation accounts for virtually all forecast-error\n",
    "variance, just as in the true economy where the single structural shock\n",
    "$ \\theta_t $ drives everything.\n",
    "\n",
    "The second and third innovations contribute negligibly.\n",
    "\n",
    "This confirms that filtering strips away the measurement noise that created\n",
    "the appearance of multiple independent sources of variation in Model 1.\n",
    "\n",
    "The covariance matrix and eigenvalues of the Model 2 innovations are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f79949",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Covariance matrix of innovations:')\n",
    "df_v2 = pd.DataFrame(np.round(V2, 4), index=labels, columns=labels)\n",
    "display(Latex(df_to_latex_matrix(df_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a696353",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Eigenvalues of covariance matrix:')\n",
    "print(np.sort(np.linalg.eigvalsh(V2))[::-1].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2445e3",
   "metadata": {},
   "source": [
    "As Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] emphasizes, the two models of measurement\n",
    "produce quite different inferences about the economy’s dynamics despite\n",
    "sharing identical underlying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5a9b2",
   "metadata": {},
   "source": [
    "### Wold impulse responses\n",
    "\n",
    "We again use orthogonalized Wold representation impulse responses with a Cholesky\n",
    "decomposition of $ V_2 $ ordered as $ y_n $, $ c $, $ \\Delta k $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20927b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "parts = []\n",
    "for i, title in enumerate(wold_titles):\n",
    "    arr = df_to_latex_array(\n",
    "      wold_response_table(resp2, i, lags)).strip('$')\n",
    "    parts.append(\n",
    "      r'\\begin{array}{c} ' + title + r' \\\\ ' + arr + r' \\end{array}')\n",
    "\n",
    "display(Latex('$' + r' \\quad '.join(parts) + '$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8678a",
   "metadata": {},
   "source": [
    "The income innovation in Model 2 produces responses that closely\n",
    "approximate the true impulse response function from the structural\n",
    "shock $ \\theta_t $.\n",
    "\n",
    "Readers can compare the left table with the table in the\n",
    "[True impulse responses](#true-impulse-responses) section above.\n",
    "\n",
    "The numbers are essentially the same.\n",
    "\n",
    "The consumption and investment innovations produce responses\n",
    "that are orders of magnitude smaller, confirming that the filtered\n",
    "data are driven by essentially one shock.\n",
    "\n",
    "Unlike Model 1, the filtered data from Model 2\n",
    "*cannot* reproduce the apparent Granger causality pattern that the\n",
    "accelerator literature has documented empirically.\n",
    "\n",
    "Hence, at the population level, the two measurement models imply different\n",
    "empirical stories even though they share the same structural economy.\n",
    "\n",
    "- In Model 1 (raw data), measurement noise creates multiple innovations\n",
    "  and an apparent Granger-causality pattern.  \n",
    "- In Model 2 (filtered data), innovations collapse back to essentially\n",
    "  one dominant shock, mirroring the true one-index economy.  \n",
    "\n",
    "\n",
    "Let’s verify these implications in a finite sample simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003b28e",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "The tables above characterize population moments of the two models.\n",
    "\n",
    "Let’s simulate 80 periods of true, measured, and filtered data\n",
    "to compare population implications with finite-sample behavior.\n",
    "\n",
    "First, we define a function to simulate the true economy, generate measured data with AR(1) measurement errors, and apply the Model 1 Kalman filter to produce filtered estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd62971",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_series(seed=7909, T=80, k0=10.0):\n",
    "    \"\"\"\n",
    "    Simulate true, measured, and filtered series.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # True state/observables\n",
    "    θ = rng.normal(0.0, 1.0, size=T)\n",
    "    k = np.empty(T + 1)\n",
    "    k[0] = k0\n",
    "\n",
    "    y = np.empty(T)\n",
    "    c = np.empty(T)\n",
    "    dk = np.empty(T)\n",
    "\n",
    "    for t in range(T):\n",
    "        x_t = np.array([k[t], θ[t]])\n",
    "        y[t], c[t], dk[t] = C @ x_t\n",
    "        k[t + 1] = k[t] + (1.0 / f) * θ[t]\n",
    "\n",
    "    # Measured data with AR(1) errors\n",
    "    v_prev = np.zeros(3)\n",
    "    v = np.empty((T, 3))\n",
    "    for t in range(T):\n",
    "        η_t = rng.multivariate_normal(np.zeros(3), Σ_η)\n",
    "        v_prev = D @ v_prev + η_t\n",
    "        v[t] = v_prev\n",
    "\n",
    "    z_meas = np.column_stack([y, c, dk]) + v\n",
    "\n",
    "    # Filtered data via Model 1 transformed filter\n",
    "    xhat_prev = np.array([k0, 0.0])\n",
    "    z_prev = np.zeros(3)\n",
    "    z_filt = np.empty((T, 3))\n",
    "    k_filt = np.empty(T)\n",
    "\n",
    "    for t in range(T):\n",
    "        z_bar_t = z_meas[t] - D @ z_prev\n",
    "        u_t = z_bar_t - C_bar @ xhat_prev\n",
    "        xhat_t = A @ xhat_prev + K1 @ u_t\n",
    "\n",
    "        z_filt[t] = C @ xhat_t\n",
    "        k_filt[t] = xhat_t[0]\n",
    "\n",
    "        xhat_prev = xhat_t\n",
    "        z_prev = z_meas[t]\n",
    "\n",
    "    out = {\n",
    "        \"y_true\": y, \"c_true\": c, \"dk_true\": dk, \"k_true\": k[:-1],\n",
    "        \"y_meas\": z_meas[:, 0], \"c_meas\": z_meas[:, 1], \n",
    "        \"dk_meas\": z_meas[:, 2],\n",
    "        \"y_filt\": z_filt[:, 0], \"c_filt\": z_filt[:, 1], \n",
    "        \"dk_filt\": z_filt[:, 2], \"k_filt\": k_filt\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "sim = simulate_series(seed=7909, T=80, k0=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47def7d3",
   "metadata": {},
   "source": [
    "We use the following helper function to plot the true series against either the measured or filtered series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab558132",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_true_vs_other(t, true_series, other_series, \n",
    "                                  other_label, ylabel=\"\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(t, true_series, lw=2, label=\"true\")\n",
    "    ax.plot(t, other_series, lw=2, ls=\"--\", label=other_label)\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "t = np.arange(1, 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d07dcd",
   "metadata": {},
   "source": [
    "Let’s first compare the true series with the measured series to see how measurement errors distort the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748d92f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"c_true\"], sim[\"c_meas\"], \n",
    "                                    \"measured\", ylabel=\"consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ab890",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"dk_true\"], sim[\"dk_meas\"], \n",
    "                                    \"measured\", ylabel=\"investment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a3a68",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"y_true\"], sim[\"y_meas\"], \n",
    "                                    \"measured\", ylabel=\"income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614363e8",
   "metadata": {},
   "source": [
    "Investment is distorted the most because its measurement error\n",
    "has the largest innovation variance ($ \\sigma_\\eta = 0.65 $),\n",
    "while income is distorted the least ($ \\sigma_\\eta = 0.05 $).\n",
    "\n",
    "For the filtered series, we expect the Kalman filter to recover the true series more closely by stripping away measurement noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc2267",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"c_true\"], sim[\"c_filt\"], \n",
    "                                    \"filtered\", ylabel=\"consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2b36b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"dk_true\"], sim[\"dk_filt\"], \n",
    "                                    \"filtered\", ylabel=\"investment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324cffe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"y_true\"], sim[\"y_filt\"], \n",
    "                                    \"filtered\", ylabel=\"income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8283e6d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_true_vs_other(t, sim[\"k_true\"], sim[\"k_filt\"], \n",
    "                                    \"filtered\", ylabel=\"capital stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c56f9",
   "metadata": {},
   "source": [
    "Indeed, Kalman-filtered estimates from Model 1 remove much of the\n",
    "measurement noise and track the truth closely.\n",
    "\n",
    "In the true model the national income identity\n",
    "$ c_t + \\Delta k_t = y_{n,t} $ holds exactly.\n",
    "\n",
    "Independent measurement errors break this accounting identity\n",
    "in the measured data.\n",
    "\n",
    "The Kalman filter approximately restores it.\n",
    "\n",
    "The following figure confirms this by showing the residual $ c_t + \\Delta k_t - y_{n,t} $ for\n",
    "both measured and filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774352a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.plot(t, sim[\"c_meas\"] + sim[\"dk_meas\"] - sim[\"y_meas\"], lw=2)\n",
    "ax1.axhline(0, color='black', lw=0.8, ls='--', alpha=0.5)\n",
    "ax1.set_xlabel(\"time\")\n",
    "ax1.set_ylabel(\"measured residual\")\n",
    "\n",
    "ax2.plot(t, sim[\"c_filt\"] + sim[\"dk_filt\"] - sim[\"y_filt\"], lw=2)\n",
    "ax2.axhline(0, color='black', lw=0.8, ls='--', alpha=0.5)\n",
    "ax2.set_xlabel(\"time\")\n",
    "ax2.set_ylabel(\"filtered residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d02f90",
   "metadata": {},
   "source": [
    "As we have predicted, the residual for the measured data is large and volatile, while the residual for the filtered data is numerically 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98405b08",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] shows how measurement error alters an\n",
    "econometrician’s view of a permanent income economy driven by\n",
    "the investment accelerator.\n",
    "\n",
    "The Wold representations and variance decompositions of Model 1\n",
    "(raw measurements) and Model 2 (filtered measurements) differ\n",
    "substantially, even though the underlying economy is the same.\n",
    "\n",
    "Measurement error can reshape inferences about which shocks\n",
    "drive which variables.\n",
    "\n",
    "Model 1 reproduces the **Granger causality** pattern documented in\n",
    "the empirical accelerator literature: income appears to Granger-cause\n",
    "consumption and investment, a result Sargent [[1989](https://python.quantecon.org/zreferences.html#id232)] attributes\n",
    "to measurement error and signal extraction in raw reported data.\n",
    "\n",
    "Model 2, working with filtered data, attributes nearly all variance\n",
    "to the single structural shock $ \\theta_t $ and *cannot* reproduce\n",
    "the Granger causality pattern.\n",
    "\n",
    "The [Kalman filter](https://python.quantecon.org/kalman.html) effectively strips measurement\n",
    "noise from the data, so the filtered series track the truth closely.\n",
    "\n",
    "Raw measurement error breaks the national income accounting identity,\n",
    "but the near-zero residual shows that the filter approximately\n",
    "restores it."
   ]
  }
 ],
 "metadata": {
  "date": 1771891006.22506,
  "filename": "measurement_models.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Two Models of Measurements and the Investment Accelerator"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}