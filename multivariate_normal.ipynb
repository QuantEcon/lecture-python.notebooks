{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='multivariate-normal-v11'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Multivariate Normal Distribution](#Multivariate-Normal-Distribution)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [The Multivariate Normal Distribution](#The-Multivariate-Normal-Distribution)  \n",
    "  - [Bivariate Example](#Bivariate-Example)  \n",
    "  - [Trivariate Example](#Trivariate-Example)  \n",
    "  - [One Dimensional Intelligence (IQ)](#One-Dimensional-Intelligence-%28IQ%29)  \n",
    "  - [Another representation](#Another-representation)  \n",
    "  - [Magic of the Cholesky factorization](#Magic-of-the-Cholesky-factorization)  \n",
    "  - [Math and Verbal Components of Intelligence](#Math-and-Verbal-Components-of-Intelligence)  \n",
    "  - [Univariate Time Series Analysis](#Univariate-Time-Series-Analysis)  \n",
    "  - [Classic Factor Analysis Model](#Classic-Factor-Analysis-Model)  \n",
    "  - [PCA as Approximation to Factor Analytic Model](#PCA-as-Approximation-to-Factor-Analytic-Model)  \n",
    "  - [Stochastic Difference Equation](#Stochastic-Difference-Equation)  \n",
    "  - [Application to Stock Price Model](#Application-to-Stock-Price-Model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture describes a workhorse in probability theory, statistics, and economics, namely,\n",
    "the **multivariate normal distribution**.\n",
    "\n",
    "In this lecture, you will learn formulas for\n",
    "\n",
    "- the joint distribution of a random vector $ x $ of length $ N $  \n",
    "- marginal distributions for all subvectors of $ x $  \n",
    "- conditional distributions for subvectors of ‚Äòmath:x conditional on other subvectors of $ x $  \n",
    "\n",
    "\n",
    "We will use  the multivariate normal distribution to formulate some classic models:\n",
    "\n",
    "- a **factor analytic model** on an intelligence quotient, i.e., IQ  \n",
    "- a **factor analytic model** or two independent inherent abilities, mathematical and verbal.  \n",
    "- a more general factor analytic model  \n",
    "- PCA as an approximation to a factor analytic model  \n",
    "- time series generated by linear stochastic difference equations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multivariate Normal Distribution\n",
    "\n",
    "This lecture defines a Python class `MultivariateNormal` to be used\n",
    "to generate **marginal** and **conditional** distributions associated\n",
    "with a multivariate normal distribution.\n",
    "\n",
    "For a multivariate normal distribution it is very convenient that\n",
    "\n",
    "- conditional expectations equal linear least squares projections  \n",
    "- conditional distributions are characterized by multivariate linear\n",
    "  regressions  \n",
    "\n",
    "\n",
    "We apply our Python class to some classic examples.\n",
    "\n",
    "We will use the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that an $ N \\times 1 $ random vector $ z $ has a\n",
    "multivariate normal probability density.\n",
    "\n",
    "This means that the probability density takes the form\n",
    "\n",
    "$$\n",
    "f\\left(z;\\mu,\\Sigma\\right)=\\left(2\\pi\\right)^{-\\left(\\frac{N}{2}\\right)}\\det\\left(\\Sigma\\right)^{-\\frac{1}{2}}\\exp\\left(-.5\\left(z-\\mu\\right)^{\\prime}\\Sigma^{-1}\\left(z-\\mu\\right)\\right)\n",
    "$$\n",
    "\n",
    "where $ \\mu=Ez $ is the mean of the random vector $ z $ and\n",
    "$ \\Sigma=E\\left(z-\\mu\\right)\\left(z-\\mu\\right)^\\prime $ is the\n",
    "covariance matrix of $ z $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def f(z, Œº, Œ£):\n",
    "    \"\"\"\n",
    "    The density function of multivariate normal distribution.\n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    z: ndarray(float, dim=2)\n",
    "        random vector, N by 1\n",
    "    Œº: ndarray(float, dim=1 or 2)\n",
    "        the mean of z, N by 1\n",
    "    Œ£: ndarray(float, dim=2)\n",
    "        the covarianece matrix of z, N by 1\n",
    "    \"\"\"\n",
    "\n",
    "    z = np.atleast_2d(z)\n",
    "    Œº = np.atleast_2d(Œº)\n",
    "    Œ£ = np.atleast_2d(Œ£)\n",
    "\n",
    "    N = z.size\n",
    "\n",
    "    temp1 = np.linalg.det(Œ£) ** (-1/2)\n",
    "    temp2 = np.exp(-.5 * (z - Œº).T @ np.linalg.inv(Œ£) @ (z - Œº))\n",
    "\n",
    "    return (2 * np.pi) ** (-N/2) * temp1 * temp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some integer $ k\\in \\{2,\\dots, N-1\\} $, partition\n",
    "$ z $ as\n",
    "$ z=\\left[\\begin{array}{c} z_{1}\\\\ z_{2} \\end{array}\\right] $, where\n",
    "$ z_1 $ is an $ \\left(N-k\\right)\\times1 $ vector and $ z_2 $\n",
    "is a $ k\\times1 $ vector.\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\mu=\\left[\\begin{array}{c}\n",
    "\\mu_{1}\\\\\n",
    "\\mu_{2}\n",
    "\\end{array}\\right],\\quad\\Sigma=\\left[\\begin{array}{cc}\n",
    "\\Sigma_{11} & \\Sigma_{12}\\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "be corresponding partitions of $ \\mu $ and $ \\Sigma $.\n",
    "\n",
    "The **marginal** distribution of $ z_1 $ is\n",
    "\n",
    "- multivariate normal with mean $ \\mu_1 $ and covariance matrix\n",
    "  $ \\Sigma_{11} $.  \n",
    "\n",
    "\n",
    "The **marginal** distribution of $ z_2 $ is\n",
    "\n",
    "- multivariate normal with mean $ \\mu_2 $ and covariance matrix\n",
    "  $ \\Sigma_{22} $.  \n",
    "\n",
    "\n",
    "The distribution of $ z_1 $ **conditional** on $ z_2 $ is\n",
    "\n",
    "- multivariate normal with mean  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_1 = \\mu_1 + \\beta \\left(z_2 -\\mu_2\\right)\n",
    "$$\n",
    "\n",
    "and covariance matrix\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma}_{11}=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}=\\Sigma_{11}-\\beta\\Sigma_{22}\\beta^{\\prime}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\beta = \\Sigma_{12}\\Sigma_{22}^{-1}\n",
    "$$\n",
    "\n",
    "is an $ \\left(N-k\\right) \\times k $ matrix of **population\n",
    "regression coefficients** of $ z_1 - \\mu_1 $ on $ z_2 - \\mu_2 $.\n",
    "\n",
    "The following class constructs a multivariate normal distribution\n",
    "instance with two methods.\n",
    "\n",
    "- a method `partition` computes $ \\beta $, taking $ k $ as an\n",
    "  input  \n",
    "- a method `cond_dist` computes either the distribution of\n",
    "  $ z_1 $ conditional on $ z_2 $ or the distribution of\n",
    "  $ z_2 $ conditional on $ z_1 $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class MultivariateNormal:\n",
    "    \"\"\"\n",
    "    Class of multivariate normal distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Œº: ndarray(float, dim=1)\n",
    "        the mean of z, N by 1\n",
    "    Œ£: ndarray(float, dim=2)\n",
    "        the covarianece matrix of z, N by 1\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Œº, Œ£:\n",
    "        see parameters\n",
    "    Œºs: list(ndarray(float, dim=1))\n",
    "        list of mean vectors Œº1 and Œº2 in order\n",
    "    Œ£s: list(list(ndarray(float, dim=2)))\n",
    "        2 dimensional list of covariance matrices\n",
    "        Œ£11, Œ£12, Œ£21, Œ£22 in order\n",
    "    Œ≤s: list(ndarray(float, dim=1))\n",
    "        list of regression coefficients Œ≤1 and Œ≤2 in order\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Œº, Œ£):\n",
    "        \"initialization\"\n",
    "        self.Œº = np.array(Œº)\n",
    "        self.Œ£ = np.atleast_2d(Œ£)\n",
    "\n",
    "    def partition(self, k):\n",
    "        \"\"\"\n",
    "        Given k, partition the random vector z into a size k vector z1\n",
    "        and a size N-k vector z2. Partition the mean vector Œº into\n",
    "        Œº1 and Œº2, and the covariance matrix Œ£ into Œ£11, Œ£12, Œ£21, Œ£22\n",
    "        correspondingly. Compute the regression coefficients Œ≤1 and Œ≤2\n",
    "        using the partitioned arrays.\n",
    "        \"\"\"\n",
    "        Œº = self.Œº\n",
    "        Œ£ = self.Œ£\n",
    "\n",
    "        self.Œºs = [Œº[:k], Œº[k:]]\n",
    "        self.Œ£s = [[Œ£[:k, :k], Œ£[:k, k:]],\n",
    "                   [Œ£[k:, :k], Œ£[k:, k:]]]\n",
    "\n",
    "        self.Œ≤s = [self.Œ£s[0][1] @ np.linalg.inv(self.Œ£s[1][1]),\n",
    "                   self.Œ£s[1][0] @ np.linalg.inv(self.Œ£s[0][0])]\n",
    "\n",
    "    def cond_dist(self, ind, z):\n",
    "        \"\"\"\n",
    "        Compute the conditional distribution of z1 given z2, or reversely.\n",
    "        Argument ind determines whether we compute the conditional\n",
    "        distribution of z1 (ind=0) or z2 (ind=1).\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        Œº_hat: ndarray(float, ndim=1)\n",
    "            The conditional mean of z1 or z2.\n",
    "        Œ£_hat: ndarray(float, ndim=2)\n",
    "            The conditional covariance matrix of z1 or z2.\n",
    "        \"\"\"\n",
    "        Œ≤ = self.Œ≤s[ind]\n",
    "        Œºs = self.Œºs\n",
    "        Œ£s = self.Œ£s\n",
    "\n",
    "        Œº_hat = Œºs[ind] + Œ≤ @ (z - Œºs[1-ind])\n",
    "        Œ£_hat = Œ£s[ind][ind] - Œ≤ @ Œ£s[1-ind][1-ind] @ Œ≤.T\n",
    "\n",
    "        return Œº_hat, Œ£_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs put this code to work on a suite of examples.\n",
    "\n",
    "We begin with a simple bivariate example; after that we‚Äôll turn to a\n",
    "trivariate example.\n",
    "\n",
    "We‚Äôll compute population moments of some conditional distributions using\n",
    "our `MultivariateNormal` class.\n",
    "\n",
    "Then for fun we‚Äôll compute sample analogs of the associated population\n",
    "regressions by generating simulations and then computing linear least\n",
    "squares regressions.\n",
    "\n",
    "We‚Äôll compare those linear least squares regressions for the simulated\n",
    "data to their population counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Example\n",
    "\n",
    "We start with a bivariate normal distribution pinned down by\n",
    "\n",
    "$$\n",
    "\\mu=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right],\\quad\\Sigma=\\left[\\begin{array}{cc}\n",
    "1 & .2\\\\\n",
    ".2 & 1\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œº = np.array([0., 0.])\n",
    "Œ£ = np.array([[1., .2], [.2 ,1.]])\n",
    "\n",
    "# construction of the multivariate normal instance\n",
    "multi_normal = MultivariateNormal(Œº, Œ£)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "k = 1 # choose partition\n",
    "\n",
    "# partition and compute regression coefficients\n",
    "multi_normal.partition(k)\n",
    "multi_normal.Œ≤s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compute the mean and variance of the distribution of $ z_1 $\n",
    "conditional on $ z_2=5 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# compute the cond. dist. of z1\n",
    "ind = 0\n",
    "z2 = np.array([5.]) # given z2\n",
    "\n",
    "Œº1_hat, Œ£1_hat = multi_normal.cond_dist(ind, z2)\n",
    "print('Œº1_hat, Œ£1_hat = ', Œº1_hat, Œ£1_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compare the preceding population mean and variance with outcomes\n",
    "from drawing a large sample and then regressing $ z_1 - \\mu_1 $ on\n",
    "$ z_2 - \\mu_2 $.\n",
    "\n",
    "We know that\n",
    "\n",
    "$$\n",
    "E z_1 | z_2 = \\left(\\mu_1 - \\beta \\mu_2 \\right) + \\beta z_2\n",
    "$$\n",
    "\n",
    "which can be arranged to\n",
    "\n",
    "$$\n",
    "z_1 - \\mu_1 = \\beta \\left( z_2 - \\mu_2 \\right) + \\epsilon,\n",
    "$$\n",
    "\n",
    "We anticipate that for larger and larger sample sizes, estimated OLS\n",
    "coefficients will converge to $ \\beta $ and the estimated variance\n",
    "of $ \\epsilon $ will converge to $ \\hat{\\Sigma}_1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 1_000_000 # sample size\n",
    "\n",
    "# simulate multivariate normal random vectors\n",
    "data = np.random.multivariate_normal(Œº, Œ£, size=n)\n",
    "z1_data = data[:, 0]\n",
    "z2_data = data[:, 1]\n",
    "\n",
    "# OLS regression\n",
    "Œº1, Œº2 = multi_normal.Œºs\n",
    "results = sm.OLS(z1_data - Œº1, z2_data - Œº2).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compare the preceding population $ \\beta $ with the OLS sample\n",
    "estimate on $ z_2 - \\mu_2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal.Œ≤s[0], results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compare our population $ \\hat{\\Sigma}_1 $ with the\n",
    "degrees-of-freedom adjusted estimate of the variance of $ \\epsilon $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œ£1_hat, results.resid @ results.resid.T / (n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let‚Äôs compute the estimate of $ \\hat{E z_1 | z_2} $ and\n",
    "compare it with $ \\hat{\\mu}_1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œº1_hat, results.predict(z2 - Œº2) + Œº1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in each case, for our very large sample size, the sample analogues\n",
    "closely approximate their population counterparts.\n",
    "\n",
    "These close approximations are foretold by a version of a Law of Large\n",
    "Numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivariate Example\n",
    "\n",
    "Let‚Äôs apply our code to a trivariate example.\n",
    "\n",
    "We‚Äôll specify the mean vector and the covariance matrix as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œº = np.random.random(3)\n",
    "C = np.random.random((3, 3))\n",
    "Œ£ = C @ C.T # positive semi-definite\n",
    "\n",
    "multi_normal = MultivariateNormal(Œº, Œ£)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œº, Œ£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "multi_normal.partition(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compute the distribution of $ z_1 $ conditional on\n",
    "$ z_{2}=\\left[\\begin{array}{c} 2\\\\ 5 \\end{array}\\right] $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ind = 0\n",
    "z2 = np.array([2., 5.])\n",
    "\n",
    "Œº1_hat, Œ£1_hat = multi_normal.cond_dist(ind, z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 1_000_000\n",
    "data = np.random.multivariate_normal(Œº, Œ£, size=n)\n",
    "z1_data = data[:, :k]\n",
    "z2_data = data[:, k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œº1, Œº2 = multi_normal.Œºs\n",
    "results = sm.OLS(z1_data - Œº1, z2_data - Œº2).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we compare population and sample regression coefficients, the\n",
    "conditional covariance matrix, and the conditional mean vector in that\n",
    "order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal.Œ≤s[0], results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œ£1_hat, results.resid @ results.resid.T / (n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œº1_hat, results.predict(z2 - Œº2) + Œº1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, sample analogues do a good job of approximating their\n",
    "populations counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Dimensional Intelligence (IQ)\n",
    "\n",
    "Let‚Äôs move closer to a real-life example, namely, inferring a\n",
    "one-dimensional measure of intelligence called IQ from a list of test\n",
    "scores.\n",
    "\n",
    "The $ i $th test score $ y_i $ equals the sum of an unknown\n",
    "scalar IQ $ \\theta $ and a random variables $ w_{i} $.\n",
    "\n",
    "$$\n",
    "y_{i} = \\theta + \\sigma_y w_i, \\quad i=1,\\dots, n\n",
    "$$\n",
    "\n",
    "The distribution of IQ‚Äôs for a cross-section of people is a normal\n",
    "random variable described by\n",
    "\n",
    "$$\n",
    "\\theta = \\mu_{\\theta} + \\sigma_{\\theta} w_{n+1}.\n",
    "$$\n",
    "\n",
    "We assume the noise in the test scores is IID and not correlated with\n",
    "IQ.\n",
    "\n",
    "In particular, we assume $ \\{w_i\\}_{i=1}^{n+1} $ are i.i.d. standard\n",
    "normal:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}=\n",
    "\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{n}\\\\\n",
    "w_{n+1}\n",
    "\\end{array}\\right]\\sim N\\left(0,I_{n+1}\\right)\n",
    "$$\n",
    "\n",
    "The following system describes the random vector $ X $ that\n",
    "interests us:\n",
    "\n",
    "$$\n",
    "X=\\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\\\\\n",
    "\\theta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\mu_{\\theta}\\\\\n",
    "\\mu_{\\theta}\\\\\n",
    "\\vdots\\\\\n",
    "\\mu_{\\theta}\\\\\n",
    "\\mu_{\\theta}\n",
    "\\end{array}\\right]+\\left[\\begin{array}{ccccc}\n",
    "\\sigma_{y} & 0 & \\cdots & 0 & \\sigma_{\\theta}\\\\\n",
    "0 & \\sigma_{y} & \\cdots & 0 & \\sigma_{\\theta}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & \\sigma_{y} & \\sigma_{\\theta}\\\\\n",
    "0 & 0 & \\cdots & 0 & \\sigma_{\\theta}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{n}\\\\\n",
    "w_{n+1}\n",
    "\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "X=\\mu_{\\theta}\\boldsymbol{1}_{n+1}+D\\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "where $ X = \\begin{bmatrix} y \\cr \\theta \\end{bmatrix} $,\n",
    "$ \\boldsymbol{1}_{n+1} $ is a vector of $ 1 $s of size\n",
    "$ n+1 $, and $ D $ is an $ n+1 $ by $ n+1 $ matrix.\n",
    "\n",
    "Let‚Äôs define a Python function that constructs the mean $ \\mu $ and\n",
    "covariance matrix $ \\Sigma $ of the random vector $ X $ that we\n",
    "know is governed by a multivariate normal distribution.\n",
    "\n",
    "As arguments, the function takes the number of tests $ n $, the mean\n",
    "$ \\mu_{\\theta} $ and the standard deviation $ \\sigma_\\theta $ of\n",
    "the IQ distribution, and the standard deviation of the randomness in\n",
    "test scores $ \\sigma_{y} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def construct_moments_IQ(n, ùúáùúÉ, ùúéùúÉ, ùúéy):\n",
    "\n",
    "    ùúá_IQ = np.ones(n+1) * ùúáùúÉ\n",
    "\n",
    "    D_IQ = np.zeros((n+1, n+1))\n",
    "    D_IQ[range(n), range(n)] = ùúéy\n",
    "    D_IQ[:, n] = ùúéùúÉ\n",
    "\n",
    "    Œ£_IQ = D_IQ @ D_IQ.T\n",
    "\n",
    "    return ùúá_IQ, Œ£_IQ, D_IQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs consider a specific instance of this model.\n",
    "\n",
    "Assume we have recorded $ 50 $ test scores and we know that\n",
    "$ \\mu_{\\theta}=100 $, $ \\sigma_{\\theta}=10 $, and\n",
    "$ \\sigma_{y}=10 $.\n",
    "\n",
    "We can compute the mean vector and covariance matrix of $ x $ easily\n",
    "with our `construct_moments_IQ` function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "ùúáùúÉ, ùúéùúÉ, ùúéy = 100., 10., 10.\n",
    "\n",
    "ùúá_IQ, Œ£_IQ, D_IQ = construct_moments_IQ(n, ùúáùúÉ, ùúéùúÉ, ùúéy)\n",
    "Œº_IQ, Œ£_IQ, D_IQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our `MultivariateNormal` class to construct an\n",
    "instance, then partition the mean vector and covariance matrix as we\n",
    "wish.\n",
    "\n",
    "We choose `k=n` so that $ z_{1} = y $ and $ z_{2} = \\theta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_IQ = MultivariateNormal(Œº_IQ, Œ£_IQ)\n",
    "\n",
    "k = n\n",
    "multi_normal_IQ.partition(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the generator `multivariate_normal`, we can make one draw of the\n",
    "random vector from our distribution and then compute the distribution of\n",
    "$ \\theta $ conditional on our test scores.\n",
    "\n",
    "Let‚Äôs do that and then print out some pertinent quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x = np.random.multivariate_normal(Œº_IQ, Œ£_IQ)\n",
    "y = x[:-1] # test scores\n",
    "ùúÉ = x[-1]  # IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# the true value\n",
    "ùúÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `cond_dist` takes test scores as input and returns the\n",
    "conditional normal distribution of the IQ $ \\theta $.\n",
    "\n",
    "Note that now $ \\theta $ is what we denoted as $ z_{2} $ in the\n",
    "general case so we need to set `ind=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ind = 1\n",
    "multi_normal_IQ.cond_dist(ind, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number is the conditional mean $ \\hat{\\mu}_{\\theta} $ and\n",
    "the second is the conditional variance $ \\hat{\\Sigma}_{\\theta} $.\n",
    "\n",
    "How do the additional test scores affect our inferences?\n",
    "\n",
    "To shed light on this, we compute a sequence of conditional\n",
    "distributions of $ \\theta $ by varying the number of test scores in\n",
    "the conditioning set from $ 1 $ to $ n $.\n",
    "\n",
    "We‚Äôll make a pretty graph showing how our judgment of the person‚Äôs IQ\n",
    "change as more test results come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# array for containing moments\n",
    "ŒºùúÉ_hat_arr = np.empty(n)\n",
    "Œ£ùúÉ_hat_arr = np.empty(n)\n",
    "\n",
    "# loop over number of test scores\n",
    "for i in range(1, n+1):\n",
    "    # construction of multivariate normal distribution instance\n",
    "    ùúá_IQ_i, Œ£_IQ_i, D_IQ_i = construct_moments_IQ(i, ùúáùúÉ, ùúéùúÉ, ùúéy)\n",
    "    multi_normal_IQ_i = MultivariateNormal(Œº_IQ_i, Œ£_IQ_i)\n",
    "\n",
    "    # partition and compute conditional distribution\n",
    "    multi_normal_IQ_i.partition(i)\n",
    "    scores_i = y[:i]\n",
    "    ŒºùúÉ_hat_i, Œ£ùúÉ_hat_i = multi_normal_IQ_i.cond_dist(1, scores_i)\n",
    "\n",
    "    # store the results\n",
    "    ŒºùúÉ_hat_arr[i-1] = ŒºùúÉ_hat_i[0]\n",
    "    Œ£ùúÉ_hat_arr[i-1] = Œ£ùúÉ_hat_i[0, 0]\n",
    "\n",
    "# transform variance to standard deviation\n",
    "ùúéùúÉ_hat_arr = np.sqrt(Œ£ùúÉ_hat_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ŒºùúÉ_hat_lower = ŒºùúÉ_hat_arr - 1.96 * ùúéùúÉ_hat_arr\n",
    "ŒºùúÉ_hat_higher = ŒºùúÉ_hat_arr + 1.96 * ùúéùúÉ_hat_arr\n",
    "\n",
    "plt.hlines(ùúÉ, 1, n+1, ls='--', label='true $ùúÉ$')\n",
    "plt.plot(range(1, n+1), ŒºùúÉ_hat_arr, color='b', label='$\\hat{Œº}_{ùúÉ}$')\n",
    "plt.plot(range(1, n+1), ŒºùúÉ_hat_lower, color='b', ls='--')\n",
    "plt.plot(range(1, n+1), ŒºùúÉ_hat_higher, color='b', ls='--')\n",
    "plt.fill_between(range(1, n+1), ŒºùúÉ_hat_lower, ŒºùúÉ_hat_higher,\n",
    "                 color='b', alpha=0.2, label='95%')\n",
    "\n",
    "plt.xlabel('number of test scores')\n",
    "plt.ylabel('$\\hat{ùúÉ}$')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid blue line in the plot above shows $ \\hat{\\mu}_{\\theta} $\n",
    "as function of the number of test scores that we have recorded and\n",
    "conditioned on.\n",
    "\n",
    "The blue area shows the span that comes from adding or deducing\n",
    "$ 1.96 \\hat{\\sigma}_{\\theta} $ from $ \\hat{\\mu}_{\\theta} $.\n",
    "\n",
    "Therefore, $ 95\\% $ of the probability mass of the conditional\n",
    "distribution falls in this range.\n",
    "\n",
    "The value of the random $ \\theta $ that we drew is shown by the\n",
    "black dotted line.\n",
    "\n",
    "As more and more test scores come in, our estimate of the person‚Äôs\n",
    "$ \\theta $ become more and more reliable.\n",
    "\n",
    "By staring at the changes in the conditional distributions, we see that\n",
    "adding more test scores makes $ \\hat{\\theta} $ settle down and\n",
    "approach $ \\theta $.\n",
    "\n",
    "Thus, each $ y_{i} $ adds information about $ \\theta $.\n",
    "\n",
    "If we drove the number of tests $ n \\rightarrow + \\infty $, the\n",
    "conditional standard deviation $ \\hat{\\sigma}_{\\theta} $ would\n",
    "converge to $ 0 $ at the rate $ \\frac{1}{n^{.5}} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another representation\n",
    "\n",
    "By using a different representation, let‚Äôs look at things from a\n",
    "different perspective.\n",
    "\n",
    "We can represent the random vector $ X $ defined above as\n",
    "\n",
    "$$\n",
    "X = \\mu_{\\theta} \\boldsymbol{1}_{n+1} + C \\epsilon, \\quad \\epsilon \\sim N\\left(0, I\\right)\n",
    "$$\n",
    "\n",
    "where $ C $ is a lower triangular **Cholesky factor** of\n",
    "$ \\Sigma $ so that\n",
    "\n",
    "$$\n",
    "\\Sigma \\equiv DD^{\\prime} = C C^\\prime\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "E \\epsilon \\epsilon' = I .\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim N(0, I) .\n",
    "$$\n",
    "\n",
    "Let $ G=C^{-1} $; $ G $ is also lower triangular.\n",
    "\n",
    "We can compute $ \\epsilon $ from the formula\n",
    "\n",
    "$$\n",
    "\\epsilon = G \\left( X - \\mu_{\\theta} \\boldsymbol{1}_{n+1} \\right)\n",
    "$$\n",
    "\n",
    "This formula confirms that the orthonormal vector $ \\epsilon $\n",
    "contains the same information as the non-orthogonal vector\n",
    "$ \\left( X - \\mu_{\\theta} \\boldsymbol{1}_{n+1} \\right) $.\n",
    "\n",
    "We can say that $ \\epsilon $ is an orthogonal basis for\n",
    "$ \\left( X - \\mu_{\\theta} \\boldsymbol{1}_{n+1} \\right) $.\n",
    "\n",
    "Let $ c_{i} $ be the $ i $th element in the last row of\n",
    "$ C $.\n",
    "\n",
    "Then we can write\n",
    "\n",
    "\n",
    "<a id='equation-mnv-1'></a>\n",
    "$$\n",
    "\\theta = \\mu_{\\theta} + c_1 \\epsilon_1 + c_2 \\epsilon_2 + \\dots + c_n \\epsilon_n + c_{n+1} \\epsilon_{n+1} \\tag{1}\n",
    "$$\n",
    "\n",
    "The mutual orthogonality of the $ \\epsilon_i $‚Äôs provides us an\n",
    "informative way to interpret them in light of equation (1).\n",
    "\n",
    "Thus, relative to what is known from tests $ i=1, \\ldots, n-1 $,\n",
    "$ c_i \\epsilon_i $ is the amount of **new information** about\n",
    "$ \\theta $ brought by the test number $ i $.\n",
    "\n",
    "Here **new information** means **surprise** or what could not be\n",
    "predicted from earlier information.\n",
    "\n",
    "Formula (1) also provides us with an enlightening way to express\n",
    "conditional means and conditional variances that we computed earlier.\n",
    "\n",
    "In particular,\n",
    "\n",
    "$$\n",
    "E\\left[\\theta \\mid y_1, \\dots, y_k\\right] = \\mu_{\\theta} + c_1 \\epsilon_1 + \\dots + c_k \\epsilon_k\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "Var\\left(\\theta \\mid y_1, \\dots, y_k\\right) = c^2_{k+1} + c^2_{k+2} + \\dots + c^2_{n+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "C = np.linalg.cholesky(Œ£_IQ)\n",
    "G = np.linalg.inv(C)\n",
    "\n",
    "ùúñ = G @ (x - ùúáùúÉ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "cùúñ = C[n, :] * ùúñ\n",
    "\n",
    "# compute the sequence of ŒºùúÉ and Œ£ùúÉ conditional on y1, y2, ..., yk\n",
    "ŒºùúÉ_hat_arr_C = np.array([np.sum(cùúñ[:k+1]) for k in range(n)]) + ùúáùúÉ\n",
    "Œ£ùúÉ_hat_arr_C = np.array([np.sum(C[n, i+1:n+1] ** 2) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that these formulas give the same answers that we computed\n",
    "earlier, we can compare the means and variances of $ \\theta $\n",
    "conditional on $ \\{y_i\\}_{i=1}^k $ with what we obtained above using\n",
    "the formulas implemented in the class `MultivariateNormal` built on\n",
    "our original representation of conditional distributions for\n",
    "multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# conditional mean\n",
    "np.max(np.abs(ŒºùúÉ_hat_arr - ŒºùúÉ_hat_arr_C)) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# conditional variance\n",
    "np.max(np.abs(Œ£ùúÉ_hat_arr - Œ£ùúÉ_hat_arr_C)) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic of the Cholesky factorization\n",
    "\n",
    "Evidently, the Cholesky factorization is automatically computing the\n",
    "population  **regression coefficients** and associated statistics\n",
    "that are produced by our `MultivariateNormal` class.\n",
    "\n",
    "And they are doing it **recursively**.\n",
    "\n",
    "Indeed, in formula (1),\n",
    "\n",
    "- the random variable $ c_i \\epsilon_i $ is information about\n",
    "  $ \\theta $ that is not contained by the information in\n",
    "  $ \\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_{i-1} $  \n",
    "- the coefficient $ c_i $ is the simple population regression\n",
    "  coefficient of $ \\theta - \\mu_\\theta $ on $ \\epsilon_i $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math and Verbal Components of Intelligence\n",
    "\n",
    "We can alter the preceding example to be more realistic.\n",
    "\n",
    "There is ample evidence that IQ is not a scalar.\n",
    "\n",
    "Some people are good in math skills but poor in language skills.\n",
    "\n",
    "Other people are good in language skills but poor in math skills.\n",
    "\n",
    "So now we shall assume that there are two dimensions of IQ,\n",
    "$ \\theta $ and $ \\eta $.\n",
    "\n",
    "These determine average performances in math and language tests,\n",
    "respectively.\n",
    "\n",
    "We observe math scores $ \\{y_i\\}_{i=1}^{n} $ and language scores\n",
    "$ \\{y_i\\}_{i=n+1}^{2n} $.\n",
    "\n",
    "When $ n=2 $, we assume that outcomes are draws from a multivariate\n",
    "normal distribution with representation\n",
    "\n",
    "$$\n",
    "X=\\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "y_{4}\\\\\n",
    "\\theta\\\\\n",
    "\\eta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\mu_{\\theta}\\\\\n",
    "\\mu_{\\theta}\\\\\n",
    "\\mu_{\\eta}\\\\\n",
    "\\mu_{\\eta}\\\\\n",
    "\\mu_{\\theta}\\\\\n",
    "\\mu_{\\eta}\n",
    "\\end{array}\\right]+\\left[\\begin{array}{cccccc}\n",
    "\\sigma_{y} & 0 & 0 & 0 & \\sigma_{\\theta} & 0\\\\\n",
    "0 & \\sigma_{y} & 0 & 0 & \\sigma_{\\theta} & 0\\\\\n",
    "0 & 0 & \\sigma_{y} & 0 & 0 & \\sigma_{\\eta}\\\\\n",
    "0 & 0 & 0 & \\sigma_{y} & 0 & \\sigma_{\\eta}\\\\\n",
    "0 & 0 & 0 & 0 & \\sigma_{\\theta} & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\sigma_{\\eta}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\\\\\n",
    "w_{4}\\\\\n",
    "w_{5}\\\\\n",
    "w_{6}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "$ w \\begin{bmatrix} w_1 \\cr w_2 \\cr \\vdots \\cr w_6 \\end{bmatrix} $\n",
    "is a standard normal random vector.\n",
    "\n",
    "We construct a Python function `construct_moments_IQ2d` to construct\n",
    "the mean vector and covariance matrix of the joint normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def construct_moments_IQ2d(n, ùúáùúÉ, ùúéùúÉ, ùúáùúÇ, ùúéùúÇ, ùúéy):\n",
    "\n",
    "    ùúá_IQ2d = np.empty(2*(n+1))\n",
    "    ùúá_IQ2d[:n] = ùúáùúÉ\n",
    "    ùúá_IQ2d[2*n] = ùúáùúÉ\n",
    "    ùúá_IQ2d[n:2*n] = ùúáùúÇ\n",
    "    ùúá_IQ2d[2*n+1] = ùúáùúÇ\n",
    "\n",
    "\n",
    "    D_IQ2d = np.zeros((2*(n+1), 2*(n+1)))\n",
    "    D_IQ2d[range(2*n), range(2*n)] = ùúéy\n",
    "    D_IQ2d[:n, 2*n] = ùúéùúÉ\n",
    "    D_IQ2d[2*n, 2*n] = ùúéùúÉ\n",
    "    D_IQ2d[n:2*n, 2*n+1] = ùúéùúÇ\n",
    "    D_IQ2d[2*n+1, 2*n+1] = ùúéùúÇ\n",
    "\n",
    "    Œ£_IQ2d = D_IQ2d @ D_IQ2d.T\n",
    "\n",
    "    return ùúá_IQ2d, Œ£_IQ2d, D_IQ2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs put the function to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "# mean and variance of ùúÉ, ùúÇ, and y\n",
    "ùúáùúÉ, ùúéùúÉ, ùúáùúÇ, ùúéùúÇ, ùúéy = 100., 10., 100., 10, 10\n",
    "\n",
    "ùúá_IQ2d, Œ£_IQ2d, D_IQ2d = construct_moments_IQ2d(n, ùúáùúÉ, ùúéùúÉ, ùúáùúÇ, ùúéùúÇ, ùúéy)\n",
    "ùúá_IQ2d, Œ£_IQ2d, D_IQ2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# take one draw\n",
    "x = np.random.multivariate_normal(Œº_IQ2d, Œ£_IQ2d)\n",
    "y1 = x[:n]\n",
    "y2 = x[n:2*n]\n",
    "ùúÉ = x[2*n]\n",
    "ùúÇ = x[2*n+1]\n",
    "\n",
    "# the true values\n",
    "ùúÉ, ùúÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the joint normal distribution of\n",
    "$ \\left(\\theta, \\eta\\right) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_IQ2d = MultivariateNormal(ùúá_IQ2d, Œ£_IQ2d)\n",
    "\n",
    "k = 2*n # the length of data vector\n",
    "multi_normal_IQ2d.partition(k)\n",
    "\n",
    "multi_normal_IQ2d.cond_dist(1, [*y1, *y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs compute distributions of $ \\theta $ and $ \\mu $\n",
    "separately conditional on various subsets of test scores.\n",
    "\n",
    "It will be fun to compare outcomes with the help of an auxiliary function\n",
    "`cond_dist_IQ2d` that we now construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def cond_dist_IQ2d(ùúá, Œ£, data):\n",
    "\n",
    "    n = len(ùúá)\n",
    "\n",
    "    multi_normal = MultivariateNormal(ùúá, Œ£)\n",
    "    multi_normal.partition(n-1)\n",
    "    ùúá_hat, Œ£_hat = multi_normal.cond_dist(1, data)\n",
    "\n",
    "    return ùúá_hat, Œ£_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs see how things work for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "for indices, IQ, conditions in [([*range(2*n), 2*n], 'ùúÉ', 'y1, y2, y3, y4'),\n",
    "                                ([*range(n), 2*n], 'ùúÉ', 'y1, y2'),\n",
    "                                ([*range(n, 2*n), 2*n], 'ùúÉ', 'y3, y4'),\n",
    "                                ([*range(2*n), 2*n+1], 'ùúÇ', 'y1, y2, y3, y4'),\n",
    "                                ([*range(n), 2*n+1], 'ùúÇ', 'y1, y2'),\n",
    "                                ([*range(n, 2*n), 2*n+1], 'ùúÇ', 'y3, y4')]:\n",
    "\n",
    "    ùúá_hat, Œ£_hat = cond_dist_IQ2d(ùúá_IQ2d[indices], Œ£_IQ2d[indices][:, indices], x[indices[:-1]])\n",
    "    print(f'The mean and variance of {IQ} conditional on {conditions: <15} are ' +\n",
    "          f'{ùúá_hat[0]:1.2f} and {Œ£_hat[0, 0]:1.2f} respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, math tests provide no information about $ \\mu $ and\n",
    "language tests provide no information about $ \\eta $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Time Series Analysis\n",
    "\n",
    "We can use the multivariate normal distribution and a little matrix\n",
    "algebra to present foundations of univariate linear time series\n",
    "analysis.\n",
    "\n",
    "Let $ x_t, y_t, v_t, w_{t+1} $ each be scalars for $ t \\geq 0 $.\n",
    "\n",
    "Consider the following model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_0 & \\sim  N\\left(0, \\sigma_0^2\\right) \\\\\n",
    "x_{t+1} & = a x_{t} + b w_{t+1}, \\quad w_{t+1} \\sim N\\left(0, 1\\right), t \\geq 0  \\\\\n",
    "y_{t} & = c x_{t} + d v_{t}, \\quad v_{t} \\sim N\\left(0, 1\\right), t \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can compute the moments of $ x_{t} $\n",
    "\n",
    "1. $ E x_{t+1}^2 = a^2 E x_{t}^2 + b^2, t \\geq 0 $, where\n",
    "  $ E x_{0}^2 = \\sigma_{0}^2 $  \n",
    "1. $ E x_{t+j} x_{t} = a^{j} E x_{t}^2, \\forall t \\ \\forall j $  \n",
    "\n",
    "\n",
    "Given some $ T $, we can formulate the sequence\n",
    "$ \\{x_{t}\\}_{t=0}^T $ as a random vector\n",
    "\n",
    "$$\n",
    "X=\\left[\\begin{array}{c}\n",
    "x_{0}\\\\\n",
    "x_{1}\\\\\n",
    "\\vdots\\\\\n",
    "x_{T}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "and the covariance matrix $ \\Sigma_{x} $ can be constructed using\n",
    "the moments we have computed above.\n",
    "\n",
    "Similarly, we can define\n",
    "\n",
    "$$\n",
    "Y=\\left[\\begin{array}{c}\n",
    "y_{0}\\\\\n",
    "y_{1}\\\\\n",
    "\\vdots\\\\\n",
    "y_{T}\n",
    "\\end{array}\\right], \\quad\n",
    "v=\\left[\\begin{array}{c}\n",
    "v_{0}\\\\\n",
    "v_{1}\\\\\n",
    "\\vdots\\\\\n",
    "v_{T}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "Y = C X + D V\n",
    "$$\n",
    "\n",
    "where $ C $ and $ D $ are both diagonal matrices with constant\n",
    "$ c $ and $ d $ as diagonal respectively.\n",
    "\n",
    "Consequently, the covariance matrix of $ Y $ is\n",
    "\n",
    "$$\n",
    "\\Sigma_{y} = E Y Y^{\\prime} = C \\Sigma_{x} C^{\\prime} + D D^{\\prime}\n",
    "$$\n",
    "\n",
    "By stacking $ X $ and $ Y $, we can write\n",
    "\n",
    "$$\n",
    "Z=\\left[\\begin{array}{c}\n",
    "X\\\\\n",
    "Y\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Sigma_{z} = EZZ^{\\prime}=\\left[\\begin{array}{cc}\n",
    "\\Sigma_{x} & \\Sigma_{x}C^{\\prime}\\\\\n",
    "C\\Sigma_{x} & \\Sigma_{y}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Thus, the stacked sequences $ \\{x_{t}\\}_{t=0}^T $ and\n",
    "$ \\{y_{t}\\}_{t=0}^T $ jointly follow the multivariate normal\n",
    "distribution $ N\\left(0, \\Sigma_{z}\\right) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# as an example, consider the case where T = 3\n",
    "T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# variance of the initial distribution x_0\n",
    "ùúé0 = 1.\n",
    "\n",
    "# parameters of the equation system\n",
    "a = .9\n",
    "b = 1.\n",
    "c = 1.0\n",
    "d = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct the covariance matrix of X\n",
    "Œ£x = np.empty((T+1, T+1))\n",
    "\n",
    "Œ£x[0, 0] = ùúé0 ** 2\n",
    "for i in range(T):\n",
    "    Œ£x[i, i+1:] = Œ£x[i, i] * a ** np.arange(1, T+1-i)\n",
    "    Œ£x[i+1:, i] = Œ£x[i, i+1:]\n",
    "\n",
    "    Œ£x[i+1, i+1] = a ** 2 * Œ£x[i, i] + b ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œ£x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct the covariance matrix of Y\n",
    "C = np.eye(T+1) * c\n",
    "D = np.eye(T+1) * d\n",
    "\n",
    "Œ£y = C @ Œ£x @ C.T + D @ D.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct the covariance matrix of Z\n",
    "Œ£z = np.empty((2*(T+1), 2*(T+1)))\n",
    "\n",
    "Œ£z[:T+1, :T+1] = Œ£x\n",
    "Œ£z[:T+1, T+1:] = Œ£x @ C.T\n",
    "Œ£z[T+1:, :T+1] = C @ Œ£x\n",
    "Œ£z[T+1:, T+1:] = Œ£y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œ£z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct the mean vector of Z\n",
    "ùúáz = np.zeros(2*(T+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python code lets us sample random vectors $ X $ and\n",
    "$ Y $.\n",
    "\n",
    "This is going to be very useful for doing the conditioning to be used in\n",
    "the fun exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "z = np.random.multivariate_normal(ùúáz, Œ£z)\n",
    "\n",
    "x = z[:T+1]\n",
    "y = z[T+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Example\n",
    "\n",
    "This is an instance of a classic `smoothing` calculation whose purpose\n",
    "is to compute $ E X \\mid Y $.\n",
    "\n",
    "An interpretation of this example is\n",
    "\n",
    "- $ X $ is a random sequence of hidden Markov state variables\n",
    "  $ x_t $  \n",
    "- $ Y $ is a sequence of observed signals $ y_t $ bearing\n",
    "  information about the hidden state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct a MultivariateNormal instance\n",
    "multi_normal_ex1 = MultivariateNormal(ùúáz, Œ£z)\n",
    "x = z[:T+1]\n",
    "y = z[T+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# partition Z into X and Y\n",
    "multi_normal_ex1.partition(T+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# compute the conditional mean and covariance matrix of X given Y=y\n",
    "\n",
    "print(\"X = \", x)\n",
    "print(\"Y = \", y)\n",
    "print(\" E [ X | Y] = \", )\n",
    "\n",
    "multi_normal_ex1.cond_dist(0, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Exercise\n",
    "\n",
    "Compute $ E\\left[x_{t} \\mid y_{t-1}, y_{t-2}, \\dots, y_{0}\\right] $.\n",
    "\n",
    "To do so, we need to first construct the mean vector and the covariance\n",
    "matrix of the subvector\n",
    "$ \\left[x_{t}, y_{0}, \\dots, y_{t-2}, y_{t-1}\\right] $.\n",
    "\n",
    "For example, let‚Äôs say that we want the conditional distribution of\n",
    "$ x_{3} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "t = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# mean of the subvector\n",
    "sub_ùúáz = np.zeros(t+1)\n",
    "\n",
    "# covariance matrix of the subvector\n",
    "sub_Œ£z = np.empty((t+1, t+1))\n",
    "\n",
    "sub_Œ£z[0, 0] = Œ£z[t, t] # x_t\n",
    "sub_Œ£z[0, 1:] = Œ£z[t, T+1:T+t+1]\n",
    "sub_Œ£z[1:, 0] = Œ£z[T+1:T+t+1, t]\n",
    "sub_Œ£z[1:, 1:] = Œ£z[T+1:T+t+1, T+1:T+t+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sub_Œ£z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_ex2 = MultivariateNormal(sub_ùúáz, sub_Œ£z)\n",
    "multi_normal_ex2.partition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sub_y = y[:t]\n",
    "\n",
    "multi_normal_ex2.cond_dist(0, sub_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Exercise\n",
    "\n",
    "Compute $ E\\left[y_{t} \\mid y_{t-j}, \\dots, y_{0} \\right] $.\n",
    "\n",
    "As what we did in exercise 2, we will construct the mean vector and\n",
    "covariance matrix of the subvector\n",
    "$ \\left[y_{t}, y_{0}, \\dots, y_{t-j-1}, y_{t-j} \\right] $.\n",
    "\n",
    "For example, we take a case in which $ t=3 $ and $ j=2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "t = 3\n",
    "j = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sub_ùúáz = np.zeros(t-j+2)\n",
    "sub_Œ£z = np.empty((t-j+2, t-j+2))\n",
    "\n",
    "sub_Œ£z[0, 0] = Œ£z[T+t+1, T+t+1]\n",
    "sub_Œ£z[0, 1:] = Œ£z[T+t+1, T+1:T+t-j+2]\n",
    "sub_Œ£z[1:, 0] = Œ£z[T+1:T+t-j+2, T+t+1]\n",
    "sub_Œ£z[1:, 1:] = Œ£z[T+1:T+t-j+2, T+1:T+t-j+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sub_Œ£z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_ex3 = MultivariateNormal(sub_ùúáz, sub_Œ£z)\n",
    "multi_normal_ex3.partition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sub_y = y[:t-j+1]\n",
    "\n",
    "multi_normal_ex3.cond_dist(0, sub_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a Wold Representation\n",
    "\n",
    "Now we‚Äôll apply Cholesky decomposition to decompose\n",
    "$ \\Sigma_{y}=H H^{\\prime} $ and form\n",
    "\n",
    "$$\n",
    "\\epsilon = H^{-1} Y.\n",
    "$$\n",
    "\n",
    "Then we can represent $ y_{t} $ as\n",
    "\n",
    "$$\n",
    "y_{t} = h_{t,t} \\epsilon_{t} + h_{t,t-1} \\epsilon_{t-1} + \\dots + h_{t,0} \\epsilon_{0}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "H = np.linalg.cholesky(Œ£y)\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúñ = np.linalg.inv(H) @ y\n",
    "\n",
    "ùúñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is an instance of what is known as a **Wold representation** in time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Factor Analysis Model\n",
    "\n",
    "The factor analysis model widely used in psychology and other fields can\n",
    "be represented as\n",
    "\n",
    "$$\n",
    "Y = \\Lambda f + U\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "1. $ Y $ is $ n \\times 1 $ random vector,\n",
    "  $ E U U^{\\prime} = D $ is a diagonal matrix,  \n",
    "1. $ \\Lambda $ is $ n \\times k $ coefficient matrix,  \n",
    "1. $ f $ is $ k \\times 1 $ random vector,\n",
    "  $ E f f^{\\prime} = I $,  \n",
    "1. $ U $ is $ n \\times 1 $ random vector, and $ U \\perp f $.  \n",
    "1. It is presumed that $ k $ is small relative to $ n $; often\n",
    "  $ k $ is only $ 1 $ or $ 2 $, as in our IQ examples.  \n",
    "\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "\\Sigma_y = E Y Y^{\\prime} = \\Lambda \\Lambda^{\\prime} + D \\\\\n",
    "E Y f^{\\prime} = \\Lambda \\\\\n",
    "E f Y^{\\prime} = \\Lambda^{\\prime}\n",
    "$$\n",
    "\n",
    "Thus, the covariance matrix $ \\Sigma_Y $ is the sum of a diagonal\n",
    "matrix $ D $ and a positive semi-definite matrix\n",
    "$ \\Lambda \\Lambda^{\\prime} $ of rank $ k $.\n",
    "\n",
    "This means that all covariances among the $ n $ components of the\n",
    "$ Y $ vector are intermediated by their common dependencies on the\n",
    "$ k< $ factors.\n",
    "\n",
    "Form\n",
    "\n",
    "$$\n",
    "Z=\\left(\\begin{array}{c}\n",
    "f\\\\\n",
    "Y\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "the covariance matrix of the expanded random vector $ Z $ can be\n",
    "computed as\n",
    "\n",
    "$$\n",
    "\\Sigma_{z} = EZZ^{\\prime}=\\left(\\begin{array}{cc}\n",
    "I & \\Lambda^{\\prime}\\\\\n",
    "\\Lambda & \\Lambda\\Lambda^{\\prime}+D\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "In the following, we first construct the mean vector and the covariance\n",
    "matrix for the case where $ N=10 $ and $ k=2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the coefficient matrix $ \\Lambda $ and the covariance matrix\n",
    "of $ U $ to be\n",
    "\n",
    "$$\n",
    "\\Lambda=\\left(\\begin{array}{cc}\n",
    "1 & 0\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right),\\quad D=\\left(\\begin{array}{cccc}\n",
    "\\sigma_{u}^{2} & 0 & \\cdots & 0\\\\\n",
    "0 & \\sigma_{u}^{2} & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & \\sigma_{u}^{2}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "where the first half of the first column of $ \\Lambda $ is filled\n",
    "with $ 1 $s and $ 0 $s for the rest half, and symmetrically\n",
    "for the second column. $ D $ is a diagonal matrix with parameter\n",
    "$ \\sigma_{u}^{2} $ on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œõ = np.zeros((N, k))\n",
    "Œõ[:N//2, 0] = 1\n",
    "Œõ[N//2:, 1] = 1\n",
    "\n",
    "ùúéu = .5\n",
    "D = np.eye(N) * ùúéu ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# compute Œ£y\n",
    "Œ£y = Œõ @ Œõ.T + D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct the mean vector and the covariance matrix for\n",
    "$ Z $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúáz = np.zeros(k+N)\n",
    "\n",
    "Œ£z = np.empty((k+N, k+N))\n",
    "\n",
    "Œ£z[:k, :k] = np.eye(k)\n",
    "Œ£z[:k, k:] = Œõ.T\n",
    "Œ£z[k:, :k] = Œõ\n",
    "Œ£z[k:, k:] = Œ£y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "z = np.random.multivariate_normal(ùúáz, Œ£z)\n",
    "\n",
    "f = z[:k]\n",
    "y = z[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_factor = MultivariateNormal(ùúáz, Œ£z)\n",
    "multi_normal_factor.partition(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compute the conditional distribution of the hidden factor\n",
    "$ f $ on the observations $ Y $, namely, $ f \\mid Y=y $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_factor.cond_dist(0, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the conditional mean\n",
    "$ E \\left[f \\mid Y=y\\right] = B Y $ where\n",
    "$ B = \\Lambda^{\\prime} \\Sigma_{y}^{-1} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "B = Œõ.T @ np.linalg.inv(Œ£y)\n",
    "\n",
    "B @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compute the conditional distribution $ Y \\mid f $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multi_normal_factor.cond_dist(1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be verified that the mean is\n",
    "$ \\Lambda I^{-1} f = \\Lambda f $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œõ @ f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as Approximation to Factor Analytic Model\n",
    "\n",
    "For fun, let‚Äôs apply a Principal Components Analysis (PCA) decomposition\n",
    "to a covariance matrix $ \\Sigma_y $ that in fact is governed by our factor-analytic\n",
    "model.\n",
    "\n",
    "Technically, this means that the PCA model is misspecified. (Can you\n",
    "explain why?)\n",
    "\n",
    "Nevertheless, this exercise will let us study how well the first two\n",
    "principal components from a PCA can approximate the conditional\n",
    "expectations $ E f_i | Y $ for our two factors $ f_i $,\n",
    "$ i=1,2 $ for the factor analytic model that we have assumed truly\n",
    "governs the data on $ Y $ we have generated.\n",
    "\n",
    "So we compute the PCA decomposition\n",
    "\n",
    "$$\n",
    "\\Sigma_{y} = P \\tilde{\\Lambda} P^{\\prime}\n",
    "$$\n",
    "\n",
    "where $ \\tilde{\\Lambda} $ is a diagonal matrix.\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "Y = P \\epsilon\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\epsilon = P^\\prime Y\n",
    "$$\n",
    "\n",
    "Note that we will arrange the eigenvectors in $ P $ in the\n",
    "*descending* order of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúÜ_tilde, P = np.linalg.eigh(Œ£y)\n",
    "\n",
    "# arrange the eigenvectors by eigenvalues\n",
    "ind = sorted(range(N), key=lambda x: ùúÜ_tilde[x], reverse=True)\n",
    "\n",
    "P = P[:, ind]\n",
    "ùúÜ_tilde = ùúÜ_tilde[ind]\n",
    "Œõ_tilde = np.diag(ùúÜ_tilde)\n",
    "\n",
    "print('ùúÜ_tilde =', ùúÜ_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# verify the orthogonality of eigenvectors\n",
    "np.abs(P @ P.T - np.eye(N)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# verify the eigenvalue decomposition is correct\n",
    "P @ Œõ_tilde @ P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúñ = P.T @ y\n",
    "\n",
    "print(\"ùúñ = \", ùúñ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# print the values of the two factors\n",
    "\n",
    "print('f = ', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we‚Äôll plot several things\n",
    "\n",
    "- the $ N $ values of $ y $  \n",
    "- the $ N $ values of the principal components $ \\epsilon $  \n",
    "- the value of the first factor $ f_1 $ plotted only for the first\n",
    "  $ N/2 $ observations of $ y $ for which it receives a\n",
    "  non-zero loading in $ \\Lambda $  \n",
    "- the value of the second factor $ f_2 $ plotted only for the final\n",
    "  $ N/2 $ observations for which it receives a non-zero loading in\n",
    "  $ \\Lambda $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(N), y, label='y')\n",
    "plt.scatter(range(N), ùúñ, label='$\\epsilon$')\n",
    "plt.hlines(f[0], 0, N//2-1, ls='--', label='$f_{1}$')\n",
    "plt.hlines(f[1], N//2, N-1, ls='-.', label='$f_{2}$')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, the first two $ \\epsilon_{j} $ correspond to the\n",
    "largest two eigenvalues.\n",
    "\n",
    "Let‚Äôs look at them, after which we‚Äôll look at $ E f | y = B y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúñ[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# compare with Ef|y\n",
    "B @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fraction of variance in $ y_{t} $ explained by the first two\n",
    "principal component can be computed as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúÜ_tilde[:2].sum() / ùúÜ_tilde.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute\n",
    "\n",
    "$$\n",
    "\\hat{Y} = P_{j} \\epsilon_{j} + P_{k} \\epsilon_{k}\n",
    "$$\n",
    "\n",
    "where $ P_{j} $ and $ P_{k} $ correspond to the largest two\n",
    "eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y_hat = P[:, :2] @ ùúñ[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, it turns out that the projection $ \\hat{Y} $ of\n",
    "$ Y $ on the first two principal components does a good job of\n",
    "approximating $ Ef \\mid y $.\n",
    "\n",
    "We confirm this in the following plot of $ f $,\n",
    "$ E y \\mid f $, $ E f \\mid y $, and $ \\hat{y} $ on the\n",
    "coordinate axis versus $ y $ on the ordinate axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(N), Œõ @ f, label='$Ey|f$')\n",
    "plt.scatter(range(N), y_hat, label='$\\hat{y}$')\n",
    "plt.hlines(f[0], 0, N//2-1, ls='--', label='$f_{1}$')\n",
    "plt.hlines(f[1], N//2, N-1, ls='-.', label='$f_{2}$')\n",
    "\n",
    "Efy = B @ y\n",
    "plt.hlines(Efy[0], 0, N//2-1, ls='--', color='b', label='$Ef_{1}|y$')\n",
    "plt.hlines(Efy[1], N//2, N-1, ls='-.', color='b', label='$Ef_{2}|y$')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix of $ \\hat{Y} $ can be computed by first\n",
    "constructing the covariance matrix of $ \\epsilon $ and then use the\n",
    "upper left block for $ \\epsilon_{1} $ and $ \\epsilon_{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Œ£ùúñjk = (P.T @ Œ£y @ P)[:2, :2]\n",
    "\n",
    "Pjk = P[:, :2]\n",
    "\n",
    "Œ£y_hat = Pjk @ Œ£ùúñjk @ Pjk.T\n",
    "print('Œ£y_hat = \\n', Œ£y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Difference Equation\n",
    "\n",
    "Consider the stochastic second-order linear difference equation\n",
    "\n",
    "$$\n",
    "y_{t} = \\alpha_{0} + \\alpha_{1} y_{y-1} + \\alpha_{2} y_{t-2} + u_{t}\n",
    "$$\n",
    "\n",
    "where $ u_{t} \\sim N \\left(0, \\sigma_{u}^{2}\\right) $ and\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "y_{-1}\\\\\n",
    "y_{0}\n",
    "\\end{array}\\right]\\sim N\\left(\\mu_{\\tilde{y}},\\Sigma_{\\tilde{y}}\\right)\n",
    "$$\n",
    "\n",
    "It can be written as a stacked system\n",
    "\n",
    "$$\n",
    "\\underset{\\equiv A}{\\underbrace{\\left[\\begin{array}{cccccccc}\n",
    "1 & 0 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\n",
    "-\\alpha_{1} & 1 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\n",
    "-\\alpha_{2} & -\\alpha_{1} & 1 & 0 & \\cdots & 0 & 0 & 0\\\\\n",
    "0 & -\\alpha_{2} & -\\alpha_{1} & 1 & \\cdots & 0 & 0 & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & -\\alpha_{2} & -\\alpha_{1} & 1\n",
    "\\end{array}\\right]}}\\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "y_{4}\\\\\n",
    "\\vdots\\\\\n",
    "y_{T}\n",
    "\\end{array}\\right]=\\underset{\\equiv b}{\\underbrace{\\left[\\begin{array}{c}\n",
    "\\alpha_{0}+\\alpha_{1}y_{0}+\\alpha_{2}y_{-1}\\\\\n",
    "\\alpha_{0}+\\alpha_{2}y_{0}\\\\\n",
    "\\alpha_{0}\\\\\n",
    "\\alpha_{0}\\\\\n",
    "\\vdots\\\\\n",
    "\\alpha_{0}\n",
    "\\end{array}\\right]}}\n",
    "$$\n",
    "\n",
    "We can compute $ y $ by solving the system\n",
    "\n",
    "$$\n",
    "y = A^{-1} \\left(b + u\\right)\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\mu_{y} = A^{-1} \\mu_{b} \\\\\n",
    "\\begin{aligned}\n",
    "\\Sigma_{y} &= A^{-1} E \\left[\\left(b - \\mu_{b} + u \\right) \\left(b - \\mu_{b} + u \\right)^{\\prime}\\right] \\left(A^{-1}\\right)^{\\prime} \\\\\n",
    "           &= A^{-1} \\left(\\Sigma_{b} + \\Sigma_{u} \\right) \\left(A^{-1}\\right)^{\\prime}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mu_{b}=\\left[\\begin{array}{c}\n",
    "\\alpha_{0}+\\alpha_{1}\\mu_{y_{0}}+\\alpha_{2}\\mu_{y_{-1}}\\\\\n",
    "\\alpha_{0}+\\alpha_{2}\\mu_{y_{0}}\\\\\n",
    "\\alpha_{0}\\\\\n",
    "\\vdots\\\\\n",
    "\\alpha_{0}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{b}=\\left[\\begin{array}{cc}\n",
    "C\\Sigma_{\\tilde{y}}C^{\\prime} & \\boldsymbol{0}_{N-2\\times N-2}\\\\\n",
    "\\boldsymbol{0}_{N-2\\times2} & \\boldsymbol{0}_{N-2\\times N-2}\n",
    "\\end{array}\\right],\\quad C=\\left[\\begin{array}{cc}\n",
    "\\alpha_{2} & \\alpha_{1}\\\\\n",
    "0 & \\alpha_{2}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{u}=\\left[\\begin{array}{cccc}\n",
    "\\sigma_{u}^{2} & 0 & \\cdots & 0\\\\\n",
    "0 & \\sigma_{u}^{2} & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & \\sigma_{u}^{2}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "T = 80\n",
    "T = 160\n",
    "# coefficients of the second order difference equation\n",
    "ùõº0 = 10\n",
    "ùõº1 = 1.53\n",
    "ùõº2 = -.9\n",
    "\n",
    "# variance of u\n",
    "ùúéu = 1.\n",
    "ùúéu = 10.\n",
    "\n",
    "# distribution of y_{-1} and y_{0}\n",
    "ùúáy_tilde = np.array([1., 0.5])\n",
    "Œ£y_tilde = np.array([[2., 1.], [1., 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct A and A^{\\prime}\n",
    "A = np.zeros((T, T))\n",
    "\n",
    "for i in range(T):\n",
    "    A[i, i] = 1\n",
    "\n",
    "    if i-1 >= 0:\n",
    "        A[i, i-1] = -ùõº1\n",
    "\n",
    "    if i-2 >= 0:\n",
    "        A[i, i-2] = -ùõº2\n",
    "\n",
    "A_inv = np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# compute the mean vectors of b and y\n",
    "ùúáb = np.ones(T) * ùõº0\n",
    "ùúáb[0] += ùõº1 * ùúáy_tilde[1] + ùõº2 * ùúáy_tilde[0]\n",
    "ùúáb[1] += ùõº2 * ùúáy_tilde[1]\n",
    "\n",
    "ùúáy = A_inv @ ùúáb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# compute the covariance matrices of b and y\n",
    "Œ£u = np.eye(T) * ùúéu ** 2\n",
    "\n",
    "Œ£b = np.zeros((T, T))\n",
    "\n",
    "C = np.array([[ùõº2, ùõº1], [0, ùõº2]])\n",
    "Œ£b[:2, :2] = C @ Œ£y_tilde @ C.T\n",
    "\n",
    "Œ£y = A_inv @ (Œ£b + Œ£u) @ A_inv.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Stock Price Model\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "p_{t} = \\sum_{j=0}^{T-t} \\beta^{j} y_{t+j}\n",
    "$$\n",
    "\n",
    "Form\n",
    "\n",
    "$$\n",
    "\\underset{\\equiv p}{\\underbrace{\\left[\\begin{array}{c}\n",
    "p_{1}\\\\\n",
    "p_{2}\\\\\n",
    "p_{3}\\\\\n",
    "\\vdots\\\\\n",
    "p_{T}\n",
    "\\end{array}\\right]}}=\\underset{\\equiv B}{\\underbrace{\\left[\\begin{array}{ccccc}\n",
    "1 & \\beta & \\beta^{2} & \\cdots & \\beta^{T-1}\\\\\n",
    "0 & 1 & \\beta & \\cdots & \\beta^{T-2}\\\\\n",
    "0 & 0 & 1 & \\cdots & \\beta^{T-3}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{array}\\right]}}\\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{T}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "\\mu_{p} = B \\mu_{y} \\\\\n",
    "\\Sigma_{p} = B \\Sigma_{y} B^{\\prime}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùõΩ = .96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# construct B\n",
    "B = np.zeros((T, T))\n",
    "\n",
    "for i in range(T):\n",
    "    B[i, i:] = ùõΩ ** np.arange(0, T-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote\n",
    "\n",
    "$$\n",
    "z=\\left[\\begin{array}{c}\n",
    "y\\\\\n",
    "p\n",
    "\\end{array}\\right]=\\underset{\\equiv D}{\\underbrace{\\left[\\begin{array}{c}\n",
    "I\\\\\n",
    "B\n",
    "\\end{array}\\right]}} y\n",
    "$$\n",
    "\n",
    "Thus, $ \\{y_t\\}_{t=1}^{T} $ and $ \\{p_t\\}_{t=1}^{T} $ jointly\n",
    "follow the multivariate normal distribution\n",
    "$ N \\left(\\mu_{z}, \\Sigma_{z}\\right) $, where\n",
    "\n",
    "$$\n",
    "\\mu_{z}=D\\mu_{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{z}=D\\Sigma_{y}D^{\\prime}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "D = np.vstack([np.eye(T), B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ùúáz = D @ ùúáy\n",
    "Œ£z = D @ Œ£y @ D.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate paths of $ y_{t} $ and $ p_{t} $ and compute the\n",
    "conditional mean $ E \\left[p_{t} \\mid y_{t-1}, y_{t}\\right] $ using\n",
    "the `MultivariateNormal` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "z = np.random.multivariate_normal(ùúáz, Œ£z)\n",
    "y, p = z[:T], z[T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "cond_Ep = np.empty(T-1)\n",
    "\n",
    "sub_ùúá = np.empty(3)\n",
    "sub_Œ£ = np.empty((3, 3))\n",
    "for t in range(2, T+1):\n",
    "    sub_ùúá[:] = ùúáz[[t-2, t-1, T-1+t]]\n",
    "    sub_Œ£[:, :] = Œ£z[[t-2, t-1, T-1+t], :][:, [t-2, t-1, T-1+t]]\n",
    "\n",
    "    multi_normal = MultivariateNormal(sub_ùúá, sub_Œ£)\n",
    "    multi_normal.partition(2)\n",
    "\n",
    "    cond_Ep[t-2] = multi_normal.cond_dist(1, y[t-2:t])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, T), y[1:], label='$y_{t}$')\n",
    "plt.plot(range(1, T), y[:-1], label='$y_{t-1}$')\n",
    "plt.plot(range(1, T), p[1:], label='$p_{t}$')\n",
    "plt.plot(range(1, T), cond_Ep, label='$Ep_{t}|y_{t}, y_{t-1}$')\n",
    "\n",
    "plt.xlabel('t')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, the green line is what the price of the stock would\n",
    "be if people had perfect foresight about the path of dividends while the\n",
    "green line is the conditional expectation $ E p_t | y_t, y_{t-1} $, which is what the price would\n",
    "be if people did not have perfect foresight but were optimally\n",
    "predicting future dividends on the basis of the information\n",
    "$ y_t, y_{t-1} $ at time $ t $."
   ]
  }
 ],
 "metadata": {
  "date": 1601775356.8744512,
  "filename": "multivariate_normal.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "next_doc": {
   "link": "time_series_with_matrices",
   "title": "Univariate Time Series with Matrix Algebra"
  },
  "prev_doc": {
   "link": "heavy_tails",
   "title": "Heavy-Tailed Distributions"
  },
  "title": "Multivariate Normal Distribution"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}