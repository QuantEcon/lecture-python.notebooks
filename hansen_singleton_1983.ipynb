{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b551b1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42d3dc",
   "metadata": {},
   "source": [
    "\n",
    "<a id='hansen-singleton-1983'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f5a19",
   "metadata": {},
   "source": [
    "# Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns\n",
    "\n",
    "\n",
    "<a id='index-0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9c948",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](#Stochastic-Consumption,-Risk-Aversion,-and-the-Temporal-Behavior-of-Asset-Returns)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Euler equation](#Euler-equation)  \n",
    "  - [The Euler equation under lognormality](#The-Euler-equation-under-lognormality)  \n",
    "  - [The restricted system and its likelihood](#The-restricted-system-and-its-likelihood)  \n",
    "  - [Likelihood implementation](#Likelihood-implementation)  \n",
    "  - [Preference parameters and likelihood ratio tests](#Preference-parameters-and-likelihood-ratio-tests)  \n",
    "  - [Predictability and the R-squared restriction](#Predictability-and-the-R-squared-restriction)  \n",
    "  - [Return-difference tests](#Return-difference-tests)  \n",
    "  - [Empirical MLE estimation](#Empirical-MLE-estimation)  \n",
    "  - [Residual diagnostics](#Residual-diagnostics)  \n",
    "  - [Connection to the equity premium puzzle](#Connection-to-the-equity-premium-puzzle)  \n",
    "  - [Another approach](#Another-approach)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fb62d",
   "metadata": {},
   "source": [
    "> Evans and Honkapohja: What were the profession’s most important responses\n",
    "to the Lucas Critique?\n",
    "\n",
    "Sargent: There were two. The first and most optimistic response was complete\n",
    "rational expectations econometrics. A rational expectations equilibrium is a\n",
    "likelihood function. Maximize it.\n",
    "\n",
    "— An Interview with Thomas J. Sargent [[Evans and Honkapohja, 2005](https://python.quantecon.org/zreferences.html#id3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf27c36",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture describes how   Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] formulated a complete statistical model of asset returns and consumption growth, then estimated its parameters by the method of  maximum likelihood.\n",
    "\n",
    "They detect a defects in their model, one of which Mehra and Prescott [[1985](https://python.quantecon.org/zreferences.html#id218)] later  called   the **equity premium puzzle**.\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] study a consumption-based asset pricing model in which a representative consumer with CRRA preferences chooses how to allocate wealth across traded assets.\n",
    "\n",
    "First-order conditions for asset holdings are  stochastic Euler equations that  connect consumption growth, asset returns, and preference parameters.\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] assume that consumption growth and asset returns are *jointly lognormal*.\n",
    "\n",
    "Then   Euler equations imply a set of restrictions on a  the joint distribution of  asset prices and returns.\n",
    "\n",
    "These restrict a linear time-series model for  logarithms of consumption growth and returns in which  predictable movements in log returns are proportional to predictable movements in log consumption growth.\n",
    "\n",
    "Hansen and Singleton estimated their model by [Maximum Likelihood Estimation](https://python.quantecon.org/mle.html).\n",
    "\n",
    "The empirical findings of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] constitute  what Mehra and Prescott [[1985](https://python.quantecon.org/zreferences.html#id218)] would later call the **equity premium puzzle**.\n",
    "\n",
    "To keep lecture this lecture narrowly focused, we estimate one return at a time (either a market proxy or a T-bill) rather than the  full multi-asset systems studied by  Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)].\n",
    "\n",
    "- we  use only monthly nondurable consumption (`ND`).  \n",
    "\n",
    "\n",
    "In addition to what comes with Anaconda, this lecture requires `pandas-datareader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59c50c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5be9ac",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Latex\n",
    "from pandas_datareader import data as web\n",
    "from scipy import stats\n",
    "from scipy.linalg import LinAlgError, cholesky, solve_triangular\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*date_parser.*\", category=FutureWarning\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af080bcc",
   "metadata": {},
   "source": [
    "We also define a helper to display DataFrames as LaTeX arrays in the hidden cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a83a2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def display_table(df, title=None, fmt=None):\n",
    "    \"\"\"\n",
    "    Display a DataFrame as a LaTeX array.\n",
    "    \"\"\"\n",
    "    if fmt is None:\n",
    "        fmt = {}\n",
    "    formatted = df.copy()\n",
    "    for col in formatted.columns:\n",
    "        if col in fmt:\n",
    "            formatted[col] = formatted[col].apply(\n",
    "                lambda x: fmt[col].format(x) if np.isfinite(x) else str(x))\n",
    "    n_cols = len(formatted.columns)\n",
    "    idx_header = r\"\\text{\" + df.index.name + \"}\" if df.index.name else \"\"\n",
    "    columns = \" & \".join(\n",
    "        [idx_header] + [r\"\\text{\" + c + \"}\" if \"\\\\\" not in c\n",
    "         and \"^\" not in c and \"_\" not in c\n",
    "         else c for c in formatted.columns])\n",
    "    rows = r\" \\\\\".join(\n",
    "        [\" & \".join([str(idx)] + [str(v) for v in row])\n",
    "         for idx, row in zip(formatted.index, formatted.values)])\n",
    "    col_format = \"r\" + \"c\" * n_cols\n",
    "    lines = [r\"\\begin{array}{\" + col_format + \"}\"]\n",
    "    lines.append(columns + r\" \\\\\")\n",
    "    lines.append(r\"\\hline\")\n",
    "    lines.append(rows)\n",
    "    lines.append(r\"\\end{array}\")\n",
    "    latex = \"\\n\".join(lines)\n",
    "    if title:\n",
    "        latex = rf\"\\textbf{{{title}}}\" + r\"\\\\\" + \"\\n\" + latex\n",
    "    display(Latex(\"$\" + latex + \"$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbeae38",
   "metadata": {},
   "source": [
    "## Euler equation\n",
    "\n",
    "Consider a single-good economy of identical consumers whose utility functions are in the CRRA form\n",
    "\n",
    "\n",
    "<a id='equation-hs83-crra'></a>\n",
    "$$\n",
    "U(c_t) = c_t^{\\gamma}/\\gamma, \\quad \\gamma < 1, \\tag{85.1}\n",
    "$$\n",
    "\n",
    "where $ c_t $ is aggregate real per capita consumption and $ U(\\cdot) $ is the period utility function.\n",
    "\n",
    "The representative consumer chooses a stochastic consumption plan to maximize the expected value of a time-additive utility function,\n",
    "\n",
    "\n",
    "<a id='equation-hs83-objective'></a>\n",
    "$$\n",
    "E_0 \\sum_{t=0}^{\\infty} \\beta^t U(c_t), \\quad 0 < \\beta < 1. \\tag{85.2}\n",
    "$$\n",
    "\n",
    "Consumers substitute present for future consumption by trading the ownership rights of $ N $ financial and capital assets.\n",
    "\n",
    "Let $ \\mathbf{w}_t $ denote the holdings of the $ N $ assets at date $ t $, $ \\mathbf{q}_t $ the vector of asset prices, $ \\mathbf{d}_t $ the vector of dividends, and $ y_t $ real labor income.\n",
    "\n",
    "A feasible consumption and investment plan $ \\{c_t, \\mathbf{w}_t\\} $ must satisfy the sequence of budget constraints\n",
    "\n",
    "\n",
    "<a id='equation-hs83-budget'></a>\n",
    "$$\n",
    "c_t + \\mathbf{q}_t \\cdot \\mathbf{w}_{t+1} \\leq (\\mathbf{q}_t + \\mathbf{d}_t) \\cdot \\mathbf{w}_t + y_t, \\tag{85.3}\n",
    "$$\n",
    "\n",
    "where $ (\\mathbf{q}_t + \\mathbf{d}_t) \\cdot \\mathbf{w}_t $ is the cum-dividend value of the portfolio carried into period $ t $.\n",
    "\n",
    "To derive the first-order conditions, attach a Lagrange multiplier $ \\lambda_t $ to the budget constraint [(85.3)](#equation-hs83-budget) at each date $ t $ and form the Lagrangian\n",
    "\n",
    "\n",
    "<a id='equation-hs83-lagrangian'></a>\n",
    "$$\n",
    "\\mathcal{L} = E_0 \\sum_{t=0}^{\\infty} \\beta^t \\left\\{ U(c_t) + \\lambda_t \\left[ (\\mathbf{q}_t + \\mathbf{d}_t) \\cdot \\mathbf{w}_t + y_t - c_t - \\mathbf{q}_t \\cdot \\mathbf{w}_{t+1} \\right] \\right\\}. \\tag{85.4}\n",
    "$$\n",
    "\n",
    "Differentiating $ \\mathcal{L} $ with respect to $ c_t $ gives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial c_t} = 0 \\implies \\lambda_t = U'(c_t).\n",
    "$$\n",
    "\n",
    "Differentiating with respect to $ w_{i,t+1} $, the holdings of asset $ i $ carried from date $ t $ into date $ t+1 $, collects two terms: $ w_{i,t+1} $ appears in the date-$ t $ budget constraint as well as in the date-$ (t+1) $ constraint:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{i,t+1}} = 0 \\implies E_0\\!\\left[\\beta^t \\big(- \\lambda_t\\, q_{it} + \\beta\\, \\lambda_{t+1} (q_{i,t+1} + d_{i,t+1})\\big)\\right] = 0.\n",
    "$$\n",
    "\n",
    "By the law of iterated expectations, this becomes\n",
    "\n",
    "$$\n",
    "E_0\\!\\left[\\beta^t\\, E_t\\!\\left(- \\lambda_t\\, q_{it} + \\beta\\, \\lambda_{t+1} (q_{i,t+1} + d_{i,t+1})\\right)\\right] = 0.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\lambda_t\\, q_{it} = \\beta\\, E_t\\!\\left[\\lambda_{t+1}(q_{i,t+1} + d_{i,t+1})\\right].\n",
    "$$\n",
    "\n",
    "Substituting $ \\lambda_t = U'(c_t) $ and $ \\lambda_{t+1} = U'(c_{t+1}) $ yields\n",
    "\n",
    "$$\n",
    "q_{it}\\, U'(c_t) = \\beta\\, E_t\\!\\left[U'(c_{t+1})(q_{i,t+1} + d_{i,t+1})\\right].\n",
    "$$\n",
    "\n",
    "Dividing both sides by $ q_{it} $ and defining the gross return $ r_{it+1} := (q_{i,t+1} + d_{i,t+1})/q_{it} $ yields the stochastic Euler equation:\n",
    "\n",
    "\n",
    "<a id='equation-hs83-foc'></a>\n",
    "$$\n",
    "U'(c_t) = \\beta E_t\\!\\left[U'(c_{t+1})\\, r_{it+1}\\right], \\quad i = 1, \\ldots, N, \\tag{85.5}\n",
    "$$\n",
    "\n",
    "where $ r_{it+1} $ is the gross real return on asset $ i $.\n",
    "\n",
    "Substituting the CRRA marginal utility $ U'(c_t) = c_t^{\\gamma-1} = c_t^{\\alpha} $ with $ \\alpha := \\gamma - 1 $ into [(85.5)](#equation-hs83-foc) and rearranging gives\n",
    "\n",
    "\n",
    "<a id='equation-hs83-euler'></a>\n",
    "$$\n",
    "E_t\\!\\left[\\beta \\left(\\frac{c_{t+1}}{c_t}\\right)^{\\alpha} r_{it+1}\\right] = 1, \\quad i = 1, \\ldots, N. \\tag{85.6}\n",
    "$$\n",
    "\n",
    "The coefficient of relative risk aversion is $ -\\alpha $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305acdd9",
   "metadata": {},
   "source": [
    "## The Euler equation under lognormality\n",
    "\n",
    "Using the Euler equation [(85.6)](#equation-hs83-euler) derived above, we now impose the distributional assumptions of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)].\n",
    "\n",
    "Let $ x_t = c_t / c_{t-1} $ denote the consumption ratio, and define $ u_{it} = x_t^\\alpha r_{it} $ where $ r_{it} $ is the gross real return on asset $ i $.\n",
    "\n",
    "The Euler equation [(85.6)](#equation-hs83-euler) states $ E_{t-1}[u_{it}] = 1/\\beta $.\n",
    "\n",
    "Define log variables $ X_t = \\log x_t $, $ R_{it} = \\log r_{it} $, and $ U_{it} = \\log u_{it} $, so that\n",
    "\n",
    "\n",
    "<a id='equation-hs83-u-def'></a>\n",
    "$$\n",
    "U_{i,t}= \\alpha X_{t}+R_{i,t}. \\tag{85.7}\n",
    "$$\n",
    "\n",
    "Assume the vector process $ \\{Y_t\\} = \\{(X_t, R_{1t}, \\ldots, R_{nt})^\\top\\} $ is jointly stationary and Gaussian.\n",
    "\n",
    "Under this assumption, $ U_{it} $ conditional on the information set $ \\psi_{t-1} $ is normal with constant variance $ \\sigma_i^2 $ and a conditional mean $ \\mu_{i,t-1} $ that is a linear function of past $ Y $’s.\n",
    "\n",
    "Because $ u_{it} = \\exp(U_{it}) $ is conditionally lognormal,\n",
    "\n",
    "\n",
    "<a id='equation-hs83-lognormal-identity'></a>\n",
    "$$\n",
    "E_{t-1}[u_{it}] = \\exp\\left(\\mu_{i,t-1} + \\tfrac{1}{2}\\sigma_i^2\\right). \\tag{85.8}\n",
    "$$\n",
    "\n",
    "Setting $ E_{t-1}[u_{it}] = 1/\\beta $ and taking logs gives $ \\mu_{i,t-1} + \\sigma_i^2/2 = -\\log\\beta $.\n",
    "\n",
    "Now define the innovation\n",
    "\n",
    "\n",
    "<a id='equation-hs83-v-it'></a>\n",
    "$$\n",
    "V_{i,t} := U_{i,t} - \\mu_{i,t-1} = \\alpha X_{t}+R_{i,t}+\\log\\beta+\\frac{\\sigma_i^2}{2},\n",
    "\\quad i = 1, \\ldots, N. \\tag{85.9}\n",
    "$$\n",
    "\n",
    "Then $ E_{t-1}[V_{i,t}]=0 $, where $ \\sigma_i^2=\\operatorname{Var}_{t-1}(\\alpha X_t + R_{it}) $ is constant under the stationarity and Gaussian assumptions.\n",
    "\n",
    "Setting $ E_{t-1}[V_{i,t}] = 0 $ gives the conditional mean restriction\n",
    "\n",
    "\n",
    "<a id='equation-hs83-cond-mean'></a>\n",
    "$$\n",
    "E_{t-1}[R_{i,t}] = -\\alpha\\, E_{t-1}[X_{t}] - \\log\\beta - \\frac{\\sigma_i^2}{2}. \\tag{85.10}\n",
    "$$\n",
    "\n",
    "Equation [(85.10)](#equation-hs83-cond-mean) is the central result of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)].\n",
    "\n",
    "The predictable component of each asset’s log return is proportional to the predictable component of log consumption growth, with proportionality factor $ -\\alpha $.\n",
    "\n",
    "The intercept absorbs the discount factor $ \\beta $ and the lognormal variance term $ \\sigma_i^2 / 2 $.\n",
    "\n",
    "Let’s consider three special cases to better understand the implications of [(85.10)](#equation-hs83-cond-mean):\n",
    "\n",
    "- Risk neutrality ($ \\alpha = 0 $): Each asset’s log return equals a constant plus a serially uncorrelated error, so returns are serially uncorrelated.  \n",
    "- Log utility ($ \\alpha = -1 $): The difference $ R_{it} - X_t $ has no time-varying predictable component, so returns and consumption growth share the same predictable component.  \n",
    "- Risk aversion ($ \\alpha < 0 $): The time-varying part of $ E_{t-1}[R_{it}] $ equals $ -\\alpha $ times $ E_{t-1}[X_t] $, so a larger $ |\\alpha| $ amplifies the sensitivity of expected returns to expected consumption growth.  \n",
    "\n",
    "\n",
    "For a given consumption–return covariance structure, higher relative risk aversion ($ -\\alpha $) widens the gap between risky-asset returns and the risk-free return.\n",
    "\n",
    "The equity premium puzzle arises because the observed spread is large but estimated $ |\\alpha| $ is small as we will see soon in our estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe920a",
   "metadata": {},
   "source": [
    "## The restricted system and its likelihood\n",
    "\n",
    "To build a likelihood, we need to parameterize the conditional expectation $ E_{t-1}[X_t] $.\n",
    "\n",
    "In the single-return case, write $ \\mathbf{Y}_t = (X_t, R_t)^\\top $ and assume that the predictable component of $ X_t $ is a finite-order linear function of past observations:\n",
    "\n",
    "\n",
    "<a id='equation-hs83-x-forecast'></a>\n",
    "$$\n",
    "E(X_t\\mid\\psi_{t-1})=\\mathbf{a}(L)^\\top \\mathbf{Y}_{t-1}+\\mu_x, \\tag{85.11}\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{a}(L) $ is a vector of lag polynomial coefficients in past $ (X, R) $ and $ \\mu_x $ is a constant.\n",
    "\n",
    "The consumption-growth equation is unrestricted, so $ X_t $ depends freely on its own lags and on lagged returns.\n",
    "\n",
    "The return equation, however, is restricted by the Euler equation.\n",
    "\n",
    "Combining [(85.10)](#equation-hs83-cond-mean) with [(85.11)](#equation-hs83-x-forecast) forces the predictable part of $ R_t $ to be $ -\\alpha $ times the predictable part of $ X_t $, plus a constant.\n",
    "\n",
    "This gives the restricted system\n",
    "\n",
    "\n",
    "<a id='equation-hs83-restricted'></a>\n",
    "$$\n",
    "\\mathbf{A}_0\\mathbf{Y}_t=\\mathbf{A}_1(L)\\mathbf{Y}_{t-1}+\\boldsymbol{\\mu}+\\mathbf{V}_t, \\tag{85.12}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "\n",
    "<a id='equation-hs83-a0a1'></a>\n",
    "$$\n",
    "\\mathbf{A}_0=\\begin{bmatrix}1&0\\\\\\alpha&1\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{A}_1(L)=\\begin{bmatrix}\\mathbf{a}(L)^\\top\\\\0\\end{bmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{\\mu}=\\begin{bmatrix}\\mu_x\\\\-\\log\\beta-\\sigma_U^2/2\\end{bmatrix}, \\tag{85.13}\n",
    "$$\n",
    "\n",
    "where $ \\sigma_U^2 := \\operatorname{Var}_{t-1}(\\alpha X_t + R_t) = \\alpha^2 \\sigma_{XX} + \\sigma_{RR} + 2\\alpha \\sigma_{XR} $ under conditional homoskedasticity.\n",
    "\n",
    "The sign in the second element of $ \\boldsymbol{\\mu} $ follows directly from [(85.10)](#equation-hs83-cond-mean).\n",
    "\n",
    "The innovations $ \\mathbf{V}_t $ are assumed to be Gaussian with density $ f_V(\\mathbf{v}) $.\n",
    "\n",
    "Since $ \\mathbf{V}_t = \\mathbf{A}_0 \\mathbf{Y}_t - \\mathbf{A}_1(L)\\mathbf{Y}_{t-1} - \\boldsymbol{\\mu} $, the observables $ \\mathbf{Y}_t $ are a linear transformation of $ \\mathbf{V}_t $ with $ \\mathbf{Y}_t = \\mathbf{A}_0^{-1}(\\mathbf{V}_t + \\mathbf{A}_1(L)\\mathbf{Y}_{t-1} + \\boldsymbol{\\mu}) $.\n",
    "\n",
    "The change-of-variables formula for densities gives\n",
    "\n",
    "$$\n",
    "f_Y(\\mathbf{y}_t \\mid \\mathbf{Y}_{t-1}) = f_V(\\mathbf{A}_0 \\mathbf{y}_t - \\mathbf{A}_1(L)\\mathbf{Y}_{t-1} - \\boldsymbol{\\mu})\\;\\left|\\det\\!\\left(\\frac{\\partial \\mathbf{V}_t}{\\partial \\mathbf{Y}_t}\\right)\\right| = f_V(\\mathbf{v}_t)\\;|\\det(\\mathbf{A}_0)|.\n",
    "$$\n",
    "\n",
    "Because $ \\mathbf{A}_0 $ is unit lower triangular, $ \\det(\\mathbf{A}_0) = 1 $, so the Jacobian term disappears and the log-likelihood of $ \\mathbf{Y}_t $ equals the log-likelihood of $ \\mathbf{V}_t $ evaluated at the residuals.\n",
    "\n",
    "Since the Jacobian is unity, the conditional density of $ \\mathbf{Y}_t $ is the Gaussian density of $ \\mathbf{V}_t $ evaluated at $ \\mathbf{v}_t = \\mathbf{A}_0 \\mathbf{Y}_t - \\mathbf{A}_1(L)\\mathbf{Y}_{t-1} - \\boldsymbol{\\mu} $.\n",
    "\n",
    "For a single observation, $ \\mathbf{V}_t \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma}) $ has density\n",
    "\n",
    "$$\n",
    "f_V(\\mathbf{v}_t) = (2\\pi)^{-1} |\\boldsymbol{\\Sigma}|^{-1/2} \\exp\\!\\left(-\\tfrac{1}{2}\\,\\mathbf{v}_t^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{v}_t\\right).\n",
    "$$\n",
    "\n",
    "Taking logarithms gives\n",
    "\n",
    "$$\n",
    "\\ell_t(\\theta) = -\\log(2\\pi) - \\tfrac{1}{2}\\log|\\boldsymbol{\\Sigma}| - \\tfrac{1}{2}\\,\\mathbf{v}_t^\\top \\boldsymbol{\\Sigma}^{-1}\\,\\mathbf{v}_t.\n",
    "$$\n",
    "\n",
    "Summing over $ T $ observations and dropping the constant $ -T\\log(2\\pi) $ yields the log-likelihood function\n",
    "\n",
    "\n",
    "<a id='equation-hs83-loglik'></a>\n",
    "$$\n",
    "L(\\theta) = -\\frac{T}{2}\\log|\\boldsymbol{\\Sigma}| - \\frac{1}{2}\\sum_{t=1}^{T}(\\mathbf{A}_0 \\mathbf{Y}_t - \\mathbf{A}_1(L)\\mathbf{Y}_{t-1} - \\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{A}_0 \\mathbf{Y}_t - \\mathbf{A}_1(L)\\mathbf{Y}_{t-1} - \\boldsymbol{\\mu}), \\tag{85.14}\n",
    "$$\n",
    "\n",
    "where $ \\boldsymbol{\\Sigma} $ is the covariance matrix of the innovation $ \\mathbf{V}_t $ and $ \\theta $ collects all free parameters including $ \\alpha $, $ \\beta $, the covariance parameters, the first-row intercept $ \\mu_x $, and the first-row lag coefficients.\n",
    "\n",
    "The restrictions imposed by [(85.6)](#equation-hs83-euler) enter [(85.14)](#equation-hs83-loglik) through the structure of $ \\mathbf{A}_0 $, $ \\mathbf{A}_1(L) $, and $ \\boldsymbol{\\mu} $ in [(85.13)](#equation-hs83-a0a1).\n",
    "\n",
    "The second row of [(85.12)](#equation-hs83-restricted) has no free lag coefficients and its intercept is determined by $ \\alpha $, $ \\beta $, and $ \\boldsymbol{\\Sigma} $.\n",
    "\n",
    "An alternative unrestricted bivariate VAR($ p $) would estimate both rows of [(85.12)](#equation-hs83-restricted) freely.\n",
    "\n",
    "Each row has 1 intercept plus $ 2p $ lag coefficients ($ p $ lags $ \\times $ 2 variables), giving $ 2(1 + 2p) $ mean parameters.\n",
    "\n",
    "Adding 3 free covariance parameters ($ \\sigma_{XX}, \\sigma_{RR}, \\sigma_{XR} $) gives a total of $ 5 + 4p $.\n",
    "\n",
    "The restricted system [(85.12)](#equation-hs83-restricted) has only $ 6 + 2p $ free parameters: the first row contributes $ 1 + 2p $ (its intercept $ \\mu_x $ and $ 2p $ lag coefficients), plus $ \\alpha $, $ \\beta $, and the 3 covariance parameters.\n",
    "\n",
    "The second row adds nothing because its lag structure and intercept are pinned down by $ \\alpha $, $ \\beta $, and $ \\boldsymbol{\\Sigma} $ via [(85.10)](#equation-hs83-cond-mean).\n",
    "\n",
    "The difference $ (\\smash{5 + 4p}) - (\\smash{6 + 2p}) = 2p - 1 $ gives the degrees of freedom for the likelihood ratio tests we will perform soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea5080",
   "metadata": {},
   "source": [
    "## Likelihood implementation\n",
    "\n",
    "Now let’s implement the likelihood [(85.14)](#equation-hs83-loglik).\n",
    "\n",
    "The building blocks are:\n",
    "\n",
    "- a function to construct lagged data matrices $ (\\mathbf{Y}_t, \\mathbf{Y}_{t-1}, \\ldots, \\mathbf{Y}_{t-p}) $,  \n",
    "- a function to map the parameter vector into the matrices $ \\mathbf{A}_0 $, $ \\mathbf{A}_1 $, $ \\boldsymbol{\\mu} $, $ \\boldsymbol{\\Sigma} $,  \n",
    "- a function to compute the restricted-system residuals $ \\mathbf{V}_t = \\mathbf{A}_0 \\mathbf{Y}_t - \\mathbf{A}_1(L) \\mathbf{Y}_{t-1} - \\boldsymbol{\\mu} $, and  \n",
    "- the Gaussian log-likelihood itself.  \n",
    "\n",
    "\n",
    "Since we are working with log-transformed data, we define a helper for the transformation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd9ec1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def to_mle_array(data):\n",
    "    valid = (data[:, 0] > 0.0) & (data[:, 1] > 0.0)\n",
    "    return np.column_stack(\n",
    "        [np.log(data[valid, 1]), np.log(data[valid, 0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f7862",
   "metadata": {},
   "source": [
    "First we build the lagged data matrices, which are the inputs to the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de4d0c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_lagged_data(data, n_lags):\n",
    "    \"\"\"\n",
    "    Build Y_t and lag stacks [Y_{t-1}, ..., Y_{t-p}] for bivariate data.\n",
    "    \"\"\"\n",
    "    if data.ndim != 2 or data.shape[1] != 2:\n",
    "        raise ValueError(\"data must be T x 2.\")\n",
    "    if data.shape[0] <= n_lags:\n",
    "        raise ValueError(\"Sample size must exceed n_lags.\")\n",
    "\n",
    "    t_obs = data.shape[0]\n",
    "    n_obs = t_obs - n_lags\n",
    "    y_t = data[n_lags:, :]\n",
    "    y_lag = np.empty((n_obs, 2 * n_lags))\n",
    "\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        y_lag[:, 2 * (lag - 1) : 2 * lag] = data[n_lags - lag : t_obs - lag, :]\n",
    "\n",
    "    return y_t, y_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53e455",
   "metadata": {},
   "source": [
    "Next, we validate and unpack the parameter vector while enforcing feasibility conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56f1cd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def unpack_parameters(params, n_lags):\n",
    "    \"\"\"\n",
    "    Validate and unpack parameter vector.\n",
    "    \"\"\"\n",
    "    if len(params) != 6 + 2 * n_lags:\n",
    "        return None\n",
    "\n",
    "    α, β, σ_x, σ_r, cov_xr, μ_x = params[:6]\n",
    "    a_lags = params[6:]\n",
    "\n",
    "    tol = 1e-8\n",
    "    if not np.isfinite(α):\n",
    "        return None\n",
    "    if not (tol < β):\n",
    "        return None\n",
    "    if not (σ_x > tol and σ_r > tol):\n",
    "        return None\n",
    "\n",
    "    Σ = np.array(\n",
    "        [\n",
    "            [σ_x ** 2, cov_xr],\n",
    "            [cov_xr, σ_r ** 2],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cholesky(Σ, lower=True)\n",
    "    except (LinAlgError, ValueError):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"α\": np.array(α),\n",
    "        \"β\": np.array(β),\n",
    "        \"σ_x\": np.array(σ_x),\n",
    "        \"σ_r\": np.array(σ_r),\n",
    "        \"cov_xr\": np.array(cov_xr),\n",
    "        \"μ_x\": np.array(μ_x),\n",
    "        \"a_lags\": a_lags,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd4ec1",
   "metadata": {},
   "source": [
    "The next step maps parameters and lagged data into restricted-system residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba10c02",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def restricted_residuals(\n",
    "    params,\n",
    "    y_t,\n",
    "    y_lag,\n",
    "    n_lags,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute V_t implied by A0 Y_t - A1 Y_{t-1} - mu.\n",
    "    \"\"\"\n",
    "    parsed = unpack_parameters(params, n_lags)\n",
    "    if parsed is None:\n",
    "        return None\n",
    "\n",
    "    α = float(parsed[\"α\"])\n",
    "    β = float(parsed[\"β\"])\n",
    "    σ_x = float(parsed[\"σ_x\"])\n",
    "    σ_r = float(parsed[\"σ_r\"])\n",
    "    cov_xr = float(parsed[\"cov_xr\"])\n",
    "    μ_x = float(parsed[\"μ_x\"])\n",
    "    a_lags = np.asarray(parsed[\"a_lags\"])\n",
    "\n",
    "    A0 = np.array([[1.0, 0.0], [α, 1.0]])\n",
    "    A1 = np.zeros((2, 2 * n_lags))\n",
    "    A1[0, :] = a_lags\n",
    "    σ_u2 = α ** 2 * σ_x ** 2 + σ_r ** 2 + 2.0 * α * cov_xr\n",
    "    μ = np.array([μ_x, -np.log(β) - 0.5 * σ_u2])\n",
    "\n",
    "    resid = y_t @ A0.T - y_lag @ A1.T - μ[None, :]\n",
    "    if np.any(np.abs(resid) > 1e10):\n",
    "        return None\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50855ffc",
   "metadata": {},
   "source": [
    "The recursive structure also lets us simulate data from the model.\n",
    "\n",
    "We can generate data from the model by drawing innovations $ \\mathbf{V}_t \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma}) $ and compute $ \\mathbf{Y}_t = \\mathbf{A}_0^{-1}(\\mathbf{A}_1(L) \\mathbf{Y}_{t-1} + \\boldsymbol{\\mu} + \\mathbf{V}_t) $ forward in time.\n",
    "\n",
    "This is useful for checking the likelihood implementation through Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b3a97",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_restricted_var(\n",
    "    params,\n",
    "    n_obs,\n",
    "    n_lags,\n",
    "    burn_in=200,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate [log consumption growth, log return] from the restricted model.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if len(params) != 6 + 2 * n_lags:\n",
    "        raise ValueError(\"Parameter vector length must be 6 + 2 * n_lags.\")\n",
    "\n",
    "    α, β, σ_x, σ_r, cov_xr, μ_x = params[:6]\n",
    "    a_lags = params[6:]\n",
    "\n",
    "    Σ_e = np.array(\n",
    "        [\n",
    "            [σ_x ** 2, cov_xr],\n",
    "            [cov_xr, σ_r ** 2],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    A0 = np.array([[1.0, 0.0], [α, 1.0]])\n",
    "    Σ_v = A0 @ Σ_e @ A0.T\n",
    "\n",
    "    eigvals = np.linalg.eigvals(Σ_v)\n",
    "    if np.min(eigvals) <= 0.0:\n",
    "        Σ_v += np.eye(2) * 1e-6\n",
    "\n",
    "    A1 = np.zeros((2, 2 * n_lags))\n",
    "    A1[0, :] = a_lags\n",
    "    σ_u2 = α ** 2 * σ_x ** 2 + σ_r ** 2 + 2.0 * α * cov_xr\n",
    "    μ = np.array([μ_x, -np.log(β) - 0.5 * σ_u2])\n",
    "\n",
    "    total_n = n_obs + burn_in\n",
    "    y = np.zeros((total_n, 2))\n",
    "\n",
    "    for t in range(n_lags, total_n):\n",
    "        lag_stack = []\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            lag_stack.append(y[t - lag, :])\n",
    "        lag_vec = np.concatenate(lag_stack)\n",
    "        shock = np.random.multivariate_normal(np.zeros(2), Σ_v)\n",
    "        y[t, :] = np.linalg.solve(A0, A1 @ lag_vec + μ + shock)\n",
    "\n",
    "    return y[burn_in:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7127352",
   "metadata": {},
   "source": [
    "Next, we encode the Gaussian log-likelihood implied by the residual covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2309c46",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood_mle(\n",
    "    params,\n",
    "    y_t,\n",
    "    y_lag,\n",
    "    n_lags,\n",
    "    include_const=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate Gaussian log-likelihood for the restricted system.\n",
    "    \"\"\"\n",
    "    parsed = unpack_parameters(params, n_lags)\n",
    "    if parsed is None:\n",
    "        return -np.inf\n",
    "\n",
    "    resid = restricted_residuals(params, y_t, y_lag, n_lags)\n",
    "    if resid is None:\n",
    "        return -np.inf\n",
    "\n",
    "    α = float(parsed[\"α\"])\n",
    "    σ_x = float(parsed[\"σ_x\"])\n",
    "    σ_r = float(parsed[\"σ_r\"])\n",
    "    cov_xr = float(parsed[\"cov_xr\"])\n",
    "\n",
    "    Σ_e = np.array(\n",
    "        [\n",
    "            [σ_x ** 2, cov_xr],\n",
    "            [cov_xr, σ_r ** 2],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    A0 = np.array([[1.0, 0.0], [α, 1.0]])\n",
    "    Σ_v = A0 @ Σ_e @ A0.T\n",
    "\n",
    "    try:\n",
    "        chol = cholesky(Σ_v, lower=True)\n",
    "        log_det = 2.0 * np.sum(np.log(np.diag(chol) + 1e-16))\n",
    "        std_resid = solve_triangular(chol, resid.T, lower=True).T\n",
    "        quad_form = np.sum(std_resid ** 2)\n",
    "    except (LinAlgError, ValueError):\n",
    "        return -np.inf\n",
    "\n",
    "    sample_size = y_t.shape[0]\n",
    "    ll = -0.5 * sample_size * log_det - 0.5 * quad_form\n",
    "    if include_const:\n",
    "        ll -= sample_size * np.log(2.0 * np.pi)\n",
    "\n",
    "    if np.isnan(ll) or np.isinf(ll):\n",
    "        return -np.inf\n",
    "    return float(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7249d61",
   "metadata": {},
   "source": [
    "To pass this into a numerical optimizer, we wrap the log-likelihood as a minimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8aaa58",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def negative_log_likelihood(\n",
    "    params,\n",
    "    y_t,\n",
    "    y_lag,\n",
    "    n_lags,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return negative log-likelihood for minimization.\n",
    "    \"\"\"\n",
    "    ll = log_likelihood_mle(params, y_t, y_lag, n_lags, include_const=False)\n",
    "    if np.isfinite(ll):\n",
    "        return -ll\n",
    "    return 1e20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1122e",
   "metadata": {},
   "source": [
    "We set parameter bounds and generate data-driven starting values for multi-start optimization.\n",
    "\n",
    "We keep $ \\beta $ positive but do not force $ \\beta < 1 $ in estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749716c9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def parameter_bounds(n_lags):\n",
    "    \"\"\"\n",
    "    Bounds for optimization.\n",
    "    \"\"\"\n",
    "    bounds = [\n",
    "        (-200.0, 200.0),   # α (= -risk aversion)\n",
    "        (1e-8, 2.0),     # β (discount factor)\n",
    "        (1e-8, None),    # σ_x (std dev of consumption innovation)\n",
    "        (1e-8, None),    # σ_r (std dev of return innovation)\n",
    "        (None, None),    # cov_xr (covariance)\n",
    "        (None, None),    # μ_x (consumption growth intercept)\n",
    "    ]\n",
    "    bounds += [(-0.99, 0.99)] * (2 * n_lags)  # VAR lag coefficients\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c4ce1",
   "metadata": {},
   "source": [
    "We use multiple starting vectors to help local solvers escape poor initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ace94f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def starting_values(y_t, y_lag, n_lags, n_starts=10):\n",
    "    \"\"\"\n",
    "    Generate multiple starting values.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(123)\n",
    "    starts = []\n",
    "    n_params = 6 + 2 * n_lags\n",
    "\n",
    "    base = np.zeros(n_params)\n",
    "    base[0] = -0.5\n",
    "    base[1] = 0.999\n",
    "    base[2] = max(float(np.std(y_t[:, 0])), 1e-3)\n",
    "    base[3] = max(float(np.std(y_t[:, 1])), 1e-3)\n",
    "    base[4] = float(np.cov(y_t.T)[0, 1])\n",
    "    base[5] = float(np.mean(y_t[:, 0]))\n",
    "    base[6:] = 0.1\n",
    "    starts.append(base.copy())\n",
    "\n",
    "    # OLS seed from unrestricted VAR\n",
    "    n_obs = y_t.shape[0]\n",
    "    x = np.column_stack([np.ones(n_obs), y_lag])\n",
    "    coef = np.linalg.lstsq(x, y_t, rcond=None)[0]\n",
    "    resid = y_t - x @ coef\n",
    "    Σ_e = resid.T @ resid / max(1, n_obs)\n",
    "\n",
    "    a_lags_ols = coef[1:, 0]\n",
    "    r_lags_ols = coef[1:, 1]\n",
    "    denom = float(a_lags_ols @ a_lags_ols)\n",
    "    if denom > 1e-10:\n",
    "        α_ols = -float((a_lags_ols @ r_lags_ols) / denom)\n",
    "    else:\n",
    "        α_ols = -0.5\n",
    "\n",
    "    μ_x_ols = float(coef[0, 0])\n",
    "    μ_r_ols = float(coef[0, 1])\n",
    "    σ_x_ols = float(np.sqrt(max(Σ_e[0, 0], 1e-8)))\n",
    "    σ_r_ols = float(np.sqrt(max(Σ_e[1, 1], 1e-8)))\n",
    "    cov_xr_ols = float(Σ_e[0, 1])\n",
    "    σ_u2_ols = (\n",
    "        α_ols ** 2 * σ_x_ols ** 2\n",
    "        + σ_r_ols ** 2\n",
    "        + 2.0 * α_ols * cov_xr_ols\n",
    "    )\n",
    "    β_ols = float(np.exp(-(μ_r_ols + α_ols * μ_x_ols + 0.5 * σ_u2_ols)))\n",
    "    β_ols = float(np.clip(β_ols, 1e-6, 2.0))\n",
    "\n",
    "    ols_seed = np.zeros(n_params)\n",
    "    ols_seed[0] = α_ols\n",
    "    ols_seed[1] = β_ols\n",
    "    ols_seed[2] = σ_x_ols\n",
    "    ols_seed[3] = σ_r_ols\n",
    "    ols_seed[4] = cov_xr_ols\n",
    "    ols_seed[5] = μ_x_ols\n",
    "    ols_seed[6:] = a_lags_ols\n",
    "    starts.append(ols_seed.copy())\n",
    "\n",
    "    seeds = [base, ols_seed]\n",
    "    while len(starts) < n_starts:\n",
    "        seed = seeds[len(starts) % len(seeds)]\n",
    "        trial = seed.copy()\n",
    "        trial[:2] += rng.normal(0.0, 0.2, 2)\n",
    "        trial[2:6] *= 1.0 + rng.normal(0.0, 0.15, 4)\n",
    "        trial[6:] += rng.normal(0.0, 0.08, 2 * n_lags)\n",
    "        trial[1] = max(trial[1], 1e-6)\n",
    "        trial[2] = max(trial[2], 1e-6)\n",
    "        trial[3] = max(trial[3], 1e-6)\n",
    "        starts.append(trial)\n",
    "\n",
    "    return starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ca20f",
   "metadata": {},
   "source": [
    "Standard errors come from an outer-product-of-gradients (OPG) approximation to the\n",
    "information matrix, computed by finite differences of per-observation\n",
    "log-likelihood contributions.\n",
    "\n",
    "This tends to be more numerically stable than finite-difference Hessians in this\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1b1b6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood_contributions(\n",
    "    params,\n",
    "    y_t,\n",
    "    y_lag,\n",
    "    n_lags,\n",
    "    include_const=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Vector of per-observation Gaussian log-likelihood contributions.\n",
    "\n",
    "    Returns an array of length T, or None if the parameter vector is infeasible.\n",
    "    \"\"\"\n",
    "    parsed = unpack_parameters(params, n_lags)\n",
    "    if parsed is None:\n",
    "        return None\n",
    "\n",
    "    resid = restricted_residuals(params, y_t, y_lag, n_lags)\n",
    "    if resid is None:\n",
    "        return None\n",
    "\n",
    "    α = float(parsed[\"α\"])\n",
    "    σ_x = float(parsed[\"σ_x\"])\n",
    "    σ_r = float(parsed[\"σ_r\"])\n",
    "    cov_xr = float(parsed[\"cov_xr\"])\n",
    "\n",
    "    Σ_e = np.array(\n",
    "        [\n",
    "            [σ_x ** 2, cov_xr],\n",
    "            [cov_xr, σ_r ** 2],\n",
    "        ]\n",
    "    )\n",
    "    A0 = np.array([[1.0, 0.0], [α, 1.0]])\n",
    "    Σ_v = A0 @ Σ_e @ A0.T\n",
    "\n",
    "    try:\n",
    "        chol = cholesky(Σ_v, lower=True)\n",
    "        log_det = 2.0 * np.sum(np.log(np.diag(chol) + 1e-16))\n",
    "        std_resid = solve_triangular(chol, resid.T, lower=True).T\n",
    "    except (LinAlgError, ValueError):\n",
    "        return None\n",
    "\n",
    "    quad = np.sum(std_resid ** 2, axis=1)\n",
    "    ll_t = -0.5 * log_det - 0.5 * quad\n",
    "    if include_const:\n",
    "        ll_t -= np.log(2.0 * np.pi)\n",
    "    if not np.all(np.isfinite(ll_t)):\n",
    "        return None\n",
    "    return ll_t\n",
    "\n",
    "\n",
    "def opg_standard_errors(\n",
    "    params,\n",
    "    y_t,\n",
    "    y_lag,\n",
    "    n_lags,\n",
    "    step=1e-6,\n",
    "    max_step_shrink=12,\n",
    "    eig_floor=1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Standard errors via OPG approximation to the information matrix.\n",
    "    \"\"\"\n",
    "    n = len(params)\n",
    "    ll0 = log_likelihood_contributions(params, y_t, y_lag, n_lags, include_const=False)\n",
    "    if ll0 is None:\n",
    "        return np.full(n, np.nan)\n",
    "\n",
    "    n_obs = int(ll0.shape[0])\n",
    "    scores = np.empty((n_obs, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        base_step = step * (abs(params[i]) + 1.0)\n",
    "        hi = base_step\n",
    "        ll_plus = None\n",
    "        ll_minus = None\n",
    "\n",
    "        for _ in range(max_step_shrink + 1):\n",
    "            p_plus = params.copy()\n",
    "            p_minus = params.copy()\n",
    "            p_plus[i] += hi\n",
    "            p_minus[i] -= hi\n",
    "            ll_plus = log_likelihood_contributions(\n",
    "                p_plus, y_t, y_lag, n_lags, include_const=False\n",
    "            )\n",
    "            ll_minus = log_likelihood_contributions(\n",
    "                p_minus, y_t, y_lag, n_lags, include_const=False\n",
    "            )\n",
    "            if ll_plus is not None and ll_minus is not None:\n",
    "                break\n",
    "            hi *= 0.5\n",
    "\n",
    "        if ll_plus is None or ll_minus is None:\n",
    "            return np.full(n, np.nan)\n",
    "\n",
    "        scores[:, i] = (ll_plus - ll_minus) / (2.0 * hi)\n",
    "\n",
    "    if not np.all(np.isfinite(scores)):\n",
    "        return np.full(n, np.nan)\n",
    "\n",
    "    # Center scores to mitigate numerical drift away\n",
    "    scores = scores - scores.mean(axis=0, keepdims=True)\n",
    "\n",
    "    opg = scores.T @ scores\n",
    "    if not np.all(np.isfinite(opg)):\n",
    "        return np.full(n, np.nan)\n",
    "    opg = 0.5 * (opg + opg.T)\n",
    "\n",
    "    try:\n",
    "        eigvals, eigvecs = np.linalg.eigh(opg)\n",
    "    except (LinAlgError, ValueError):\n",
    "        return np.full(n, np.nan)\n",
    "\n",
    "    floor = float(eig_floor) * max(1.0, float(np.max(eigvals)))\n",
    "    eigvals = np.clip(eigvals, floor, None)\n",
    "    cov = eigvecs @ np.diag(1.0 / eigvals) @ eigvecs.T\n",
    "    se = np.sqrt(np.maximum(np.diag(cov), 0.0))\n",
    "    se[~np.isfinite(se)] = np.nan\n",
    "    return se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075699d",
   "metadata": {},
   "source": [
    "The multi-start MLE estimator below combines these pieces and returns parameters, fit criteria, and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee346c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def estimate_mle(data, n_lags, verbose=False):\n",
    "    \"\"\"\n",
    "    Estimate the restricted model by multi-start local optimization.\n",
    "    \"\"\"\n",
    "    y_t, y_lag = build_lagged_data(data, n_lags)\n",
    "    bounds = parameter_bounds(n_lags)\n",
    "    starts = starting_values(y_t, y_lag, n_lags)\n",
    "\n",
    "    best_result = None\n",
    "    best_ll = -np.inf\n",
    "\n",
    "    for i, x0 in enumerate(starts):\n",
    "        try:\n",
    "            result = minimize(\n",
    "                negative_log_likelihood,\n",
    "                x0=x0,\n",
    "                args=(y_t, y_lag, n_lags),\n",
    "                method=\"L-BFGS-B\",\n",
    "                bounds=bounds,\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if np.isfinite(result.fun):\n",
    "            ll_val = log_likelihood_mle(result.x, y_t, y_lag, n_lags)\n",
    "            if np.isfinite(ll_val) and ll_val > best_ll:\n",
    "                best_ll = ll_val\n",
    "                best_result = result\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"start={i}, success={result.success}, \"\n",
    "                        f\"status={result.status}, loglike={ll_val:.2f}\"\n",
    "                    )\n",
    "\n",
    "    n_params = 6 + 2 * n_lags\n",
    "\n",
    "    if best_result is None:\n",
    "        return {\n",
    "            \"params\": np.full(n_params, np.nan),\n",
    "            \"se\": np.full(n_params, np.nan),\n",
    "            \"loglike\": -np.inf,\n",
    "            \"converged\": False,\n",
    "            \"optimizer_success\": False,\n",
    "            \"residuals\": None,\n",
    "            \"n_obs\": int(y_t.shape[0]),\n",
    "        }\n",
    "\n",
    "    params = best_result.x\n",
    "    se = opg_standard_errors(params, y_t, y_lag, n_lags)\n",
    "    resid = restricted_residuals(params, y_t, y_lag, n_lags)\n",
    "    ll_val = log_likelihood_mle(params, y_t, y_lag, n_lags)\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"se\": se,\n",
    "        \"loglike\": ll_val,\n",
    "        \"converged\": bool(np.isfinite(ll_val)),\n",
    "        \"optimizer_success\": bool(best_result.success),\n",
    "        \"residuals\": resid,\n",
    "        \"n_obs\": int(y_t.shape[0]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49e7d7",
   "metadata": {},
   "source": [
    "Residual diagnostics below summarize normality and serial-correlation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737026f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def residual_diagnostics(resid):\n",
    "    \"\"\"\n",
    "    Compute basic residual diagnostics.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    for i, label in enumerate([\"consumption\", \"return\"]):\n",
    "        jb_stat, jb_pval = stats.jarque_bera(resid[:, i])\n",
    "        out[f\"{label}_jb_stat\"] = float(jb_stat)\n",
    "        out[f\"{label}_jb_pval\"] = float(jb_pval)\n",
    "        out[f\"{label}_dw\"] = float(durbin_watson(resid[:, i]))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60a5d9",
   "metadata": {},
   "source": [
    "Finally, a lag-loop wrapper runs MLE across different lag lengths and collects results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3b9f9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def run_mle_by_lag(\n",
    "    data,\n",
    "    lags=(2, 4, 6),\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate restricted MLE models by lag length.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    fits = {}\n",
    "\n",
    "    for lag in lags:\n",
    "        fit = estimate_mle(data, n_lags=lag, verbose=verbose)\n",
    "        fits[lag] = fit\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"NLAG\": lag,\n",
    "                \"α_hat\": fit[\"params\"][0],\n",
    "                \"se_α\": fit[\"se\"][0],\n",
    "                \"β_hat\": fit[\"params\"][1],\n",
    "                \"se_β\": fit[\"se\"][1],\n",
    "                \"loglike\": fit[\"loglike\"],\n",
    "                \"n_obs\": fit[\"n_obs\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    table = pd.DataFrame(rows).set_index(\"NLAG\")\n",
    "    return table, fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de126988",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Before applying the likelihood to real data, we verify that it recovers known parameters from simulated data generated by the restricted system.\n",
    "\n",
    "For `n_lags = 1`, the parameter vector is\n",
    "\n",
    "$$\n",
    "\\theta = (\\alpha,\\ \\beta,\\ \\sigma_x,\\ \\sigma_r,\\ \\sigma_{xr},\\ \\mu_x,\\ a_{x,1},\\ a_{r,1}),\n",
    "$$\n",
    "\n",
    "where the last two entries are the coefficients on $ X_{t-1} $ and $ R_{t-1} $ in the first-row regression for $ X_t $.\n",
    "\n",
    "More generally, for `n_lags = p` we pack `a_lags` in the order\n",
    "$ [a_{x,1}, a_{r,1}, \\ldots, a_{x,p}, a_{r,p}] $, so the full parameter vector has length $ 6 + 2p $.\n",
    "\n",
    "The covariance parameters $ (\\sigma_x, \\sigma_r, \\sigma_{xr}) $ describe the reduced-form shocks to $ (X_t, R_t) $: if $ \\boldsymbol{\\varepsilon}_t = (\\varepsilon_{x,t}, \\varepsilon_{r,t})^\\top \\sim N(0, \\boldsymbol{\\Sigma}_\\varepsilon) $, then\n",
    "$ \\boldsymbol{\\Sigma}_\\varepsilon = \\begin{bmatrix}\\sigma_x^2 & \\sigma_{xr}\\\\ \\sigma_{xr} & \\sigma_r^2\\end{bmatrix} $.\n",
    "\n",
    "The restricted-system innovation is $ \\mathbf{V}_t = \\mathbf{A}_0 \\boldsymbol{\\varepsilon}_t $, so its covariance is $ \\boldsymbol{\\Sigma}_V = \\mathbf{A}_0 \\boldsymbol{\\Sigma}_\\varepsilon \\mathbf{A}_0^\\top $, which is what enters the likelihood.\n",
    "\n",
    "In the simulation recursion we draw $ \\mathbf{V}_t \\sim N(0, \\boldsymbol{\\Sigma}_V) $ and compute $ \\mathbf{Y}_t = \\mathbf{A}_0^{-1}(\\mathbf{A}_1(L)\\mathbf{Y}_{t-1} + \\boldsymbol{\\mu} + \\mathbf{V}_t) $ forward.\n",
    "\n",
    "The following table compares the true parameters to the MLE estimates from a large simulated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f71909",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α_true = -1.00\n",
    "β_true = 0.993\n",
    "σ_x_true = 0.015\n",
    "σ_r_true = 0.020\n",
    "cov_xr_true = 0.0001\n",
    "μ_x_true = 0.002\n",
    "a_x1_true = 0.40\n",
    "a_r1_true = 0.10\n",
    "\n",
    "true_params = np.array(\n",
    "    [\n",
    "        α_true,\n",
    "        β_true,\n",
    "        σ_x_true,\n",
    "        σ_r_true,\n",
    "        cov_xr_true,\n",
    "        μ_x_true,\n",
    "        a_x1_true,\n",
    "        a_r1_true,\n",
    "    ]\n",
    ")\n",
    "\n",
    "sim_mle_data = simulate_restricted_var(\n",
    "    params=true_params,\n",
    "    n_obs=50000,\n",
    "    n_lags=1,\n",
    "    burn_in=5000,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "fit_sim = estimate_mle(sim_mle_data, n_lags=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade09462",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sim_results = pd.DataFrame({\n",
    "    \"true\": true_params[:2],\n",
    "    \"estimate\": fit_sim[\"params\"][:2],\n",
    "    \"se\": fit_sim[\"se\"][:2],\n",
    "}, index=[r\"α\", r\"β\"])\n",
    "sim_results[r\"t\\ (H_0{:}\\ \\text{true})\"] = (\n",
    "    (sim_results[\"estimate\"] - sim_results[\"true\"]) / sim_results[\"se\"]\n",
    ")\n",
    "display_table(sim_results, fmt={\n",
    "    \"true\": \"{:.4f}\", \"estimate\": \"{:.4f}\", \"se\": \"{:.6f}\", r\"t\\ (H_0{:}\\ \\text{true})\": \"{:.2f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c28dad",
   "metadata": {},
   "source": [
    "The point estimates are close to the true parameters, and the t-statistics for the null hypothesis that the true value is correct are small in magnitude, consistent with sampling variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d88ce2",
   "metadata": {},
   "source": [
    "## Preference parameters and likelihood ratio tests\n",
    "\n",
    "The restricted system embeds preference parameters through cross-equation restrictions.\n",
    "\n",
    "The parameter $ \\alpha $ links predictable variation in returns to predictable variation in consumption growth.\n",
    "\n",
    "Under the model, the return equation’s dependence on lagged variables is entirely determined by $ -\\alpha $ times the consumption equation’s lag coefficients.\n",
    "\n",
    "The parameter $ \\beta $ shifts the return-equation intercept through $ -\\log\\beta - \\sigma_U^2/2 $.\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] test these restrictions by comparing the restricted system to an unrestricted bivariate VAR estimated on the same sample.\n",
    "\n",
    "If the restrictions imposed by the Euler-equation are correct, the restricted model should fit nearly as well as the unrestricted model.\n",
    "\n",
    "The standard test is the likelihood ratio statistic\n",
    "\n",
    "\n",
    "<a id='equation-hs83-lr-test'></a>\n",
    "$$\n",
    "LR = 2(\\ell_u - \\ell_r) \\Rightarrow \\chi^2_d, \\tag{85.15}\n",
    "$$\n",
    "\n",
    "where $ \\ell_u $ and $ \\ell_r $ are the maximized log-likelihoods of the unrestricted and restricted models, and $ d $ is the difference in the number of free parameters.\n",
    "\n",
    "Both likelihoods must be evaluated on the same effective sample for the LR distribution to be valid.\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] report that for the value-weighted aggregate stock return, the $ \\chi^2 $ test statistics provide little evidence against the model.\n",
    "\n",
    "In the tables below we report both `chi2.cdf(LR, df)` (corresponding to what HS report in parentheses) and the usual right-tail `p(LR) = 1 - chi2.cdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f460bf2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def likelihood_ratio_test(\n",
    "    fit_restricted,\n",
    "    fit_unrestricted,\n",
    "    df_diff,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare nested specifications with an LR test.\n",
    "    \"\"\"\n",
    "    if not (fit_restricted[\"converged\"] and fit_unrestricted[\"converged\"]):\n",
    "        return {\"lr_stat\": np.nan, \"p_value\": np.nan, \"chi2_cdf\": np.nan}\n",
    "    if fit_restricted.get(\"n_obs\") != fit_unrestricted.get(\"n_obs\"):\n",
    "        return {\"lr_stat\": np.nan, \"p_value\": np.nan, \"chi2_cdf\": np.nan}\n",
    "\n",
    "    lr_stat = 2.0 * (fit_unrestricted[\"loglike\"] - fit_restricted[\"loglike\"])\n",
    "    chi2_cdf = stats.chi2.cdf(lr_stat, df=df_diff)\n",
    "    p_value = 1.0 - chi2_cdf\n",
    "    return {\n",
    "        \"lr_stat\": float(lr_stat),\n",
    "        \"p_value\": float(p_value),\n",
    "        \"chi2_cdf\": float(chi2_cdf),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678eb3a",
   "metadata": {},
   "source": [
    "The unrestricted benchmark is a Gaussian VAR for $ \\mathbf{Y}_t = (X_t, R_t)^\\top $ with free coefficients on all lags in both equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a216e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def estimate_unrestricted_var(data, n_lags):\n",
    "    \"\"\"\n",
    "    Estimate an unrestricted Gaussian VAR for Y_t = [X_t, R_t].\n",
    "    \"\"\"\n",
    "    y_t, y_lag = build_lagged_data(data, n_lags)\n",
    "    n_obs = y_t.shape[0]\n",
    "    x = np.column_stack([np.ones(n_obs), y_lag])\n",
    "    coef = np.linalg.lstsq(x, y_t, rcond=None)[0]\n",
    "    resid = y_t - x @ coef\n",
    "    Σ = resid.T @ resid / n_obs\n",
    "\n",
    "    try:\n",
    "        chol = cholesky(Σ, lower=True)\n",
    "        log_det = 2.0 * np.sum(np.log(np.diag(chol) + 1e-16))\n",
    "        std_resid = solve_triangular(chol, resid.T, lower=True).T\n",
    "        quad_form = np.sum(std_resid ** 2)\n",
    "    except (LinAlgError, ValueError):\n",
    "        return {\n",
    "            \"coef\": coef,\n",
    "            \"Σ\": np.full((2, 2), np.nan),\n",
    "            \"residuals\": resid,\n",
    "            \"loglike\": -np.inf,\n",
    "            \"converged\": False,\n",
    "            \"n_obs\": int(n_obs),\n",
    "        }\n",
    "\n",
    "    d = y_t.shape[1]\n",
    "    loglike = float(-0.5 * n_obs * d * np.log(2.0 * np.pi) \n",
    "            - 0.5 * n_obs * log_det - 0.5 * quad_form)\n",
    "\n",
    "    return {\n",
    "        \"coef\": coef,\n",
    "        \"Σ\": Σ,\n",
    "        \"residuals\": resid,\n",
    "        \"loglike\": loglike,\n",
    "        \"converged\": True,\n",
    "        \"n_obs\": int(n_obs),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c79f5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def run_unrestricted_var_by_lag(data, lags=(2, 4, 6)):\n",
    "    \"\"\"\n",
    "    Estimate unrestricted VAR models by lag length.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    fits = {}\n",
    "\n",
    "    for lag in lags:\n",
    "        fit = estimate_unrestricted_var(data, n_lags=lag)\n",
    "        fits[lag] = fit\n",
    "        rows.append(\n",
    "            {\n",
    "                \"NLAG\": lag,\n",
    "                \"loglike\": fit[\"loglike\"],\n",
    "                \"n_obs\": fit[\"n_obs\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    table = pd.DataFrame(rows).set_index(\"NLAG\")\n",
    "    return table, fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df5877",
   "metadata": {},
   "source": [
    "The following function computes the LR statistic at each lag length, replicating the testing strategy of Table 1 in Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafa241",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def restricted_vs_unrestricted_lr(mle_fits, unrestricted_fits, lags=(2, 4, 6)):\n",
    "    \"\"\"\n",
    "    Compute LR tests of restricted model versus unrestricted VAR.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for lag in lags:\n",
    "        fit_r = mle_fits[lag]\n",
    "        fit_u = unrestricted_fits[lag]\n",
    "        df_diff = (2 * (1 + 2 * lag) + 3) - (6 + 2 * lag)\n",
    "        lr = likelihood_ratio_test(fit_restricted=fit_r, fit_unrestricted=fit_u, df_diff=df_diff)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"NLAG\": lag,\n",
    "                \"lr_stat\": lr[\"lr_stat\"],\n",
    "                \"p_value\": lr[\"p_value\"],\n",
    "                \"chi2_cdf\": lr[\"chi2_cdf\"],\n",
    "                \"df\": df_diff,\n",
    "                \"T\": fit_r.get(\"n_obs\", np.nan),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(\"NLAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff77072",
   "metadata": {},
   "source": [
    "## Predictability and the R-squared restriction\n",
    "\n",
    "Section II of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] emphasizes an implication of the restricted system for return predictability.\n",
    "\n",
    "From [(85.10)](#equation-hs83-cond-mean) and [(85.11)](#equation-hs83-x-forecast), the predictable component of the log return is\n",
    "\n",
    "\n",
    "<a id='equation-hs83-predictable-return'></a>\n",
    "$$\n",
    "E(R_t \\mid \\psi_{t-1}) = -\\alpha\\, E(X_t \\mid \\psi_{t-1}) - \\log\\beta - \\frac{\\sigma_U^2}{2}. \\tag{85.16}\n",
    "$$\n",
    "\n",
    "Since the predictable return is a linear function of the predictable consumption growth, the variance of the predictable return component is exactly $ \\alpha^2 $ times the variance of the predictable consumption-growth component:\n",
    "\n",
    "\n",
    "<a id='equation-hs83-var-pred'></a>\n",
    "$$\n",
    "\\operatorname{Var}[E(R_t \\mid \\psi_{t-1})] = \\alpha^2 \\operatorname{Var}[E(X_t \\mid \\psi_{t-1})]. \\tag{85.17}\n",
    "$$\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] derive the implied $ R^2 $ of the return projection onto $ \\psi_{t-1} $:\n",
    "\n",
    "\n",
    "<a id='equation-hs83-r2'></a>\n",
    "$$\n",
    "R_R^2 = \\frac{\\alpha^2 \\operatorname{Var}[E(X_t \\mid \\psi_{t-1})]}{\\operatorname{Var}(R_t \\mid \\psi_{t-1}) + \\alpha^2 \\operatorname{Var}[E(X_t \\mid \\psi_{t-1})]}. \\tag{85.18}\n",
    "$$\n",
    "\n",
    "If $ \\alpha = 0 $ (risk neutrality), then $ R_R^2 = 0 $ and asset returns are unpredictable.\n",
    "\n",
    "If $ \\alpha = -1 $ (log utility), then $ R_t - X_t $ is unpredictable, so returns and consumption growth share the same predictable component.\n",
    "\n",
    "More generally, the $ R_R^2 $ for returns will be small whenever the variance of the unpredictable return component $ \\operatorname{Var}(R_t \\mid \\psi_{t-1}) $ is large relative to the predictable variance, which is the case for stock returns.\n",
    "\n",
    "The function below reports:\n",
    "\n",
    "- restriction-side predictable-variance terms implied by the Euler equation, and  \n",
    "- $ R_X^2 $ and $ R_R^2 $ from the unrestricted VAR.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57050a3a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def predictability_metrics(data, restricted_fit, unrestricted_fit, n_lags):\n",
    "    \"\"\"\n",
    "    Compute predictable-component metrics and unrestricted VAR R^2 values.\n",
    "    \"\"\"\n",
    "    y_t, y_lag = build_lagged_data(data, n_lags)\n",
    "    parsed = unpack_parameters(restricted_fit[\"params\"], n_lags)\n",
    "    α = float(parsed[\"α\"])\n",
    "    β = float(parsed[\"β\"])\n",
    "    σ_x = float(parsed[\"σ_x\"])\n",
    "    σ_r = float(parsed[\"σ_r\"])\n",
    "    cov_xr = float(parsed[\"cov_xr\"])\n",
    "    μ_x = float(parsed[\"μ_x\"])\n",
    "    a_lags = np.asarray(parsed[\"a_lags\"])\n",
    "\n",
    "    pred_x = y_lag @ a_lags + μ_x\n",
    "    σ_u2 = α ** 2 * σ_x ** 2 + σ_r ** 2 + 2.0 * α * cov_xr\n",
    "    pred_r = -α * pred_x - np.log(β) - 0.5 * σ_u2\n",
    "\n",
    "    x = y_t[:, 0]\n",
    "    r = y_t[:, 1]\n",
    "    resid_x = x - pred_x\n",
    "    resid_r = r - pred_r\n",
    "\n",
    "    r2_x = 1.0 - np.var(resid_x) / np.var(x)\n",
    "    r2_r = 1.0 - np.var(resid_r) / np.var(r)\n",
    "\n",
    "    if unrestricted_fit[\"converged\"] and unrestricted_fit.get(\"coef\") is not None:\n",
    "        n_obs = y_t.shape[0]\n",
    "        x_u = np.column_stack([np.ones(n_obs), y_lag])\n",
    "        pred_u = x_u @ unrestricted_fit[\"coef\"]\n",
    "        resid_u = y_t - pred_u\n",
    "        var_x = np.var(y_t[:, 0])\n",
    "        var_r = np.var(y_t[:, 1])\n",
    "        r2_x_unres = np.nan if var_x <= 0.0 else 1.0 - np.var(resid_u[:, 0]) / var_x\n",
    "        r2_r_unres = np.nan if var_r <= 0.0 else 1.0 - np.var(resid_u[:, 1]) / var_r\n",
    "    else:\n",
    "        r2_x_unres = np.nan\n",
    "        r2_r_unres = np.nan\n",
    "\n",
    "    return {\n",
    "        \"alpha_hat\": α,\n",
    "        \"var_pred_x\": float(np.var(pred_x)),\n",
    "        \"var_pred_r\": float(np.var(pred_r)),\n",
    "        \"alpha2_var_pred_x\": float(α ** 2 * np.var(pred_x)),\n",
    "        \"r2_x_restricted\": float(r2_x),\n",
    "        \"r2_r_restricted\": float(r2_r),\n",
    "        \"r2_x_unrestricted\": float(r2_x_unres),\n",
    "        \"r2_r_unrestricted\": float(r2_r_unres),\n",
    "        \"T\": int(y_t.shape[0]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69058afd",
   "metadata": {},
   "source": [
    "## Return-difference tests\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] also propose tests based on differences in log returns across assets.\n",
    "\n",
    "From [(85.10)](#equation-hs83-cond-mean), the conditional mean of asset $ i $’s log return is $ E_{t-1}[R_{it}] = -\\alpha\\, E_{t-1}[X_t] - \\log\\beta - \\sigma_i^2/2 $.\n",
    "\n",
    "The term $ -\\alpha\\, E_{t-1}[X_t] - \\log\\beta $ is common to all assets, so it cancels in the difference:\n",
    "\n",
    "$$\n",
    "E_{t-1}[R_{it} - R_{jt}] = \\frac{\\sigma_j^2 - \\sigma_i^2}{2},\n",
    "$$\n",
    "\n",
    "which is a constant that does not depend on time-$ (t-1) $ information.\n",
    "\n",
    "Return differences should therefore be unpredictable if the model is correct, regardless of the values of $ \\alpha $ and $ \\beta $.\n",
    "\n",
    "These tests avoid the need to measure consumption, at the cost of losing the ability to identify $ \\alpha $ and $ \\beta $.\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] report that the return-difference restrictions are strongly rejected for models with multiple stock returns, providing substantial evidence against the CRRA-lognormal specification even when consumption measurement problems are eliminated.\n",
    "\n",
    "The code below is an illustration of this logic on simulated data.\n",
    "\n",
    "Reproducing the paper’s empirical return-difference tables requires estimating multi-asset systems that are outside this lecture’s scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cd446",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_multi_asset_nominal_returns(\n",
    "    n_obs,\n",
    "    n_assets=3,\n",
    "    α_true=-1.0,\n",
    "    β_true=0.993,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate log nominal returns satisfying E_t[beta * exp(alpha X) * r_i] = 1.\n",
    "    \"\"\"\n",
    "    if n_assets < 2:\n",
    "        raise ValueError(\"n_assets must be at least 2.\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.empty(n_obs)\n",
    "    x[0] = 0.001\n",
    "    for t in range(1, n_obs):\n",
    "        x[t] = 0.001 + 0.4 * (x[t - 1] - 0.001) + 0.006 * rng.standard_normal()\n",
    "\n",
    "    sigmas = np.linspace(0.03, 0.06, n_assets)\n",
    "    eps = rng.standard_normal((n_obs, n_assets)) * sigmas\n",
    "    log_returns = -np.log(β_true) - α_true * x[:, None] + eps \\\n",
    "            - 0.5 * sigmas[None, :] ** 2\n",
    "    return x, log_returns\n",
    "\n",
    "\n",
    "def return_difference_test(log_returns, n_lags=2):\n",
    "    \"\"\"\n",
    "    Test predictability of pairwise log-return differences.\n",
    "    \"\"\"\n",
    "    if log_returns.ndim != 2 or log_returns.shape[1] < 2:\n",
    "        raise ValueError(\"log_returns must be T x m with m >= 2.\")\n",
    "    if log_returns.shape[0] <= n_lags + 1:\n",
    "        raise ValueError(\"Sample size must exceed n_lags + 1.\")\n",
    "\n",
    "    t_obs, n_assets = log_returns.shape\n",
    "    pairs = list(combinations(range(n_assets), 2))\n",
    "    n_obs = t_obs - n_lags - 1\n",
    "    z = np.empty((n_obs, 1 + n_assets * n_lags))\n",
    "    z[:, 0] = 1.0\n",
    "\n",
    "    for j in range(n_lags):\n",
    "        z[:, 1 + j * n_assets : 1 + (j + 1) * n_assets] = log_returns[\n",
    "            n_lags - j : t_obs - 1 - j, :\n",
    "        ]\n",
    "\n",
    "    rows = []\n",
    "    for i, j in pairs:\n",
    "        y = log_returns[n_lags + 1 :, i] - log_returns[n_lags + 1 :, j]\n",
    "        coef = np.linalg.lstsq(z, y, rcond=None)[0]\n",
    "        resid = y - z @ coef\n",
    "        sigma2 = float((resid @ resid) / max(1, n_obs - z.shape[1]))\n",
    "        cov = sigma2 * np.linalg.pinv(z.T @ z)\n",
    "        slopes = coef[1:]\n",
    "        cov_slopes = cov[1:, 1:]\n",
    "        stat = float(slopes @ np.linalg.pinv(cov_slopes) @ slopes)\n",
    "        p_value = float(1.0 - stats.chi2.cdf(stat, df=slopes.shape[0]))\n",
    "        rows.append(\n",
    "            {\n",
    "                \"pair\": f\"{i+1}-{j+1}\",\n",
    "                \"wald_chi2\": stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"mean_spread\": float(np.mean(y)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(\"pair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9144164",
   "metadata": {},
   "source": [
    "We run `return_difference_test` on simulated data with $ m = 3 $ assets, giving $ \\binom{3}{2} = 3 $ pairs.\n",
    "\n",
    "For each pair, the function regresses the return spread on a constant and `n_lags` lags of all asset returns, then tests whether the slope coefficients are jointly zero using a Wald $ \\chi^2 $ statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b85c9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "_, sim_log_returns = simulate_multi_asset_nominal_returns(\n",
    "    n_obs=1500,\n",
    "    n_assets=3,\n",
    "    α_true=-1.0,\n",
    "    β_true=0.993,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "spread_test = return_difference_test(sim_log_returns, n_lags=2)\n",
    "spread_pretty = spread_test.rename(columns={\n",
    "    \"wald_chi2\": r\"\\chi^2\", \"p_value\": \"p\", \"mean_spread\": r\"\\overline{\\Delta R}\",\n",
    "})\n",
    "display_table(spread_pretty, fmt={\n",
    "    r\"\\chi^2\": \"{:.3f}\",\n",
    "    \"p\": \"{:.3f}\",\n",
    "    r\"\\overline{\\Delta R}\": \"{:.5f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905f8e9",
   "metadata": {},
   "source": [
    "Large $ p $-values confirm that return differences are unpredictable in this simulation, exactly as the model predicts when $ \\alpha = -1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88000e",
   "metadata": {},
   "source": [
    "## Empirical MLE estimation\n",
    "\n",
    "We now apply the maximum likelihood estimator of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] to real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a827a41",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Both this lecture and the companion lecture [Estimating Euler Equations by Generalized Method of Moments](https://python.quantecon.org/hansen_singleton_1982.html) use the same data construction.\n",
    "\n",
    "Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] and Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] use monthly data on real per capita consumption (nondurables) and stock returns from CRSP for the period 1959:2 through 1978:12.\n",
    "\n",
    "To align with the paper, we set the default sample to 1959:2–1978:12.\n",
    "\n",
    "You can pass different `start` and `end` dates to study later periods.\n",
    "\n",
    "This lecture pulls stock-market and one-month bill returns from the Ken French data library (`F-F_Research_Data_Factors`) and constructs gross nominal returns as `1 + (Mkt-RF + RF)/100` for the market and `1 + RF/100` for T-bills.\n",
    "\n",
    "`Mkt-RF` is the value-weighted return on all CRSP firms minus the risk-free rate, and `RF` is the one-month Treasury bill return (see [here](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) for details).\n",
    "\n",
    "While Hansen-Singleton use CRSP value-weighted NYSE returns, we use the Ken French market factor as the closest open-access alternative.\n",
    "\n",
    "The consumption series is constructed from consumption of nondurables (`ND`) with the nondurables deflator.\n",
    "\n",
    "The hidden cell below pulls the relevant FRED series, constructs per capita real consumption, and joins with the Ken French returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180d88d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fred_codes = {\n",
    "    \"population_16plus\": \"CNP16OV\",\n",
    "    \"cons_nd_real_index\": \"DNDGRA3M086SBEA\",\n",
    "    \"cons_nd_price_index\": \"DNDGRG3M086SBEA\",\n",
    "}\n",
    "\n",
    "def to_month_end(index):\n",
    "    \"\"\"\n",
    "    Convert a date index to month-end timestamps.\n",
    "    \"\"\"\n",
    "    return pd.PeriodIndex(pd.DatetimeIndex(index), freq=\"M\").to_timestamp(\"M\")\n",
    "\n",
    "\n",
    "def load_hs_monthly_data(\n",
    "    start=\"1959-02-01\",\n",
    "    end=\"1978-12-01\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build monthly gross real return and gross consumption-growth series.\n",
    "    \"\"\"\n",
    "    start_period = pd.Timestamp(start).to_period(\"M\")\n",
    "    end_period = pd.Timestamp(end).to_period(\"M\")\n",
    "\n",
    "    # Pull one extra month to build the first in-sample growth rate\n",
    "    fetch_start = (start_period - 1).to_timestamp(how=\"start\")\n",
    "    fetch_end = end_period.to_timestamp(\"M\")\n",
    "    sample_start = start_period.to_timestamp(\"M\")\n",
    "    sample_end = end_period.to_timestamp(\"M\")\n",
    "\n",
    "    fred = web.DataReader(\n",
    "        list(fred_codes.values()), \"fred\", fetch_start, fetch_end)\n",
    "    fred = fred.rename(columns={v: k for k, v in fred_codes.items()})\n",
    "    fred.index = to_month_end(fred.index)\n",
    "    fred[\"cons_real_level\"] = fred[\"cons_nd_real_index\"]\n",
    "    fred[\"cons_price_index\"] = fred[\"cons_nd_price_index\"]\n",
    "    fred[\"consumption_per_capita\"] = fred[\"cons_real_level\"] \\\n",
    "        / fred[\"population_16plus\"]\n",
    "    fred[\"gross_cons_growth\"] = (\n",
    "        fred[\"consumption_per_capita\"] / fred[\"consumption_per_capita\"].shift(1)\n",
    "    )\n",
    "    fred[\"gross_inflation_cons\"] = (\n",
    "        fred[\"cons_price_index\"] / fred[\"cons_price_index\"].shift(1)\n",
    "    )\n",
    "\n",
    "    ff = web.DataReader(\n",
    "        \"F-F_Research_Data_Factors\", \"famafrench\", \n",
    "        fetch_start, fetch_end)[0].copy()\n",
    "    ff.columns = [str(col).strip() for col in ff.columns]\n",
    "    if (\"Mkt-RF\" not in ff.columns) or (\"RF\" not in ff.columns):\n",
    "        raise KeyError(\n",
    "            \"Fama-French data missing required columns: 'Mkt-RF' and 'RF'.\")\n",
    "\n",
    "    # Mkt-RF and RF are reported in percent per month\n",
    "    ff[\"gross_nom_return\"] = 1.0 + (ff[\"Mkt-RF\"] + ff[\"RF\"]) / 100.0\n",
    "    ff[\"gross_nom_tbill\"] = 1.0 + ff[\"RF\"] / 100.0\n",
    "    ff.index = ff.index.to_timestamp(how=\"end\")\n",
    "    ff.index = to_month_end(ff.index)\n",
    "    market = ff[[\"gross_nom_return\", \"gross_nom_tbill\"]]\n",
    "\n",
    "    out = fred.join(market, how=\"inner\")\n",
    "    out[\"gross_real_return\"] = out[\"gross_nom_return\"] \\\n",
    "        / out[\"gross_inflation_cons\"]\n",
    "    out[\"gross_real_tbill\"] = out[\"gross_nom_tbill\"] \\\n",
    "        / out[\"gross_inflation_cons\"]\n",
    "    out = out.loc[sample_start:sample_end].dropna()\n",
    "\n",
    "    required_cols = [\n",
    "        \"gross_real_return\",\n",
    "        \"gross_cons_growth\",\n",
    "        \"gross_inflation_cons\",\n",
    "        \"consumption_per_capita\",\n",
    "        \"gross_real_tbill\",\n",
    "    ]\n",
    "    return out[required_cols].copy()\n",
    "\n",
    "\n",
    "def get_estimation_data(\n",
    "    start=\"1959-02-01\",\n",
    "    end=\"1978-12-01\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Return (dataframe, array) using observed data.\n",
    "    \"\"\"\n",
    "    frame = load_hs_monthly_data(start=start, end=end)\n",
    "    data = frame[[\"gross_real_return\", \"gross_cons_growth\"]].to_numpy()\n",
    "    return frame, data\n",
    "\n",
    "\n",
    "def get_tbill_estimation_data(\n",
    "    start=\"1959-02-01\",\n",
    "    end=\"1978-12-01\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Return (dataframe, array) using Treasury bill data.\n",
    "    \"\"\"\n",
    "    frame = load_hs_monthly_data(start=start, end=end)\n",
    "    data = frame[[\"gross_real_tbill\", \"gross_cons_growth\"]].to_numpy()\n",
    "    return frame, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877feec",
   "metadata": {},
   "source": [
    "### MLE estimation and predictability summaries\n",
    "\n",
    "With the data in hand, we can run the MLE estimation and compute the predictability summaries.\n",
    "\n",
    "Lognormality makes the MLE tractable when the assumption is correct.\n",
    "\n",
    "As Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] stress, however, $ \\alpha $ is estimated with relatively large standard errors, and precise inference on risk aversion cannot be expected from aggregate monthly data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fe209",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lags = (2, 4, 6)\n",
    "\n",
    "emp_frame, emp_data = get_estimation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7547592",
   "metadata": {},
   "source": [
    "The following table reports the restricted MLE estimates of $ \\hat\\alpha $ and $ \\hat\\beta $ by lag length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2860f0f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "emp_log_data = to_mle_array(emp_data)\n",
    "mle_table, mle_fits = run_mle_by_lag(\n",
    "    emp_log_data, lags=lags, verbose=False\n",
    ")\n",
    "mle_pretty = mle_table.rename(columns={\n",
    "    \"α_hat\": r\"\\hat{\\alpha}\", \"se_α\": r\"\\mathrm{se}(\\hat{\\alpha})\",\n",
    "    \"β_hat\": r\"\\hat{\\beta}\", \"se_β\": r\"\\mathrm{se}(\\hat{\\beta})\",\n",
    "    \"loglike\": \"logL\", \"n_obs\": \"T\",\n",
    "})\n",
    "display_table(mle_pretty, fmt={\n",
    "    r\"\\hat{\\alpha}\": \"{:.4f}\", r\"\\mathrm{se}(\\hat{\\alpha})\": \"{:.4f}\",\n",
    "    r\"\\hat{\\beta}\": \"{:.4f}\", r\"\\mathrm{se}(\\hat{\\beta})\": \"{:.4f}\",\n",
    "    \"logL\": \"{:.1f}\", \"T\": \"{:.0f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a538c2",
   "metadata": {},
   "source": [
    "The table reports $ \\hat\\alpha $ and $ \\hat\\beta $ by lag length for the sample used in the code cell above.\n",
    "\n",
    "For comparison, Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] report $ \\hat\\alpha $ values of $ -0.32 $ to $ -1.25 $ (standard errors $ 0.65 $ to $ 0.83 $) for the value-weighted return with nondurables consumption.\n",
    "\n",
    "Our numbers fall into those ranges.\n",
    "\n",
    "In risk-aversion units, this corresponds to $ -\\hat\\alpha $ between $ 0.32 $ and $ 1.25 $.\n",
    "\n",
    "We now compute the predictability summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c13f5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "unres_table, unres_fits = run_unrestricted_var_by_lag(\n",
    "    emp_log_data,\n",
    "    lags=lags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e88e0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "pred_rows = []\n",
    "for lag in lags:\n",
    "    metrics = predictability_metrics(\n",
    "        emp_log_data,\n",
    "        restricted_fit=mle_fits[lag],\n",
    "        unrestricted_fit=unres_fits[lag],\n",
    "        n_lags=lag,\n",
    "    )\n",
    "    pred_rows.append({\"NLAG\": lag, **metrics})\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows).set_index(\"NLAG\")\n",
    "pred_pretty = pred_df[\n",
    "    [\n",
    "        \"alpha_hat\",\n",
    "        \"var_pred_x\",\n",
    "        \"var_pred_r\",\n",
    "        \"alpha2_var_pred_x\",\n",
    "        \"r2_x_unrestricted\",\n",
    "        \"r2_r_unrestricted\",\n",
    "        \"T\",\n",
    "    ]\n",
    "].rename(columns={\n",
    "    \"alpha_hat\": r\"\\hat{\\alpha}\",\n",
    "    \"var_pred_x\": r\"\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\",\n",
    "    \"var_pred_r\": r\"\\mathrm{Var}(\\hat E[R_t\\mid\\psi_{t-1}])\",\n",
    "    \"alpha2_var_pred_x\": r\"\\hat{\\alpha}^2\\,\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\",\n",
    "    \"r2_x_unrestricted\": r\"R_X^2\",\n",
    "    \"r2_r_unrestricted\": r\"R_R^2\",\n",
    "    \"T\": \"T\",\n",
    "})\n",
    "display_table(pred_pretty, fmt={\n",
    "    r\"\\hat{\\alpha}\": \"{:.4f}\",\n",
    "    r\"\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\": \"{:.6f}\",\n",
    "    r\"\\mathrm{Var}(\\hat E[R_t\\mid\\psi_{t-1}])\": \"{:.6f}\",\n",
    "    r\"\\hat{\\alpha}^2\\,\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\": \"{:.6f}\",\n",
    "    r\"R_X^2\": \"{:.4f}\",\n",
    "    r\"R_R^2\": \"{:.4f}\",\n",
    "    \"T\": \"{:.0f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5bf52",
   "metadata": {},
   "source": [
    "The column $ \\hat\\alpha^2 \\operatorname{Var}(\\hat E[X_t \\mid \\psi_{t-1}]) $ should equal $ \\operatorname{Var}(\\hat E[R_t \\mid \\psi_{t-1}]) $ if the restriction [(85.17)](#equation-hs83-var-pred) holds.\n",
    "\n",
    "The columns  $ R_X^2 $ and $ R_R^2 $ match the paper convention of reporting the values from the unrestricted VAR projections.\n",
    "\n",
    "In Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)], $ R_R^2 $ is small ($ 0.02 $ to $ 0.06 $) even when $ R_X^2 $ is nontrivial: most stock-return variation is unpredictable.\n",
    "\n",
    "Our estimates show the same pattern.\n",
    "\n",
    "We now test the Euler-equation restrictions by comparing the restricted system to an unrestricted VAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dbd67",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lr_hs83 = restricted_vs_unrestricted_lr(mle_fits, unres_fits, lags=lags)\n",
    "lr_pretty = lr_hs83.rename(columns={\n",
    "    \"lr_stat\": \"LR\",\n",
    "    \"chi2_cdf\": \"chi2.cdf(LR,df)\",\n",
    "    \"p_value\": \"p(LR)\",\n",
    "    \"df\": \"df\",\n",
    "    \"T\": \"T\",\n",
    "})\n",
    "display_table(lr_pretty, fmt={\n",
    "    \"LR\": \"{:.3f}\",\n",
    "    \"chi2.cdf(LR,df)\": \"{:.3f}\",\n",
    "    \"p(LR)\": \"{:.3f}\",\n",
    "    \"df\": \"{:.0f}\",\n",
    "    \"T\": \"{:.0f}\",\n",
    "})\n",
    "\n",
    "sig_level = 0.05\n",
    "rejected_lags = [int(lag) for lag in lr_hs83.index if lr_hs83.loc[lag, \"p_value\"] < sig_level]\n",
    "not_rejected_lags = [int(lag) for lag in lr_hs83.index if lr_hs83.loc[lag, \"p_value\"] >= sig_level]\n",
    "print(f\"Not rejected at 5%: {not_rejected_lags if not_rejected_lags else 'none'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7978a5",
   "metadata": {},
   "source": [
    "The LR test does not reject the model for the value-weighted return, consistent with Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b2ee9",
   "metadata": {},
   "source": [
    "### Treasury bill estimation\n",
    "\n",
    "We now repeat the estimation using the 1-month Treasury bill return in place of the stock return.\n",
    "\n",
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] find that the model is strongly rejected for Treasury bills (Table 4 of their paper).\n",
    "\n",
    "Because the nominally risk-free T-bill return is much more predictable than stock returns, the proportionality restriction has more bite, and the LR test has greater power to detect violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a13212",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_frame, tbill_data = get_tbill_estimation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73528fb6",
   "metadata": {},
   "source": [
    "The following table reports the restricted MLE estimates for Treasury bill returns by lag length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde55fa5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_log_data = to_mle_array(tbill_data)\n",
    "tbill_mle_table, tbill_mle_fits = run_mle_by_lag(\n",
    "    tbill_log_data, lags=lags, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb69a1b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_mle_pretty = tbill_mle_table.rename(columns={\n",
    "    \"α_hat\": r\"\\hat{\\alpha}\", \"se_α\": r\"\\mathrm{se}(\\hat{\\alpha})\",\n",
    "    \"β_hat\": r\"\\hat{\\beta}\", \"se_β\": r\"\\mathrm{se}(\\hat{\\beta})\",\n",
    "    \"loglike\": \"logL\", \"n_obs\": \"T\",\n",
    "})\n",
    "display_table(tbill_mle_pretty, fmt={\n",
    "    r\"\\hat{\\alpha}\": \"{:.4f}\", r\"\\mathrm{se}(\\hat{\\alpha})\": \"{:.4f}\",\n",
    "    r\"\\hat{\\beta}\": \"{:.4f}\", r\"\\mathrm{se}(\\hat{\\beta})\": \"{:.4f}\",\n",
    "    \"logL\": \"{:.1f}\", \"T\": \"{:.0f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2c103",
   "metadata": {},
   "source": [
    "The LR test below remains the main specification check in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72798704",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_unres_table, tbill_unres_fits = run_unrestricted_var_by_lag(\n",
    "    tbill_log_data,\n",
    "    lags=lags,\n",
    ")\n",
    "\n",
    "tbill_unres_pretty = tbill_unres_table.rename(columns={\"loglike\": \"logL\", \"n_obs\": \"T\"})\n",
    "display_table(tbill_unres_pretty, fmt={\n",
    "    \"logL\": \"{:.1f}\", \"T\": \"{:.0f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329cf28",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_pred_rows = []\n",
    "for lag in lags:\n",
    "    metrics = predictability_metrics(\n",
    "        tbill_log_data,\n",
    "        restricted_fit=tbill_mle_fits[lag],\n",
    "        unrestricted_fit=tbill_unres_fits[lag],\n",
    "        n_lags=lag,\n",
    "    )\n",
    "    tbill_pred_rows.append({\"NLAG\": lag, **metrics})\n",
    "\n",
    "tbill_pred_df = pd.DataFrame(tbill_pred_rows).set_index(\"NLAG\")\n",
    "tbill_pred_pretty = tbill_pred_df[\n",
    "    [\n",
    "        \"alpha_hat\",\n",
    "        \"var_pred_x\",\n",
    "        \"var_pred_r\",\n",
    "        \"alpha2_var_pred_x\",\n",
    "        \"r2_x_unrestricted\",\n",
    "        \"r2_r_unrestricted\",\n",
    "        \"T\",\n",
    "    ]\n",
    "].rename(columns={\n",
    "    \"alpha_hat\": r\"\\hat{\\alpha}\",\n",
    "    \"var_pred_x\": r\"\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\",\n",
    "    \"var_pred_r\": r\"\\mathrm{Var}(\\hat E[R_t\\mid\\psi_{t-1}])\",\n",
    "    \"alpha2_var_pred_x\": r\"\\hat{\\alpha}^2\\,\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\",\n",
    "    \"r2_x_unrestricted\": r\"R_X^2\",\n",
    "    \"r2_r_unrestricted\": r\"R_R^2\",\n",
    "    \"T\": \"T\",\n",
    "})\n",
    "display_table(tbill_pred_pretty, fmt={\n",
    "    r\"\\hat{\\alpha}\": \"{:.4f}\",\n",
    "    r\"\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\": \"{:.6f}\",\n",
    "    r\"\\mathrm{Var}(\\hat E[R_t\\mid\\psi_{t-1}])\": \"{:.6f}\",\n",
    "    r\"\\hat{\\alpha}^2\\,\\mathrm{Var}(\\hat E[X_t\\mid\\psi_{t-1}])\": \"{:.6f}\",\n",
    "    r\"R_X^2\": \"{:.4f}\",\n",
    "    r\"R_R^2\": \"{:.4f}\",\n",
    "    \"T\": \"{:.0f}\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e772d4bb",
   "metadata": {},
   "source": [
    "The following table reports the likelihood ratio tests for the Treasury bill model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4518e7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_lr = restricted_vs_unrestricted_lr(tbill_mle_fits, tbill_unres_fits, lags=lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c7502",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tbill_lr_pretty = tbill_lr.rename(columns={\n",
    "    \"lr_stat\": \"LR\",\n",
    "    \"chi2_cdf\": \"chi2.cdf(LR,df)\",\n",
    "    \"p_value\": \"p(LR)\",\n",
    "    \"df\": \"df\",\n",
    "    \"T\": \"T\",\n",
    "})\n",
    "display_table(tbill_lr_pretty, fmt={\n",
    "    \"LR\": \"{:.3f}\",\n",
    "    \"chi2.cdf(LR,df)\": \"{:.3f}\",\n",
    "    \"p(LR)\": \"{:.3f}\",\n",
    "    \"df\": \"{:.0f}\",\n",
    "    \"T\": \"{:.0f}\",\n",
    "})\n",
    "\n",
    "tbill_rejected_lags = [int(lag) for lag in tbill_lr.index if tbill_lr.loc[lag, \"p_value\"] < 0.05]\n",
    "print(f\"T-bill model rejected at 5% for lags: {tbill_rejected_lags if tbill_rejected_lags else 'none'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206d6c9",
   "metadata": {},
   "source": [
    "Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] find the same qualitative pattern in their 1959–1978 sample: the Treasury bill model is rejected much more strongly than the value-weighted stock-return model.\n",
    "\n",
    "Why does the LR test not reject the model for stocks?\n",
    "\n",
    "As we hinted earlier, one reason may be limited test power when return predictability is small (as reflected in the low $ R_R^2 $ for stocks).\n",
    "\n",
    "When aggregate stock returns are nearly unpredictable, there is almost no predictable variation to constrain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8afceb",
   "metadata": {},
   "source": [
    "## Residual diagnostics\n",
    "\n",
    "As a final check, let’s inspect the residual paths, histograms, and diagnostic statistics from the restricted model to assess whether the normality and serial independence assumptions hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e919e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "diag_lag = 2\n",
    "diag_fit = mle_fits[diag_lag]\n",
    "resid = diag_fit[\"residuals\"]\n",
    "\n",
    "if diag_fit[\"converged\"] and resid is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes[0, 0].plot(resid[:, 0], lw=2)\n",
    "    axes[0, 0].axhline(0.0, color=\"black\", lw=2)\n",
    "    axes[0, 0].set_ylabel(\"consumption residual\")\n",
    "    axes[0, 0].set_xlabel(\"observation\")\n",
    "\n",
    "    axes[0, 1].plot(resid[:, 1], lw=2)\n",
    "    axes[0, 1].axhline(0.0, color=\"black\", lw=2)\n",
    "    axes[0, 1].set_ylabel(\"return residual\")\n",
    "    axes[0, 1].set_xlabel(\"observation\")\n",
    "\n",
    "    axes[1, 0].hist(resid[:, 0], bins=30, edgecolor=\"white\")\n",
    "    axes[1, 0].set_xlabel(\"consumption residual\")\n",
    "    axes[1, 0].set_ylabel(\"count\")\n",
    "\n",
    "    axes[1, 1].hist(resid[:, 1], bins=30, edgecolor=\"white\")\n",
    "    axes[1, 1].set_xlabel(\"return residual\")\n",
    "    axes[1, 1].set_ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb53e6",
   "metadata": {},
   "source": [
    "The following table reports Jarque-Bera normality tests and Durbin-Watson serial correlation statistics for the restricted-model residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3aa4a5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "if diag_fit[\"converged\"] and resid is not None:\n",
    "    diag = residual_diagnostics(resid)\n",
    "    diag_df = pd.DataFrame({\n",
    "        \"JB stat\": [diag[\"consumption_jb_stat\"], diag[\"return_jb_stat\"]],\n",
    "        \"JB p-val\": [diag[\"consumption_jb_pval\"], diag[\"return_jb_pval\"]],\n",
    "        \"DW\": [diag[\"consumption_dw\"], diag[\"return_dw\"]],\n",
    "    }, index=pd.Index([\"consumption\", \"return\"], name=\"series\"))\n",
    "    display_table(diag_df, fmt={\n",
    "        \"JB stat\": \"{:.2f}\", \"JB p-val\": \"{:.4f}\", \"DW\": \"{:.3f}\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b40bf2",
   "metadata": {},
   "source": [
    "The residual time-series plots reveal periods of volatility clustering, while the histograms show departures from bell curve with fatter tails.\n",
    "\n",
    "The Jarque-Bera statistics are large for both series, rejecting the normality assumption that underlies the likelihood.\n",
    "\n",
    "The Durbin-Watson statistics are close to 2 for both series, so serial correlation is not a concern.\n",
    "\n",
    "This motivates the companion lecture [Estimating Euler Equations by Generalized Method of Moments](https://python.quantecon.org/hansen_singleton_1982.html) where we discuss how GMM avoids the normality assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5996b1",
   "metadata": {},
   "source": [
    "## Connection to the equity premium puzzle\n",
    "\n",
    "Our estimates reproduce the pattern that Mehra and Prescott [[1985](https://python.quantecon.org/zreferences.html#id218)] later called  the **equity premium puzzle**.\n",
    "\n",
    "- *Low estimated risk aversion:* The estimated $ \\hat\\alpha $ values (and thus risk aversion $ -\\hat\\alpha $) from the table above are similar to those in Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)], who report $ \\hat\\alpha $ between $ -0.32 $ and $ -1.25 $.  \n",
    "- *Tiny return predictability:* The unrestricted-VAR $ R_R^2 $ values are comparable to the 0.02 to 0.06 range in Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] — the predictable component of stock returns is small relative to the unpredictable component.  \n",
    "- *Strong rejection for Treasury bills:* The Euler-equation restrictions are decisively rejected for the nominally risk-free Treasury bill return, just as in Table 4 of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)].  \n",
    "\n",
    "\n",
    "The restrictions have more bite when the return series is more predictable (the unrestricted-VAR $ R_R^2 $ for bills is much larger than for stocks), a precursor to what Weil [[1989](https://python.quantecon.org/zreferences.html#id136)] would later call the **risk-free rate puzzle**.\n",
    "\n",
    "Mehra and Prescott [[1985](https://python.quantecon.org/zreferences.html#id218)] presented closely related findings.\n",
    "\n",
    "In a calibrated version of the consumption-based model with CRRA utility, they showed that for relative risk aversion $ \\gamma_{\\text{MP}} $ in the range of 0 to 10, the model could not simultaneously match:\n",
    "\n",
    "1. the average annual equity premium of about 6%,  \n",
    "1. the average annual risk-free rate of about 1%.  \n",
    "\n",
    "\n",
    "Matching the equity premium requires $ \\gamma_{\\text{MP}} $ well above 10, while matching the risk-free rate requires low $ \\gamma_{\\text{MP}} $.\n",
    "\n",
    "The same difficulty appears in our estimates: the implied risk aversion $ -\\hat\\alpha $ is too low to generate a large equity premium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639362e",
   "metadata": {},
   "source": [
    "## Another approach\n",
    "\n",
    "A companion lecture [Estimating Euler Equations by Generalized Method of Moments](https://python.quantecon.org/hansen_singleton_1982.html) describes a closely related paper in which\n",
    "Hansen and Singleton  specified less about the joint  distribution of returns and consumption growth.\n",
    "\n",
    "They formulated an incomplete probability model that stops  short of specifying a likelihood function.\n",
    "\n",
    "To proceed, they used  a generalized methods of moments (GMM) estimator to estimate  key parameters that appear in the Euler equation."
   ]
  }
 ],
 "metadata": {
  "date": 1772080448.539531,
  "filename": "hansen_singleton_1983.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}