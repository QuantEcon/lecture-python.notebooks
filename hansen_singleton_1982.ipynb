{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf86f86",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8abf17",
   "metadata": {},
   "source": [
    "\n",
    "<a id='hansen-singleton-1982'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56a258",
   "metadata": {},
   "source": [
    "# Estimating Euler Equations by Generalized Method of Moments\n",
    "\n",
    "\n",
    "<a id='index-0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0a367",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Estimating Euler Equations by Generalized Method of Moments](#Estimating-Euler-Equations-by-Generalized-Method-of-Moments)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [The economic model](#The-economic-model)  \n",
    "  - [From conditional to unconditional moments](#From-conditional-to-unconditional-moments)  \n",
    "  - [GMM criterion and asymptotic theory](#GMM-criterion-and-asymptotic-theory)  \n",
    "  - [Covariance estimation and the choice of instruments](#Covariance-estimation-and-the-choice-of-instruments)  \n",
    "  - [Empirical GMM estimation](#Empirical-GMM-estimation)  \n",
    "  - [Summary](#Summary)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265996e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture implements the generalized instrumental variables estimator of Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] for nonlinear rational expectations models.\n",
    "\n",
    "The preceding lecture [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html) derives the consumption Euler equation from the representative consumer’s problem with CRRA preferences and estimates it by maximum likelihood under normality assumptions.\n",
    "\n",
    "That approach requires specifying the joint distribution of consumption and returns, and its validity depends on lognormality being correct.\n",
    "\n",
    "However, as we saw in [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html), the lognormal model is rejected by the data.\n",
    "\n",
    "Moreover, outside of linear-quadratic environments, closed-form solutions for equilibrium typically require strong assumptions about the stochastic properties of forcing variables, the nature of preferences, or the production technology.\n",
    "\n",
    "Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] propose an estimation strategy that circumvents this requirement.\n",
    "\n",
    "The key idea is that the Euler equations from economic agents’ optimization problems imply a set of population orthogonality conditions that depend on observable variables and unknown preference parameters.\n",
    "\n",
    "By making sample counterparts of these orthogonality conditions close to zero, the parameters can be estimated without explicitly solving for the stochastic equilibrium and without specifying the distribution of the observable variables.\n",
    "\n",
    "Though maximum likelihood estimators (such as the MLE in [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html)) will be asymptotically more efficient when the distributional assumptions are correctly specified.\n",
    "\n",
    "Relative to the full paper, we only estimate one return at a time (value-weighted stock returns), using only monthly nondurable consumption (`ND`), and omitting their maximum-likelihood comparison (Table II) and multi-return systems (Table III).\n",
    "\n",
    "In addition to what comes with Anaconda, this lecture requires `pandas-datareader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb7fc0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95055f8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Latex\n",
    "from numba import njit\n",
    "from pandas_datareader import data as web\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*date_parser.*\", category=FutureWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4c57e",
   "metadata": {},
   "source": [
    "We also define a helper to display DataFrames as LaTeX arrays in the hidden cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7aa74a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def display_table(df, title=None, fmt=None):\n",
    "    \"\"\"\n",
    "    Display a DataFrame as a LaTeX array.\n",
    "    \"\"\"\n",
    "    if fmt is None:\n",
    "        fmt = {}\n",
    "    formatted = df.copy()\n",
    "    for col in formatted.columns:\n",
    "        if col in fmt:\n",
    "            formatted[col] = formatted[col].apply(\n",
    "                lambda x: fmt[col].format(x) if np.isfinite(x) else str(x))\n",
    "    n_cols = len(formatted.columns)\n",
    "    idx_header = r\"\\text{\" + df.index.name + \"}\" if df.index.name else \"\"\n",
    "    columns = \" & \".join(\n",
    "        [idx_header] + [r\"\\text{\" + c + \"}\" if \"\\\\\" not in c\n",
    "         and \"^\" not in c and \"_\" not in c\n",
    "         else c for c in formatted.columns])\n",
    "    rows = r\" \\\\\".join(\n",
    "        [\" & \".join([str(idx)] + [str(v) for v in row])\n",
    "         for idx, row in zip(formatted.index, formatted.values)])\n",
    "    col_format = \"r\" + \"c\" * n_cols\n",
    "    lines = [r\"\\begin{array}{\" + col_format + \"}\"]\n",
    "    lines.append(columns + r\" \\\\\")\n",
    "    lines.append(r\"\\hline\")\n",
    "    lines.append(rows)\n",
    "    lines.append(r\"\\end{array}\")\n",
    "    latex = \"\\n\".join(lines)\n",
    "    if title:\n",
    "        latex = rf\"\\textbf{{{title}}}\" + r\"\\\\\" + \"\\n\" + latex\n",
    "    display(Latex(\"$\" + latex + \"$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781731ee",
   "metadata": {},
   "source": [
    "## The economic model\n",
    "\n",
    "We consider a single-good economy with a representative consumer whose preferences are of the CRRA type, following Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] and Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)].\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The following discussion is very close to the setup in [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html), but it is more general in that it allows for multiple assets with different maturities and does not assume lognormality.\n",
    "\n",
    "The representative consumer chooses stochastic consumption and investment plans to maximize\n",
    "\n",
    "\n",
    "<a id='equation-hs82-problem'></a>\n",
    "$$\n",
    "\\max E_0 \\sum_{t=0}^{\\infty} \\beta^t u(C_t) \\tag{86.1}\n",
    "$$\n",
    "\n",
    "where $ C_t $ is consumption in period $ t $, $ \\beta \\in (0,1) $ is a subjective discount factor, and $ u(\\cdot) $ is a strictly concave period utility function.\n",
    "\n",
    "The consumer trades $ N $ assets with potentially different maturities.\n",
    "\n",
    "Let $ Q_{jt} $ denote the quantity of asset $ j $ held at the end of date $ t $, $ P_{jt} $ its price at date $ t $, and $ R_{jt} $ the date-$ t $ payoff from holding one unit of an $ M_j $-period asset purchased at date $ t - M_j $.\n",
    "\n",
    "Feasible consumption and investment plans must satisfy the sequence of budget constraints\n",
    "\n",
    "\n",
    "<a id='equation-hs82-budget'></a>\n",
    "$$\n",
    "C_t + \\sum_{j=1}^{N} P_{jt} Q_{jt} \\leq \\sum_{j=1}^{N} R_{jt} Q_{jt-M_j} + W_t, \\tag{86.2}\n",
    "$$\n",
    "\n",
    "where $ W_t $ is real labor income at date $ t $.\n",
    "\n",
    "We specialize to CRRA preferences\n",
    "\n",
    "\n",
    "<a id='equation-hs82-crra'></a>\n",
    "$$\n",
    "u(C_t) = \\frac{C_t^{1-\\gamma}}{1-\\gamma}, \\quad \\gamma > 0, \\tag{86.3}\n",
    "$$\n",
    "\n",
    "where $ \\gamma $ is the coefficient of relative risk aversion.\n",
    "\n",
    "The maximization of [(86.1)](#equation-hs82-problem) subject to [(86.2)](#equation-hs82-budget) gives the first-order necessary conditions (see Lucas [[1978](https://python.quantecon.org/zreferences.html#id210)], Brock [[1982](https://python.quantecon.org/zreferences.html#id259)], Prescott and Mehra [[1980](https://python.quantecon.org/zreferences.html#id260)], and [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html)):\n",
    "\n",
    "\n",
    "<a id='equation-hs82-general-euler'></a>\n",
    "$$\n",
    "P_{jt} u'(C_t) = \\beta^{M_j} E_t\\!\\left[R_{jt+M_j} u'(C_{t+M_j})\\right], \\quad j = 1, \\ldots, N. \\tag{86.4}\n",
    "$$\n",
    "\n",
    "When asset $ j $ is a one-period stock ($ M_j = 1 $) with payoff $ R_{jt+1} = P_{jt+1} + D_{jt+1} $ where $ D_{jt} $ is the dividend, the **gross real return** is $ R_{t+1}^i = (P_{i,t+1}+D_{i,t+1})/P_{i,t} $.\n",
    "\n",
    "Substituting the CRRA marginal utility into [(86.4)](#equation-hs82-general-euler) and dividing both sides by $ P_{jt} u'(C_t) $ yields the Euler equation\n",
    "\n",
    "\n",
    "<a id='equation-hs82-euler'></a>\n",
    "$$\n",
    "E_t\\!\\left[\\beta \\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\gamma} R_{t+1}^i\\right] = 1, \\tag{86.5}\n",
    "$$\n",
    "\n",
    "where $ R_{t+1}^i $ is the gross real return on asset $ i $.\n",
    "\n",
    "We define the **stochastic discount factor** $ M_{t+1}(\\theta) = \\beta (C_{t+1}/C_t)^{-\\gamma} $ with parameter vector $ \\theta = (\\gamma, \\beta) $.\n",
    "\n",
    "In this notation the Euler equation becomes $ E_t[M_{t+1}(\\theta) R_{t+1}^i - 1] = 0 $.\n",
    "\n",
    "As we have seen and will see, equation [(86.5)](#equation-hs82-euler) is the central object of both [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html) and this lecture.\n",
    "\n",
    "It holds for every traded asset for which the agent’s optimality conditions apply (interior solution, no binding portfolio constraints or transaction costs).\n",
    "\n",
    "It depends on observable quantities (consumption growth and returns) and unknown preference parameters ($ \\gamma $ and $ \\beta $).\n",
    "\n",
    "The challenge that Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] address is how to estimate $ \\theta $ from [(86.5)](#equation-hs82-euler) without specifying the rest of the economic environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e76ff",
   "metadata": {},
   "source": [
    "## From conditional to unconditional moments\n",
    "\n",
    "A natural approach to estimating $ \\theta $ from [(86.5)](#equation-hs82-euler) would be to specify the entire economic environment, solve for equilibrium, and apply maximum likelihood.\n",
    "\n",
    "Just like what we did together in [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html),\n",
    "\n",
    "Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] argue that this is impractical for most nonlinear models because it requires strong assumptions about the stochastic properties of “forcing variables” and the production technology.\n",
    "\n",
    "Their alternative is to work directly with the Euler equation’s implications for observable moments.\n",
    "\n",
    "The Euler equation [(86.5)](#equation-hs82-euler) states that $ E_t[M_{t+1}(\\theta_0) R_{t+1}^i - 1] = 0 $.\n",
    "\n",
    "Let $ z_t $ denote any $ q $-dimensional vector of variables that are in the agent’s time-$ t $ information set and observed by the econometrician.\n",
    "\n",
    "Because $ z_t $ is known at date $ t $,\n",
    "\n",
    "$$\n",
    "E\\!\\left[\\left(M_{t+1}(\\theta_0)R_{t+1}^i - 1\\right) \\otimes z_t\\right]\n",
    "= E\\!\\left[z_t \\otimes \\underbrace{E_t\\!\\left[M_{t+1}(\\theta_0)R_{t+1}^i - 1\\right]}_{=\\,0\\text{ by the Euler equation}}\\right] = 0.\n",
    "$$\n",
    "\n",
    "This yields the moment restriction\n",
    "\n",
    "\n",
    "<a id='equation-hs82-uncond'></a>\n",
    "$$\n",
    "E\\!\\left[\\left(M_{t+1}(\\theta_0)R_{t+1}^i - 1\\right) \\otimes z_t\\right] = 0, \\tag{86.6}\n",
    "$$\n",
    "\n",
    "where $ \\otimes $ denotes the Kronecker product.\n",
    "\n",
    "The vector $ z_t $ plays the role of **instruments**.\n",
    "\n",
    "The conditional Euler equation $ E_t[M_{t+1}R_{t+1}^i - 1] = 0 $ says that the pricing error is unpredictable given *everything* in the agent’s time-$ t $ information set.\n",
    "\n",
    "That is a very strong restriction — it says the pricing error is orthogonal to every time-$ t $ measurable random variable.\n",
    "\n",
    "We cannot use the entire information set in practice, but we can pick any finite collection of time-$ t $ observable variables $ z_t $ and the orthogonality must still hold.\n",
    "\n",
    "Each variable we include in $ z_t $ gives us one sample moment condition $ \\frac{1}{T}\\sum_t (M_{t+1}R_{t+1}^i - 1)\\, z_{kt} \\approx 0 $ that we can compute from data.\n",
    "\n",
    "More instruments means more orthogonality conditions to match, which can improve efficiency and provides more overidentifying restrictions to test the model against.\n",
    "\n",
    "With $ q $ instruments and $ m $ Euler equations we obtain $ mq $ moment restrictions for estimating the parameter vector $ \\theta $.\n",
    "\n",
    "For one return and $ p $ lags of instruments, Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] use\n",
    "\n",
    "\n",
    "<a id='equation-hs82-instruments'></a>\n",
    "$$\n",
    "z_t = \\left[1, R_t, g_t, R_{t-1}, g_{t-1}, \\ldots, R_{t-p+1}, g_{t-p+1}\\right]^\\top, \\tag{86.7}\n",
    "$$\n",
    "\n",
    "where $ g_t = C_t / C_{t-1} $ is gross consumption growth.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] describe $ z_t $ as “lagged values of $ x_{t+1} $” where $ x_{t+1} = (R_{t+1}, g_{t+1}) $.\n",
    "\n",
    "The constant 1 is not stated explicitly but is implied by the degrees of freedom reported in their Table I.\n",
    "\n",
    "More generally, Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] write the first-order condition as $ E_t[h(x_{t+n}, b_0)] = 0 $, where $ x_{t+n} $ is a vector of observables dated $ t+n $ and $ b_0 $ is the true parameter vector.\n",
    "\n",
    "The disturbance $ u_{t+1} = h(x_{t+1}, b_0) $ is serially uncorrelated in the one-period stock case.\n",
    "\n",
    "Stacking moment conditions via the Kronecker product gives $ f(x_{t+n}, z_t, b) = h(x_{t+n}, b) \\otimes z_t $, a vector of dimension $ mq $.\n",
    "\n",
    "The resulting unconditional restriction $ E[f(x_{t+n}, z_t, b_0)] = 0 $ nests both the single-return one-period Euler equation and the multi-maturity asset pricing restrictions.\n",
    "\n",
    "The orthogonality condition and lagged instrument vector follow equations [(86.6)](#equation-hs82-uncond) and [(86.7)](#equation-hs82-instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b64f5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def euler_error_horizon(params, exog, horizon=1):\n",
    "    \"\"\"\n",
    "    Compute Euler-equation pricing errors for (γ, β) at a given horizon.\n",
    "    \"\"\"\n",
    "    if horizon < 1:\n",
    "        raise ValueError(\"horizon must be at least one.\")\n",
    "    γ, β = params\n",
    "    gross_return = exog[:, 0]\n",
    "    gross_cons_growth = exog[:, 1]\n",
    "    return (β ** horizon) * gross_cons_growth ** (-γ) * gross_return - 1.0\n",
    "\n",
    "\n",
    "def euler_error_grad_horizon(params, exog, horizon=1):\n",
    "    \"\"\"\n",
    "    Gradient of the Euler error wrt (γ, β) at a given horizon.\n",
    "    \"\"\"\n",
    "    if horizon < 1:\n",
    "        raise ValueError(\"horizon must be at least one.\")\n",
    "    γ, β = params\n",
    "    gross_return = exog[:, 0]\n",
    "    gross_cons_growth = exog[:, 1]\n",
    "\n",
    "    g_pow = gross_cons_growth ** (-γ)\n",
    "    common = (β ** horizon) * g_pow * gross_return\n",
    "\n",
    "    dγ = -common * np.log(gross_cons_growth)\n",
    "    dβ = horizon * (β ** (horizon - 1)) * g_pow * gross_return\n",
    "    return np.column_stack([dγ, dβ])\n",
    "\n",
    "\n",
    "def euler_error(params, exog):\n",
    "    \"\"\"\n",
    "    One-period Euler-equation pricing errors for (γ, β).\n",
    "    \"\"\"\n",
    "    return euler_error_horizon(params, exog, horizon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77651f09",
   "metadata": {},
   "source": [
    "A helper aligns outcomes and lagged instruments for nonlinear IV-GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac89f48",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_gmm_arrays(data, n_lags):\n",
    "    \"\"\"\n",
    "    Build endog, exog, and instruments for nonlinear IV-GMM.\n",
    "    \"\"\"\n",
    "    if n_lags < 1:\n",
    "        raise ValueError(\"n_lags must be at least one.\")\n",
    "    if data.shape[0] <= n_lags:\n",
    "        raise ValueError(\"Sample size must exceed n_lags.\")\n",
    "\n",
    "    t_obs = data.shape[0]\n",
    "    exog = data[n_lags:, :]\n",
    "    endog = np.zeros(exog.shape[0])\n",
    "    n_obs = t_obs - n_lags\n",
    "    n_instr = 2 * n_lags + 1\n",
    "\n",
    "    instruments = np.empty((n_obs, n_instr))\n",
    "    instruments[:, 0] = 1.0\n",
    "\n",
    "    for j in range(n_lags):\n",
    "        left = 2 * j + 1\n",
    "        right = left + 2\n",
    "        instruments[:, left:right] = data[n_lags - 1 - j : t_obs - 1 - j, :]\n",
    "\n",
    "    return endog, exog, instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466646b7",
   "metadata": {},
   "source": [
    "When the asset has maturity $ n > 1 $, the Euler equation involves $ n $-period compounded returns and consumption growth.\n",
    "\n",
    "For CRRA preferences, the $ n $-period Euler restriction is\n",
    "\n",
    "\n",
    "<a id='equation-hs82-euler-n'></a>\n",
    "$$\n",
    "E_t\\!\\left[\\beta^n \\left(\\frac{C_{t+n}}{C_t}\\right)^{-\\gamma} R_{t,t+n}^i\\right] = 1. \\tag{86.8}\n",
    "$$\n",
    "\n",
    "For estimation, the $ n $-period exog can either use directly observed $ n $-period returns/payoffs, or be constructed by compounding one-period returns and consumption growth over $ n $ consecutive periods.\n",
    "\n",
    "Again, a key requirement from Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] is that instruments $ z_t $ must lie in the agent’s time-$ t $ information set $ \\mathcal{I}_t $.\n",
    "\n",
    "For the multi-period case, instruments must be measurable with respect to $ \\mathcal{I}_t $.\n",
    "\n",
    "In particular, one should avoid any lagged multi-period aggregates that still include periods after $ t $, since these would embed realizations not in $ \\mathcal{I}_t $.\n",
    "\n",
    "The $ n $-period exog is constructed by compounding one-period data, and instruments are timed to lie in $ \\mathcal{I}_t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed12cee",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_gmm_arrays_horizon(one_period_data, n_lags, horizon):\n",
    "    \"\"\"\n",
    "    Build endog, exog, and instruments for multi-period GMM.\n",
    "\n",
    "    Exog contains n-period compounded returns and consumption growth.\n",
    "    \"\"\"\n",
    "    if horizon < 1:\n",
    "        raise ValueError(\"horizon must be at least one.\")\n",
    "    if n_lags < 1:\n",
    "        raise ValueError(\"n_lags must be at least one.\")\n",
    "    T = one_period_data.shape[0]\n",
    "    if T <= n_lags + horizon:\n",
    "        raise ValueError(\"Sample size too small for given n_lags and horizon.\")\n",
    "\n",
    "    # Each observation starts at index t (the first period in the window)\n",
    "    # The window spans one_period_data[t : t + horizon]\n",
    "    # Instruments use one_period_data[t - 1], ..., one_period_data[t - n_lags]\n",
    "    starts = np.arange(n_lags, T - horizon + 1)\n",
    "    n_obs = len(starts)\n",
    "\n",
    "    exog = np.empty((n_obs, 2))\n",
    "    n_instr = 2 * n_lags + 1\n",
    "    instruments = np.empty((n_obs, n_instr))\n",
    "    instruments[:, 0] = 1.0\n",
    "\n",
    "    for i, t in enumerate(starts):\n",
    "        window = one_period_data[t : t + horizon, :]\n",
    "        exog[i, 0] = np.prod(window[:, 0])   # n-period return\n",
    "        exog[i, 1] = np.prod(window[:, 1])   # n-period consumption growth\n",
    "        for j in range(n_lags):\n",
    "            instruments[\n",
    "                i, 2 * j + 1 : 2 * j + 3] = one_period_data[t - 1 - j, :]\n",
    "\n",
    "    endog = np.zeros(n_obs)\n",
    "    return endog, exog, instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3289ada",
   "metadata": {},
   "source": [
    "When $ n > 1 $, the Euler equation involves variables dated $ t + n $, and the disturbance $ u_{t+n} = h(x_{t+n}, b_0) $ will generally be serially correlated.\n",
    "\n",
    "As Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] note, if the $ m $ assets are all one-period stocks, then $ u $ is serially uncorrelated because observations on $ x_{t-s} $, $ s \\geq 0 $, are in the agent’s time-$ t $ information set and $ E_t[h(x_{t+1}, b_0)] = 0 $.\n",
    "\n",
    "But if $ n_j > 1 $ for some asset $ j $, the condition $ E_t[h(x_{t+n}, b_0)] = 0 $ does not preclude serial correlation in $ u $, since $ x_{t+n-1} $ is not necessarily in $ I_t $ when $ n > 1 $.\n",
    "\n",
    "The number of population autocovariances in the long-run covariance $ S_0 $ is determined by $ n $, the order of the moving average disturbance term $ u_t $.\n",
    "\n",
    "We implement this directly as a finite-order covariance estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629d38c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def finite_ma_covariance(moment_series, ma_order):\n",
    "    \"\"\"\n",
    "    Estimate \n",
    "    S = Gamma_0 + sum_{j=1}^{ma_order}(Gamma_j + Gamma_j.T) for moment vectors.\n",
    "    \"\"\"\n",
    "    if ma_order < 0:\n",
    "        raise ValueError(\"ma_order must be nonnegative.\")\n",
    "    if moment_series.ndim != 2:\n",
    "        raise ValueError(\"moment_series must be 2D.\")\n",
    "\n",
    "    t_obs, n_mom = moment_series.shape\n",
    "    if t_obs <= ma_order:\n",
    "        raise ValueError(\"Need more observations than ma_order.\")\n",
    "\n",
    "    # Use the *uncentered* cross products\n",
    "    # T^{-1} sum_t f_t f_{t-j}' and then add the symmetric lag terms.\n",
    "    s_hat = moment_series.T @ moment_series / t_obs\n",
    "\n",
    "    for j in range(1, ma_order + 1):\n",
    "        gamma_j = moment_series[j:, :].T @ moment_series[:-j, :] / t_obs\n",
    "        s_hat += gamma_j + gamma_j.T\n",
    "\n",
    "    ridge = 1e-8 * np.eye(n_mom)\n",
    "    return s_hat + ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf7220",
   "metadata": {},
   "source": [
    "The estimation procedure in Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] is a two-step generalized instrumental variables procedure.\n",
    "\n",
    "In the first step, we minimize the GMM criterion with a suboptimal weighting matrix (the identity) to obtain a consistent preliminary estimate $ b_T $.\n",
    "\n",
    "In the second step, we use $ b_T $ to estimate the covariance matrix of the sample moment conditions and invert it to form the optimal weighting matrix, then re-minimize the criterion.\n",
    "\n",
    "Let’s implement this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2fb0b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def two_step_gmm(data, n_lags, ma_order=0, horizon=1, start_params=None):\n",
    "    \"\"\"\n",
    "    Two-step GMM with finite-order covariance.\n",
    "\n",
    "    The Euler error uses β**horizon.\n",
    "    \"\"\"\n",
    "    if start_params is None:\n",
    "        start_params = np.array([1.0, 0.99])\n",
    "    else:\n",
    "        start_params = np.asarray(start_params, dtype=float)\n",
    "\n",
    "    if horizon == 1:\n",
    "        _, exog, instruments = build_gmm_arrays(data, n_lags)\n",
    "    else:\n",
    "        _, exog, instruments = build_gmm_arrays_horizon(data, n_lags, horizon)\n",
    "    n_obs = exog.shape[0]\n",
    "\n",
    "    def sample_moments(params):\n",
    "        err = euler_error_horizon(params, exog, horizon=horizon)\n",
    "        return err[:, None] * instruments\n",
    "\n",
    "    def objective(params, weight_matrix):\n",
    "        g_bar = sample_moments(params).mean(axis=0)\n",
    "        return float(g_bar @ weight_matrix @ g_bar)\n",
    "\n",
    "    def objective_grad(params, weight_matrix):\n",
    "        g_bar = sample_moments(params).mean(axis=0)\n",
    "        grad_err = euler_error_grad_horizon(params, exog, horizon=horizon)\n",
    "        d_bar = (instruments.T @ grad_err) / n_obs\n",
    "        return 2.0 * d_bar.T @ weight_matrix @ g_bar\n",
    "\n",
    "    q = instruments.shape[1]\n",
    "    w_identity = np.eye(q)\n",
    "\n",
    "    bounds = [(-2.0, 10.0), (0.85, 1.5)]\n",
    "\n",
    "    def coarse_starts(weight_matrix, n_best=5):\n",
    "        γ_grid = np.linspace(bounds[0][0], bounds[0][1], 33)\n",
    "        β_grid = np.linspace(bounds[1][0], bounds[1][1], 33)\n",
    "        scored = []\n",
    "        for γ0 in γ_grid:\n",
    "            for β0 in β_grid:\n",
    "                params0 = np.array([γ0, β0])\n",
    "                val = objective(params0, weight_matrix)\n",
    "                if np.isfinite(val):\n",
    "                    scored.append((val, params0))\n",
    "        scored.sort(key=lambda item: item[0])\n",
    "        return [params for _, params in scored[:n_best]] or [start_params]\n",
    "\n",
    "    def best_local_minimize(weight_matrix, starts):\n",
    "        best = None\n",
    "        for x0 in starts:\n",
    "            res = minimize(\n",
    "                objective,\n",
    "                x0=x0,\n",
    "                args=(weight_matrix,),\n",
    "                jac=objective_grad,\n",
    "                method=\"L-BFGS-B\",\n",
    "                bounds=bounds,\n",
    "                options={\"maxiter\": 25_000},\n",
    "            )\n",
    "            if not np.isfinite(res.fun):\n",
    "                continue\n",
    "            if best is None or (res.fun < best.fun):\n",
    "                best = res\n",
    "        return best if best is not None else minimize(\n",
    "            objective,\n",
    "            x0=start_params,\n",
    "            args=(weight_matrix,),\n",
    "            jac=objective_grad,\n",
    "            method=\"L-BFGS-B\",\n",
    "            bounds=bounds,\n",
    "            options={\"maxiter\": 25_000},\n",
    "        )\n",
    "\n",
    "    step1_starts = [start_params] + coarse_starts(w_identity, n_best=5)\n",
    "    step1 = best_local_minimize(w_identity, step1_starts)\n",
    "    params1 = step1.x\n",
    "\n",
    "    m1 = sample_moments(params1)\n",
    "    s_hat = finite_ma_covariance(m1, ma_order=ma_order)\n",
    "    w_opt = np.linalg.pinv(s_hat)\n",
    "\n",
    "    step2_starts = [params1] + coarse_starts(w_opt, n_best=5)\n",
    "    step2 = best_local_minimize(w_opt, step2_starts)\n",
    "    params2 = step2.x\n",
    "\n",
    "    m2 = sample_moments(params2)\n",
    "    s_hat2 = finite_ma_covariance(m2, ma_order=ma_order)\n",
    "    w_opt2 = np.linalg.pinv(s_hat2)\n",
    "    g2 = m2.mean(axis=0)\n",
    "    j_stat = float(n_obs * (g2 @ w_opt2 @ g2))\n",
    "    df = instruments.shape[1] - len(params2)\n",
    "    j_prob = float(stats.chi2.cdf(j_stat, df=df)) if df > 0 else np.nan\n",
    "    p_value = float(1.0 - j_prob) if df > 0 else np.nan\n",
    "\n",
    "    # Asymptotic covariance under optimal weighting: (D' S^{-1} D)^{-1} / T.\n",
    "    grad_err = euler_error_grad_horizon(params2, exog, horizon=horizon)\n",
    "    d_hat = (instruments.T @ grad_err) / n_obs\n",
    "    cov_hat = np.linalg.pinv(d_hat.T @ w_opt2 @ d_hat) / n_obs\n",
    "    se_hat = np.sqrt(np.diag(cov_hat))\n",
    "\n",
    "    return {\n",
    "        \"params_step1\": params1,\n",
    "        \"params_step2\": params2,\n",
    "        \"se_step2\": se_hat,\n",
    "        \"weight_opt\": w_opt2,\n",
    "        \"j_stat\": j_stat,\n",
    "        \"j_df\": int(df),\n",
    "        \"j_prob\": j_prob,\n",
    "        \"j_pval\": p_value,\n",
    "        \"n_obs\": int(n_obs),\n",
    "        \"success\": bool(step2.success),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff00853",
   "metadata": {},
   "source": [
    "## GMM criterion and asymptotic theory\n",
    "\n",
    "We now formalize the estimation procedure.\n",
    "\n",
    "Let $ m_t(\\theta) = (M_{t+1}(\\theta) R_{t+1}^i - 1) \\otimes z_t $ denote the vector of moment conditions at date $ t $, and define the sample mean\n",
    "\n",
    "\n",
    "<a id='equation-hs82-sample-moments'></a>\n",
    "$$\n",
    "g_T(\\theta) = \\frac{1}{T} \\sum_{t=1}^T m_t(\\theta). \\tag{86.9}\n",
    "$$\n",
    "\n",
    "If the model is correctly specified, $ g_T(\\theta_0) $ should be close to zero for large $ T $.\n",
    "\n",
    "We estimate $ \\theta $ by choosing the parameter vector that makes $ g_T $ as close to zero as possible, measured by a quadratic form:\n",
    "\n",
    "\n",
    "<a id='equation-hs82-criterion'></a>\n",
    "$$\n",
    "\\hat\\theta = \\arg\\min_\\theta g_T(\\theta)^\\top W_T g_T(\\theta) \\tag{86.10}\n",
    "$$\n",
    "\n",
    "where $ W_T $ is a symmetric positive-definite weighting matrix.\n",
    "\n",
    "Under regularity conditions given in Hansen [[1982](https://python.quantecon.org/zreferences.html#id261)], the GMM estimator is consistent, asymptotically normal, and has the sandwich covariance matrix\n",
    "\n",
    "\n",
    "<a id='equation-hs82-asymptotic'></a>\n",
    "$$\n",
    "\\sqrt{T}(\\hat\\theta-\\theta_0) \\Rightarrow N\\!\\left(0, (D^\\top W D)^{-1} D^\\top W S W D (D^\\top W D)^{-1}\\right), \\tag{86.11}\n",
    "$$\n",
    "\n",
    "where $ D = E[\\partial m_t(\\theta_0)/\\partial\\theta^\\top] $ is the Jacobian of the moment conditions, $ S $ is the long-run covariance matrix of $ m_t(\\theta_0) $, and $ W $ is the probability limit of $ W_T $.\n",
    "\n",
    "Hansen [[1982](https://python.quantecon.org/zreferences.html#id261)] shows that the optimal weighting matrix is $ W^* = S^{-1} $, which yields the smallest asymptotic covariance matrix among all choices of $ W $.\n",
    "\n",
    "Under $ W = S^{-1} $ the sandwich simplifies to $ (D^\\top S^{-1} D)^{-1} $.\n",
    "\n",
    "When the number of moment conditions $ r $ (e.g., $ r = mq $ for $ m $ Euler equations and $ q $ instruments) exceeds the number of parameters $ k $, the model is overidentified and we can test whether the data are consistent with the maintained restrictions.\n",
    "\n",
    "Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] test the overidentifying restrictions using a result from Hansen [[1982](https://python.quantecon.org/zreferences.html#id261)]:\n",
    "\n",
    "\n",
    "<a id='equation-hs82-jtest'></a>\n",
    "$$\n",
    "J_T = T\\, g_T(\\hat\\theta)^\\top \\hat S^{-1} g_T(\\hat\\theta) \\Rightarrow \\chi^2_{r-k}, \\tag{86.12}\n",
    "$$\n",
    "\n",
    "where $ \\hat S $ is a consistent estimator of $ S $.\n",
    "\n",
    "A large $ J_T $ relative to $ \\chi^2_{r-k} $ critical values leads to rejection of the model’s overidentifying restrictions.\n",
    "\n",
    "For the multi-period case ($ n > 1 $), the disturbance is at most MA($ n-1 $) as discussed above, so Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] obtain the optimal weighting matrix by setting $ W_0 = S_0^{-1} $ where\n",
    "\n",
    "\n",
    "<a id='equation-hs82-finite-so'></a>\n",
    "$$\n",
    "S_0 = \\sum_{j=-n+1}^{n-1} E\\!\\left[f(x_{t+n}, z_t, b_0)\\, f(x_{t+n-j}, z_{t-j}, b_0)^\\top\\right]. \\tag{86.13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7723f95",
   "metadata": {},
   "source": [
    "## Covariance estimation and the choice of instruments\n",
    "\n",
    "For the one-period Euler equation ($ n = 1 $), the disturbance $ u_{t+1} = M_{t+1}(\\theta_0) R_{t+1} - 1 $ is a martingale difference sequence.\n",
    "\n",
    "In this case the moment vector $ m_t(\\theta_0) = u_{t+1} \\otimes z_t $ is serially uncorrelated, and the long-run covariance $ S $ equals the contemporaneous variance $ E[m_t m_t^\\top] $.\n",
    "\n",
    "The covariance estimator therefore requires no kernel or bandwidth choice.\n",
    "\n",
    "We simply use the sample analogue $ \\hat S = T^{-1} \\sum_t m_t m_t^\\top $.\n",
    "\n",
    "In our implementation below, we use a HAC (heteroskedasticity and autocorrelation consistent) estimator  against possible mild serial dependence from time aggregation or measurement timing.\n",
    "\n",
    "This is a modern precaution, not part of the original Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] procedure, which exploits the known MA order directly as in [(86.13)](#equation-hs82-finite-so).\n",
    "\n",
    "The number of instrument lags $ p $ determines how many orthogonality conditions we use and hence the power of the $ J $ test.\n",
    "\n",
    "Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] report results for NLAG $ = 1, 2, 4, 6 $ and note (footnote 12) that using more orthogonality conditions may lead to estimators with less desirable small-sample properties.\n",
    "\n",
    "Below we implement the estimation procedure and allow the user to choose the number of lags and whether to use a HAC estimator for the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f81c2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def estimate_gmm(\n",
    "    data,\n",
    "    n_lags,\n",
    "    start_params=None,\n",
    "    use_hac=True,\n",
    "    hac_maxlag=None,\n",
    "    maxiter=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate Euler-equation parameters with nonlinear IV-GMM.\n",
    "    \"\"\"\n",
    "    if start_params is None:\n",
    "        start_params = np.array([1.0, 0.99])\n",
    "\n",
    "    endog, exog, instruments = build_gmm_arrays(data, n_lags)\n",
    "    model = gmm.NonlinearIVGMM(endog, exog, instruments, euler_error)\n",
    "\n",
    "    if use_hac:\n",
    "        if hac_maxlag is None:\n",
    "            hac_maxlag = max(\n",
    "                1, int(\n",
    "                    np.floor(4.0 * (endog.shape[0] / 100.0) ** (2.0 / 9.0))))\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=maxiter,\n",
    "            optim_method=\"bfgs\",\n",
    "            optim_args={\"disp\": False},\n",
    "            weights_method=\"hac\",\n",
    "            wargs={\"maxlag\": hac_maxlag},\n",
    "        )\n",
    "    else:\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=maxiter,\n",
    "            optim_method=\"bfgs\",\n",
    "            optim_args={\"disp\": False},\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce80098",
   "metadata": {},
   "source": [
    "Next we include a helper to run GMM estimation across lag lengths and summarize results in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c2c28",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def run_gmm_by_lag(\n",
    "    data,\n",
    "    lags=(1, 2, 4, 6),\n",
    "    use_hac=True,\n",
    "    hac_maxlag=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate GMM models by lag length and return a summary table.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    results = {}\n",
    "\n",
    "    for lag in lags:\n",
    "        res = estimate_gmm(data, n_lags=lag, use_hac=use_hac, hac_maxlag=hac_maxlag)\n",
    "        results[lag] = res\n",
    "        j_stat, j_pval, j_df = res.jtest()\n",
    "        j_prob = float(stats.chi2.cdf(j_stat, df=j_df)) if j_df > 0 else np.nan\n",
    "        rows.append(\n",
    "            {\n",
    "                \"NLAG\": lag,\n",
    "                \"γ_hat\": res.params[0],\n",
    "                \"se_γ\": res.bse[0],\n",
    "                \"β_hat\": res.params[1],\n",
    "                \"se_β\": res.bse[1],\n",
    "                \"j_stat\": j_stat,\n",
    "                \"j_prob\": j_prob,\n",
    "                \"j_pval\": j_pval,\n",
    "                \"j_df\": int(j_df),\n",
    "                \"n_obs\": int(res.nobs),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    table = pd.DataFrame(rows).set_index(\"NLAG\")\n",
    "    return table, results\n",
    "\n",
    "\n",
    "def run_two_step_by_lag(\n",
    "    data,\n",
    "    lags=(1, 2, 4, 6),\n",
    "    horizon=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Two-step GMM with exact S0 (MA order 0) across lag lengths.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    start_params = None\n",
    "    for lag in lags:\n",
    "        res = two_step_gmm(\n",
    "            data,\n",
    "            n_lags=lag,\n",
    "            ma_order=0,\n",
    "            horizon=horizon,\n",
    "            start_params=start_params,\n",
    "        )\n",
    "        start_params = res[\"params_step2\"]\n",
    "        rows.append(\n",
    "            {\n",
    "                \"NLAG\": lag,\n",
    "                \"γ_hat\": res[\"params_step2\"][0],\n",
    "                \"se_γ\": res[\"se_step2\"][0],\n",
    "                \"β_hat\": res[\"params_step2\"][1],\n",
    "                \"se_β\": res[\"se_step2\"][1],\n",
    "                \"j_stat\": res[\"j_stat\"],\n",
    "                \"j_prob\": res[\"j_prob\"],\n",
    "                \"j_pval\": res[\"j_pval\"],\n",
    "                \"j_df\": res[\"j_df\"],\n",
    "                \"n_obs\": res[\"n_obs\"],\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows).set_index(\"NLAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d86a4",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Before applying the estimator to real data, we verify that GMM recovers known parameters from simulated data.\n",
    "\n",
    "We build a simulator that generates synthetic return-growth pairs satisfying the Euler equation by construction.\n",
    "\n",
    "We generate log consumption growth from a stationary AR(1), compute the stochastic discount factor at known true parameters, and construct gross returns as\n",
    "$ R_{t+1} = \\xi_{t+1} / M_{t+1}(\\theta_0) $ where $ \\xi_{t+1} $ is an iid lognormal shock with mean one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ccae8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def _ar1_simulate(mu_c, phi_c, sigma_c, shocks_c, total_n):\n",
    "    \"\"\"\n",
    "    Simulate AR(1) log consumption growth.\n",
    "    \"\"\"\n",
    "    delta_c = np.empty(total_n)\n",
    "    delta_c[0] = mu_c\n",
    "    for t in range(1, total_n):\n",
    "        delta_c[t] = mu_c * (1.0 - phi_c) + phi_c * delta_c[t - 1] + sigma_c * shocks_c[t]\n",
    "    return delta_c\n",
    "\n",
    "\n",
    "def simulate_euler_sample(\n",
    "    n_obs,\n",
    "    γ_true=0.8,\n",
    "    β_true=0.993,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate [gross real return, gross consumption growth].\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mu_c = 0.0015\n",
    "    sigma_c = 0.006\n",
    "    phi_c = 0.4\n",
    "    sigma_eta = 0.02\n",
    "    burn_in = 200\n",
    "\n",
    "    total_n = n_obs + burn_in\n",
    "    shocks_c = rng.standard_normal(total_n)\n",
    "    delta_c = _ar1_simulate(mu_c, phi_c, sigma_c, shocks_c, total_n)\n",
    "\n",
    "    cons_growth = np.exp(delta_c[burn_in:])\n",
    "    sdf = β_true * cons_growth ** (-γ_true)\n",
    "\n",
    "    # Positive mean-one return shock: E[ξ]=1 so E[M R]=1 by construction.\n",
    "    eps = rng.standard_normal(n_obs)\n",
    "    xi = np.exp(sigma_eta * eps - 0.5 * sigma_eta**2)\n",
    "    gross_return = xi / sdf\n",
    "\n",
    "    return np.column_stack([gross_return, cons_growth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935929fe",
   "metadata": {},
   "source": [
    "We set $ \\gamma = 2 $ and $ \\beta = 0.995 $ as the true parameters and generate 700 monthly observations from the Euler-consistent DGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5319f02",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "γ_true = 2.0\n",
    "β_true = 0.995\n",
    "sim_data = simulate_euler_sample(\n",
    "    n_obs=5000,\n",
    "    γ_true=γ_true,\n",
    "    β_true=β_true,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print(f\"Simulation sample size: {sim_data.shape[0]}\")\n",
    "print(f\"True γ: {γ_true:.3f}\")\n",
    "print(f\"True β: {β_true:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10281001",
   "metadata": {},
   "source": [
    "We now estimate GMM across lag lengths, following the format of Table I in Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff059a18",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sim_table = run_two_step_by_lag(sim_data, lags=(1, 2, 4, 6), horizon=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273dea8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sim_pretty = sim_table[\n",
    "    [\"γ_hat\", \"se_γ\", \"β_hat\", \"se_β\", \"j_stat\", \"j_df\", \"j_prob\"]].rename(\n",
    "    columns={\n",
    "        \"γ_hat\": r\"\\hat{\\gamma}\",\n",
    "        \"se_γ\": r\"\\mathrm{se}(\\hat{\\gamma})\",\n",
    "        \"β_hat\": r\"\\hat{\\beta}\",\n",
    "        \"se_β\": r\"\\mathrm{se}(\\hat{\\beta})\",\n",
    "        \"j_stat\": \"J\",\n",
    "        \"j_df\": \"df\",\n",
    "        \"j_prob\": \"Prob(J)\",\n",
    "    }\n",
    ")\n",
    "display_table(\n",
    "    sim_pretty,\n",
    "    fmt={\n",
    "        r\"\\hat{\\gamma}\": \"{:.4f}\",\n",
    "        r\"\\mathrm{se}(\\hat{\\gamma})\": \"{:.4f}\",\n",
    "        r\"\\hat{\\beta}\": \"{:.4f}\",\n",
    "        r\"\\mathrm{se}(\\hat{\\beta})\": \"{:.4f}\",\n",
    "        \"J\": \"{:.3f}\",\n",
    "        \"Prob(J)\": \"{:.3f}\",\n",
    "        \"df\": \"{:.0f}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d5729",
   "metadata": {},
   "source": [
    "GMM recovers the true $ \\gamma $ and $ \\beta $ across lag specifications pretty closely.\n",
    "\n",
    "For hypothesis testing, the right-tail $ p $ value is $ 1-\\mathrm{Prob}(J) $.\n",
    "\n",
    "The large standard errors visible in both papers’ tables suggest that the preference parameters $ \\gamma $ and $ \\beta $ can be weakly identified.\n",
    "\n",
    "To visualize this, we plot the GMM criterion over a $ (\\gamma, \\beta) $ grid using the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59aeee",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def gmm_objective_surface(\n",
    "    data,\n",
    "    n_lags=2,\n",
    "    γ_grid=None,\n",
    "    β_grid=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute identity-weighted GMM objective on a parameter grid.\n",
    "    \"\"\"\n",
    "    _, exog, instruments = build_gmm_arrays(data, n_lags)\n",
    "\n",
    "    if γ_grid is None:\n",
    "        γ_grid = np.linspace(-1.0, 8.0, 70)\n",
    "    if β_grid is None:\n",
    "        β_grid = np.linspace(0.96, 1.02, 70)\n",
    "\n",
    "    objective = np.empty((len(β_grid), len(γ_grid)))\n",
    "\n",
    "    for i, β_val in enumerate(β_grid):\n",
    "        for j, γ_val in enumerate(γ_grid):\n",
    "            err = euler_error(np.array([γ_val, β_val]), exog)\n",
    "            moments = (err[:, None] * instruments).mean(axis=0)\n",
    "            objective[i, j] = moments @ moments\n",
    "\n",
    "    return γ_grid, β_grid, objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb4a47",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "γ_grid, β_grid, objective = gmm_objective_surface(sim_data, n_lags=2)\n",
    "log_obj = np.log10(objective + 1e-12)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "contours = ax.contourf(γ_grid, β_grid, log_obj, levels=30, cmap=\"viridis\")\n",
    "ax.set_xlabel(r\"$\\gamma$\")\n",
    "ax.set_ylabel(r\"$\\beta$\")\n",
    "ax.plot(γ_true, β_true, \"k*\", ms=12, lw=2, label=\"true values\")\n",
    "ax.legend()\n",
    "plt.colorbar(contours, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7aac6",
   "metadata": {},
   "source": [
    "The criterion surface may have elongated valleys where many parameter combinations fit the moments nearly equally well.\n",
    "\n",
    "To illustrate the multi-period case from Section 2 of Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)], we estimate the three-period Euler restriction using overlapping-horizon returns and consumption growth, with instruments formed from one-period data dated $ t $ or earlier and the finite-order covariance appropriate for MA(2) disturbances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c5c0f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "horizon_n = 3\n",
    "two_step = two_step_gmm(\n",
    "    sim_data,\n",
    "    n_lags=2,\n",
    "    ma_order=horizon_n - 1,\n",
    "    horizon=horizon_n,\n",
    ")\n",
    "\n",
    "print(f\"Horizon n: {horizon_n}\")\n",
    "print(f\"Step-2 converged: {two_step['success']}\")\n",
    "print(f\"Step-2 gamma: {two_step['params_step2'][0]:.4f}\")\n",
    "print(f\"Step-2 beta (one-period): {two_step['params_step2'][1]:.4f}\")\n",
    "print(\n",
    "    f\"J({two_step['j_df']}): {two_step['j_stat']:.3f}, \"\n",
    "    f\"Prob={two_step['j_prob']:.3f}, p={two_step['j_pval']:.3f}\"\n",
    ")\n",
    "\n",
    "_, exog_n, _ = build_gmm_arrays_horizon(sim_data, n_lags=2, horizon=horizon_n)\n",
    "acf_n = acf(\n",
    "    euler_error_horizon(two_step[\"params_step2\"], exog_n, horizon=horizon_n),\n",
    "    nlags=6,\n",
    "    fft=True,\n",
    ")\n",
    "print(\"Euler-error ACF lags 1-3:\", \", \".join([f\"{v:.3f}\" for v in acf_n[1:4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230bece",
   "metadata": {},
   "source": [
    "The low-lag ACF is consistent with the MA(2) dependence implied by the 3-period horizon.\n",
    "\n",
    "We now run a Monte Carlo exercise with 500 replications to visualize the finite-sample distribution of $ \\hat\\gamma $, $ \\hat\\beta $, and the $ J $ statistic and verify that the asymptotic theory from Section 3 of Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] provides a reasonable approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded53730",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_rep = 500\n",
    "estimates = []\n",
    "j_stats = []\n",
    "\n",
    "for rep in range(n_rep):\n",
    "    rep_data = simulate_euler_sample(\n",
    "        n_obs=900,\n",
    "        γ_true=γ_true,\n",
    "        β_true=β_true,\n",
    "        seed=rep,\n",
    "    )\n",
    "    rep_res = estimate_gmm(rep_data, n_lags=2, use_hac=True, maxiter=2)\n",
    "    estimates.append(rep_res.params)\n",
    "    j_stats.append(rep_res.jval)\n",
    "\n",
    "estimates = np.asarray(estimates)\n",
    "j_stats = np.asarray(j_stats)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].hist(estimates[:, 0], bins=20, edgecolor=\"white\")\n",
    "axes[0].axvline(γ_true, color=\"red\", ls=\"--\", lw=2)\n",
    "axes[0].set_xlabel(r\"$\\hat{\\gamma}$\")\n",
    "axes[1].hist(estimates[:, 1], bins=20, edgecolor=\"white\")\n",
    "axes[1].axvline(β_true, color=\"red\", ls=\"--\", lw=2)\n",
    "axes[1].set_xlabel(r\"$\\hat{\\beta}$\")\n",
    "\n",
    "df_j = 2 * 2 + 1 - 2\n",
    "axes[2].hist(j_stats, bins=20, density=True, edgecolor=\"white\")\n",
    "grid = np.linspace(0.0, max(j_stats.max(), 1.0), 200)\n",
    "axes[2].plot(grid, stats.chi2.pdf(grid, df_j), \"r-\", lw=2)\n",
    "axes[2].set_xlabel(\"j-statistic\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766b98b",
   "metadata": {},
   "source": [
    "Both $ \\hat\\gamma $ and $ \\hat\\beta $ are centered near their true values, and the $ J $ histogram tracks the $ \\chi^2 $ density, supporting the asymptotic approximation at this sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df902102",
   "metadata": {},
   "source": [
    "## Empirical GMM estimation\n",
    "\n",
    "We now apply GMM to observed data, following the empirical strategy of Section 5 in Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)].\n",
    "\n",
    "Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] use monthly per capita consumption of nondurables (ND) and nondurables plus services (NDS) paired with the equally-weighted (EWR) and value-weighted (VWR) aggregate stock returns from CRSP, for 1959:2 through 1978:12.\n",
    "\n",
    "We focus on their ND+VWR specification using FRED nondurables consumption and the Ken French value-weighted market return as a proxy for CRSP, on the same 1959:2–1978:12 sample period.\n",
    "\n",
    "Because the Ken French return is not identical to the original CRSP NYSE value-weighted return, we only want to match the paper qualitatively.\n",
    "\n",
    "Both this lecture and the companion lecture [Stochastic Consumption, Risk Aversion, and the Temporal Behavior of Asset Returns](https://python.quantecon.org/hansen_singleton_1983.html) use the same data construction.\n",
    "\n",
    "The hidden cell below pulls the relevant FRED series, constructs per capita real consumption, and joins with Ken French market returns via `pandas-datareader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577db8b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fred_codes = {\n",
    "    \"population_16plus\": \"CNP16OV\",\n",
    "    \"cons_nd_real_index\": \"DNDGRA3M086SBEA\",\n",
    "    \"cons_nd_price_index\": \"DNDGRG3M086SBEA\",\n",
    "}\n",
    "\n",
    "def to_month_end(index):\n",
    "    \"\"\"\n",
    "    Convert a date index to month-end timestamps.\n",
    "    \"\"\"\n",
    "    return pd.PeriodIndex(pd.DatetimeIndex(index), freq=\"M\").to_timestamp(\"M\")\n",
    "\n",
    "\n",
    "def load_hs_monthly_data(\n",
    "    start=\"1959-02-01\",\n",
    "    end=\"1978-12-01\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build monthly gross real return and gross consumption-growth series.\n",
    "    \"\"\"\n",
    "    start_period = pd.Timestamp(start).to_period(\"M\")\n",
    "    end_period = pd.Timestamp(end).to_period(\"M\")\n",
    "\n",
    "    # Pull one extra month to build the first in-sample growth rate.\n",
    "    fetch_start = (start_period - 1).to_timestamp(how=\"start\")\n",
    "    fetch_end = end_period.to_timestamp(\"M\")\n",
    "    sample_start = start_period.to_timestamp(\"M\")\n",
    "    sample_end = end_period.to_timestamp(\"M\")\n",
    "\n",
    "    fred = web.DataReader(\n",
    "        list(fred_codes.values()), \"fred\", fetch_start, fetch_end)\n",
    "    fred = fred.rename(columns={v: k for k, v in fred_codes.items()})\n",
    "    fred.index = to_month_end(fred.index)\n",
    "    fred[\"cons_real_level\"] = fred[\"cons_nd_real_index\"]\n",
    "    fred[\"cons_price_index\"] = fred[\"cons_nd_price_index\"]\n",
    "    fred[\"consumption_per_capita\"] = fred[\"cons_real_level\"] \\\n",
    "        / fred[\"population_16plus\"]\n",
    "    fred[\"gross_cons_growth\"] = (\n",
    "        fred[\"consumption_per_capita\"] \n",
    "        / fred[\"consumption_per_capita\"].shift(1)\n",
    "    )\n",
    "    fred[\"gross_inflation_cons\"] = (\n",
    "        fred[\"cons_price_index\"] / fred[\"cons_price_index\"].shift(1)\n",
    "    )\n",
    "\n",
    "    ff = web.DataReader(\n",
    "        \"F-F_Research_Data_Factors\", \"famafrench\",\n",
    "        fetch_start, fetch_end)[0].copy()\n",
    "    ff.columns = [str(col).strip() for col in ff.columns]\n",
    "    if (\"Mkt-RF\" not in ff.columns) or (\"RF\" not in ff.columns):\n",
    "        raise KeyError(\n",
    "            \"Fama-French data missing required columns: 'Mkt-RF' and 'RF'.\")\n",
    "\n",
    "    # Mkt-RF and RF are reported in percent per month.\n",
    "    ff[\"gross_nom_return\"] = 1.0 + (ff[\"Mkt-RF\"] + ff[\"RF\"]) / 100.0\n",
    "    ff.index = ff.index.to_timestamp(how=\"end\")\n",
    "    ff.index = to_month_end(ff.index)\n",
    "    market = ff[[\"gross_nom_return\"]]\n",
    "\n",
    "    out = fred.join(market, how=\"inner\")\n",
    "    out[\"gross_real_return\"] = out[\"gross_nom_return\"] \\\n",
    "        / out[\"gross_inflation_cons\"]\n",
    "    out = out.loc[sample_start:sample_end].dropna()\n",
    "\n",
    "    required_cols = [\n",
    "        \"gross_real_return\",\n",
    "        \"gross_cons_growth\",\n",
    "    ]\n",
    "    return out[required_cols].copy()\n",
    "\n",
    "\n",
    "def get_estimation_data(\n",
    "    start=\"1959-02-01\",\n",
    "    end=\"1978-12-01\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Return (dataframe, array) using observed data.\n",
    "    \"\"\"\n",
    "    frame = load_hs_monthly_data(start=start, end=end)\n",
    "    data = frame[[\"gross_real_return\", \"gross_cons_growth\"]].to_numpy()\n",
    "    return frame, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1163b41",
   "metadata": {},
   "source": [
    "We first examine the raw data moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69f60c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "LAGS = (1, 2, 4, 6)\n",
    "\n",
    "emp_frame, emp_data = get_estimation_data()\n",
    "\n",
    "print(f\"Mean net real return: {(emp_data[:, 0].mean() - 1.0) * 100:.3f}%\")\n",
    "print(f\"Std net real return: {emp_data[:, 0].std() * 100:.3f}%\")\n",
    "print(f\"Mean net consumption growth: {(emp_data[:, 1].mean() - 1.0) * 100:.3f}%\")\n",
    "print(f\"Std net consumption growth: {emp_data[:, 1].std() * 100:.3f}%\")\n",
    "print(f\"Std log consumption growth: {np.log(emp_data[:, 1]).std() * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c040bac",
   "metadata": {},
   "source": [
    "One feature of these data is the large gap between the volatility of returns and the volatility of consumption growth.\n",
    "\n",
    "This is again an empirical fact underlying the equity premium puzzle of Mehra and Prescott [[1985](https://python.quantecon.org/zreferences.html#id218)]: matching the observed equity premium with CRRA preferences requires implausibly high risk aversion.\n",
    "\n",
    "We now estimate the Euler equation using the two-step generalized instrumental variables (GIV) / GMM procedure in Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)].\n",
    "\n",
    "For the one-period stock-return Euler equation ($ n=1 $), the disturbance is a martingale difference sequence, so the optimal weighting matrix uses the contemporaneous covariance $ S_0 = E[m_t m_t^\\top] $.\n",
    "\n",
    "To match Table I, we report the paper’s exponent parameter $ \\alpha $ in\n",
    "$ E_t[\\beta (C_{t+1}/C_t)^\\alpha R_{t+1} - 1] = 0 $.\n",
    "\n",
    "The two-step GMM estimates of $ \\hat{\\alpha} $ and $ \\hat{\\beta} $ by lag length are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e8999",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "gmm_raw = run_two_step_by_lag(emp_data, lags=LAGS, horizon=1)\n",
    "gmm_raw.index.name = \"NLAG\"\n",
    "\n",
    "table_i = pd.DataFrame(index=gmm_raw.index)\n",
    "table_i.index.name = \"NLAG\"\n",
    "table_i[r\"\\hat{\\alpha}\"] = -gmm_raw[\"γ_hat\"]\n",
    "table_i[r\"SE(\\hat{\\alpha})\"] = gmm_raw[\"se_γ\"]\n",
    "table_i[r\"\\beta\"] = gmm_raw[\"β_hat\"]\n",
    "table_i[r\"\\mathrm{SE}(\\beta)\"] = gmm_raw[\"se_β\"]\n",
    "table_i[r\"\\chi^2\"] = gmm_raw[\"j_stat\"]\n",
    "table_i[\"DF\"] = gmm_raw[\"j_df\"]\n",
    "table_i[\"Prob\"] = gmm_raw[\"j_prob\"]\n",
    "\n",
    "display_table(\n",
    "    table_i,\n",
    "    fmt={\n",
    "        r\"\\hat{\\alpha}\": \"{:.4f}\",\n",
    "        r\"SE(\\hat{\\alpha})\": \"{:.4f}\",\n",
    "        r\"\\beta\": \"{:.4f}\",\n",
    "        r\"\\mathrm{SE}(\\beta)\": \"{:.4f}\",\n",
    "        r\"\\chi^2\": \"{:.4f}\",\n",
    "        \"DF\": \"{:.0f}\",\n",
    "        \"Prob\": \"{:.4f}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fba2ab",
   "metadata": {},
   "source": [
    "For comparison, Table I of Hansen and Singleton [[1982](https://python.quantecon.org/zreferences.html#id255)] (as corrected in the [1984 *Econometrica* errata](https://www.jstor.org/stable/1911486?seq=2)) reports the following ND+VWR values for 1959:2–1978:12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f3314",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "table_i_paper = pd.DataFrame(\n",
    "    {\n",
    "        r\"\\alpha\": [-1.2028, -0.5761, -0.6565, -0.9638],\n",
    "        r\"SE(\\alpha)\": [0.7789, 0.7067, 0.6896, 0.6425],\n",
    "        r\"\\beta\": [0.9976, 0.9975, 0.9978, 0.9985],\n",
    "        r\"\\mathrm{SE}(\\beta)\": [0.0027, 0.0027, 0.0027, 0.0027],\n",
    "        r\"\\chi^2\": [1.457, 5.819, 7.923, 10.522],\n",
    "        \"DF\": [1, 3, 7, 11],\n",
    "        \"Prob\": [0.7726, 0.8792, 0.6606, 0.5159],\n",
    "    },\n",
    "    index=pd.Index([1, 2, 4, 6], name=\"NLAG\"),\n",
    ")\n",
    "\n",
    "display_table(\n",
    "    table_i_paper,\n",
    "    fmt={\n",
    "        r\"\\alpha\": \"{:.4f}\",\n",
    "        r\"SE(\\alpha)\": \"{:.4f}\",\n",
    "        r\"\\beta\": \"{:.4f}\",\n",
    "        r\"\\mathrm{SE}(\\beta)\": \"{:.4f}\",\n",
    "        r\"\\chi^2\": \"{:.4f}\",\n",
    "        \"DF\": \"{:.0f}\",\n",
    "        \"Prob\": \"{:.4f}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cff64a",
   "metadata": {},
   "source": [
    "They are very close to our estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e7c79",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The GMM estimator requires only the orthogonality conditions implied by the Euler equation and a set of conditioning variables.\n",
    "\n",
    "It does not require assumptions about the joint distribution of consumption and returns, the production technology, or any other part of the economic environment beyond the representative agent’s first-order conditions.\n",
    "\n",
    "So GMM is provides a way to estimate some objects of interest while not estimating all of the parameters that Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] estimated in their loglinear model of consumption growth and returns.\n",
    "\n",
    "- If the complete model of Hansen and Singleton [[1983](https://python.quantecon.org/zreferences.html#id254)] is correctly specified, then their maximum likelihood estimators promise to be more efficient that are the GMM estimators described in this lecture.  \n",
    "- The theme of estimating *something* while not estimating *everything* runs through much of Lars Peter Hansen’s work. See Hansen [[2014](https://python.quantecon.org/zreferences.html#id4)]  "
   ]
  }
 ],
 "metadata": {
  "date": 1772080447.9016333,
  "filename": "hansen_singleton_1982.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Estimating Euler Equations by Generalized Method of Moments"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}