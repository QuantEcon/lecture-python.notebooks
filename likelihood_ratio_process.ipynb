{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf079d5",
   "metadata": {},
   "source": [
    "\n",
    "<a id='likelihood-ratio-process'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ca931",
   "metadata": {},
   "source": [
    "# Likelihood Ratio Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6b112",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Likelihood Ratio Processes](#Likelihood-Ratio-Processes)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Likelihood Ratio Process](#Likelihood-Ratio-Process)  \n",
    "  - [Nature Permanently Draws from Density g](#Nature-Permanently-Draws-from-Density-g)  \n",
    "  - [Peculiar Property](#Peculiar-Property)  \n",
    "  - [Nature Permanently Draws from Density f](#Nature-Permanently-Draws-from-Density-f)  \n",
    "  - [Likelihood Ratio Test](#Likelihood-Ratio-Test)  \n",
    "  - [Kullback–Leibler Divergence](#Kullback–Leibler-Divergence)  \n",
    "  - [Heterogeneous Beliefs and Financial Markets](#Heterogeneous-Beliefs-and-Financial-Markets)  \n",
    "  - [Hypothesis Testing and Classification](#Hypothesis-Testing-and-Classification)  \n",
    "  - [Measuring discrepancies between distributions](#Measuring-discrepancies-between-distributions)  \n",
    "  - [Related Lectures](#Related-Lectures)  \n",
    "  - [Exercises](#Exercises)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350b7bb",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture describes likelihood ratio processes and some of their uses.\n",
    "\n",
    "We’ll study the same  setting that is also used in  [this lecture on exchangeability](https://python.quantecon.org/exchangeable.html).\n",
    "\n",
    "Among the things that we’ll learn are\n",
    "\n",
    "- How a likelihood ratio process is a key ingredient in frequentist hypothesis testing  \n",
    "- How a **receiver operator characteristic curve** summarizes information about a false alarm probability and power in frequentist hypothesis testing  \n",
    "- How a statistician can combine frequentist probabilities of type I and type II errors to form posterior probabilities of mistakes in a model selection or in an individual-classification problem  \n",
    "- How likelihood ratios helped Lawrence Blume and David Easley formulate an answer to  ‘‘If you’re so smart, why aren’t you rich?’’ [[Blume and Easley, 2006](https://python.quantecon.org/zreferences.html#id5)]  \n",
    "- How to use a Kullback-Leibler divergence to quantify the difference between two probability distributions with the same support  \n",
    "- How during World War II the United States Navy devised a decision rule for doing quality control on lots of ammunition, a topic that sets the stage for [this lecture](https://python.quantecon.org/wald_friedman.html)  \n",
    "- A peculiar property of likelihood ratio processes  \n",
    "\n",
    "\n",
    "Let’s start by importing some Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064e046",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import brentq, minimize_scalar\n",
    "from scipy.stats import beta as beta_dist\n",
    "import pandas as pd\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20bf87",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Process\n",
    "\n",
    "A nonnegative random variable $ W $ has one of two probability density functions, either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "Before the beginning of time, nature once and for all decides whether she will draw a sequence of IID draws from either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "We will sometimes let $ q $ be the density that nature chose once and for all, so\n",
    "that $ q $ is either $ f $ or $ g $, permanently.\n",
    "\n",
    "Nature knows which density it permanently draws from, but we the observers do not.\n",
    "\n",
    "We know both $ f $ and $ g $ but we don’t know which density nature\n",
    "chose.\n",
    "\n",
    "But we want to know.\n",
    "\n",
    "To do that, we use observations.\n",
    "\n",
    "We observe a sequence $ \\{w_t\\}_{t=1}^T $ of $ T $ IID draws that we know came from either $ f $ or $ g $.\n",
    "\n",
    "We want to use these observations to infer whether nature chose $ f $ or $ g $.\n",
    "\n",
    "A **likelihood ratio process** is a useful tool for this task.\n",
    "\n",
    "To begin, we define a key component of a likelihood ratio process, namely, the time $ t $ likelihood ratio as the random variable\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "We assume that $ f $ and $ g $ both put positive probabilities on the\n",
    "same intervals of possible realizations of the random variable $ W $.\n",
    "\n",
    "That means that under the $ g $ density, $ \\ell (w_t)=\n",
    "\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)} $\n",
    "is a nonnegative random variable with mean $ 1 $.\n",
    "\n",
    "A **likelihood ratio process** for sequence\n",
    "$ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ is defined as\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "where $ w^t=\\{ w_1,\\dots,w_t\\} $ is a history of\n",
    "observations up to and including time $ t $.\n",
    "\n",
    "Sometimes for shorthand we’ll write $ L_t = L(w^t) $.\n",
    "\n",
    "Notice that the likelihood process satisfies the *recursion*\n",
    "\n",
    "$$\n",
    "L(w^t) = \\ell (w_t) L (w^{t-1}) .\n",
    "$$\n",
    "\n",
    "The likelihood ratio and its logarithm are key tools for making\n",
    "inferences using a classic frequentist approach due to Neyman and\n",
    "Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id259)].\n",
    "\n",
    "To help us appreciate how things work, the following Python code evaluates $ f $ and $ g $ as two different\n",
    "Beta distributions, then computes and simulates an associated likelihood\n",
    "ratio process by generating a sequence $ w^t $ from one of the two\n",
    "probability distributions, for example, a sequence of IID draws from $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3513e8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters in the two Beta distributions.\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# The two density functions.\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12e1ee",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate(a, b, T=50, N=500):\n",
    "    '''\n",
    "    Generate N sets of T observations of the likelihood ratio,\n",
    "    return as N x T matrix.\n",
    "    '''\n",
    "\n",
    "    l_arr = np.empty((N, T))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(T):\n",
    "            w = np.random.beta(a, b)\n",
    "            l_arr[i, j] = f(w) / g(w)\n",
    "\n",
    "    return l_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a86a6",
   "metadata": {},
   "source": [
    "\n",
    "<a id='nature-likeli'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a1c33",
   "metadata": {},
   "source": [
    "## Nature Permanently Draws from Density g\n",
    "\n",
    "We first simulate the likelihood ratio process when nature permanently\n",
    "draws from $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006a119",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b25df",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_g.shape\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    plt.plot(range(T), l_seq_g[i, :], color='b', lw=0.8, alpha=0.5)\n",
    "\n",
    "plt.ylim([0, 3])\n",
    "plt.title(\"$L(w^{t})$ paths\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52597eca",
   "metadata": {},
   "source": [
    "Evidently, as sample length $ T $ grows, most probability mass\n",
    "shifts toward zero\n",
    "\n",
    "To see this more clearly, we plot over time the fraction of\n",
    "paths $ L\\left(w^{t}\\right) $ that fall in the interval\n",
    "$ \\left[0, 0.01\\right] $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76366b1c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(T), np.sum(l_seq_g <= 0.01, axis=0) / N)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d53f3",
   "metadata": {},
   "source": [
    "Despite the evident convergence of most probability mass to a\n",
    "very small interval near $ 0 $, the unconditional mean of\n",
    "$ L\\left(w^t\\right) $ under probability density $ g $ is\n",
    "identically $ 1 $ for all $ t $.\n",
    "\n",
    "To verify this assertion, first notice that as mentioned earlier the unconditional mean\n",
    "$ E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right] $ is $ 1 $ for\n",
    "all $ t $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right]  &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=\\int f\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=1,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which immediately implies\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[L\\left(w^{1}\\right)\\bigm|q=g\\right]  &=E\\left[\\ell \\left(w_{1}\\right)\\bigm|q=g\\right]\\\\\n",
    "    &=1.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because $ L(w^t) = \\ell(w_t) L(w^{t-1}) $ and\n",
    "$ \\{w_t\\}_{t=1}^t $ is an IID sequence, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]  &=E\\left[L\\left(w^{t-1}\\right)\\ell \\left(w_{t}\\right)\\bigm|q=g\\right] \\\\\n",
    "         &=E\\left[L\\left(w^{t-1}\\right)E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g,w^{t-1}\\right]\\bigm|q=g\\right] \\\\\n",
    "     &=E\\left[L\\left(w^{t-1}\\right)E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right]\\bigm|q=g\\right] \\\\\n",
    "    &=E\\left[L\\left(w^{t-1}\\right)\\bigm|q=g\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for any $ t \\geq 1 $.\n",
    "\n",
    "Mathematical induction implies\n",
    "$ E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]=1 $ for all\n",
    "$ t \\geq 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7dfb7a",
   "metadata": {},
   "source": [
    "## Peculiar Property\n",
    "\n",
    "How can $ E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]=1 $ possibly be true when most probability mass of the likelihood\n",
    "ratio process is piling up near $ 0 $ as\n",
    "$ t \\rightarrow + \\infty $?\n",
    "\n",
    "The answer is that as $ t \\rightarrow + \\infty $, the\n",
    "distribution of $ L_t $ becomes more and more fat-tailed:\n",
    "enough mass shifts to larger and larger values of $ L_t $ to make\n",
    "the mean of $ L_t $ continue to be one despite most of the probability mass piling up\n",
    "near $ 0 $.\n",
    "\n",
    "To illustrate this peculiar property, we simulate many paths and\n",
    "calculate the unconditional mean of $ L\\left(w^t\\right) $ by\n",
    "averaging across these many paths at each $ t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5be18",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b, N=50000)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8955",
   "metadata": {},
   "source": [
    "It would be useful to use simulations to verify that unconditional means\n",
    "$ E\\left[L\\left(w^{t}\\right)\\right] $ equal unity by averaging across sample\n",
    "paths.\n",
    "\n",
    "But it would be too computer-time-consuming for us to do that here simply by applying a standard Monte Carlo simulation approach.\n",
    "\n",
    "The reason is that the distribution of $ L\\left(w^{t}\\right) $ is extremely skewed for large values of $ t $.\n",
    "\n",
    "Because the probability density in the right tail is close to $ 0 $, it just takes too much computer time to sample enough points from the right tail.\n",
    "\n",
    "We explain the problem in more detail in [this lecture](https://python.quantecon.org/imp_sample.html).\n",
    "\n",
    "There we describe an alternative way to compute the mean of a likelihood ratio by computing the mean of a *different* random variable by sampling from a *different* probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768cb635",
   "metadata": {},
   "source": [
    "## Nature Permanently Draws from Density f\n",
    "\n",
    "Now suppose that before time $ 0 $ nature permanently decided to draw repeatedly from density $ f $.\n",
    "\n",
    "While the mean of the likelihood ratio $ \\ell \\left(w_{t}\\right) $ under density\n",
    "$ g $ is $ 1 $, its mean under the density $ f $ exceeds one.\n",
    "\n",
    "To see this, we compute\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=f\\right]  &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}f\\left(w_{t}\\right)dw_{t} \\\\\n",
    "     &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "     &=\\int \\ell \\left(w_{t}\\right)^{2}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "     &=E\\left[\\ell \\left(w_{t}\\right)^{2}\\mid q=g\\right] \\\\\n",
    "     &=E\\left[\\ell \\left(w_{t}\\right)\\mid q=g\\right]^{2}+Var\\left(\\ell \\left(w_{t}\\right)\\mid q=g\\right) \\\\\n",
    "     &>E\\left[\\ell \\left(w_{t}\\right)\\mid q=g\\right]^{2} = 1 \\\\\n",
    "       \\end{aligned}\n",
    "$$\n",
    "\n",
    "This in turn implies that the unconditional mean of the likelihood ratio process $ L(w^t) $\n",
    "diverges toward $ + \\infty $.\n",
    "\n",
    "Simulations below confirm this conclusion.\n",
    "\n",
    "Please note the scale of the $ y $ axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdef7a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_f = simulate(F_a, F_b, N=50000)\n",
    "l_seq_f = np.cumprod(l_arr_f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4973c83",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_f.shape\n",
    "plt.plot(range(T), np.mean(l_seq_f, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f57bc",
   "metadata": {},
   "source": [
    "We also plot the probability that $ L\\left(w^t\\right) $ falls into\n",
    "the interval $ [10000, \\infty) $ as a function of time and watch how\n",
    "fast probability mass diverges to $ +\\infty $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc8061",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(T), np.sum(l_seq_f > 10000, axis=0) / N)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c96c4",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Test\n",
    "\n",
    "We now describe how to employ the machinery\n",
    "of Neyman and Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id259)] to test the hypothesis that history $ w^t $ is generated by repeated\n",
    "IID draws from density $ f $.\n",
    "\n",
    "Denote $ q $ as the data generating process, so that\n",
    "$ q=f \\text{ or } g $.\n",
    "\n",
    "Upon observing a sample $ \\{W_i\\}_{i=1}^t $, we want to decide\n",
    "whether nature is drawing from $ g $ or from $ f $ by performing a (frequentist)\n",
    "hypothesis test.\n",
    "\n",
    "We specify\n",
    "\n",
    "- Null hypothesis $ H_0 $: $ q=f $,  \n",
    "- Alternative hypothesis $ H_1 $: $ q=g $.  \n",
    "\n",
    "\n",
    "Neyman and Pearson proved that the best way to test this hypothesis is to use a **likelihood ratio test** that takes the\n",
    "form:\n",
    "\n",
    "- accept $ H_0 $ if $ L(W^t) > c $,  \n",
    "- reject $ H_0 $ if $ L(W^t) < c $,  \n",
    "\n",
    "\n",
    "where $ c $ is a given discrimination threshold.\n",
    "\n",
    "Setting $ c =1 $ is a common choice.\n",
    "\n",
    "We’ll discuss consequences of other choices of $ c $ below.\n",
    "\n",
    "This test is *best* in the sense that it is **uniformly most powerful**.\n",
    "\n",
    "To understand what this means, we have to define probabilities of two important events that\n",
    "allow us to characterize a test associated with a given\n",
    "threshold $ c $.\n",
    "\n",
    "The two probabilities are:\n",
    "\n",
    "- Probability of a Type I error in which we reject $ H_0 $ when it is true:  \n",
    "  $$\n",
    "  \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\}\n",
    "  $$\n",
    "- Probability of a Type II error in which we accept $ H_0 $ when it is false:  \n",
    "  $$\n",
    "  \\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)>c\\mid q=g\\right\\}\n",
    "  $$\n",
    "\n",
    "\n",
    "These two probabilities underlie the following two concepts:\n",
    "\n",
    "- Probability of false alarm (= significance level = probability of\n",
    "  Type I error):  \n",
    "  $$\n",
    "  \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\}\n",
    "  $$\n",
    "- Probability of detection (= power = 1 minus probability\n",
    "  of Type II error):  \n",
    "  $$\n",
    "  1-\\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=g\\right\\}\n",
    "  $$\n",
    "\n",
    "\n",
    "The [Neyman-Pearson\n",
    "Lemma](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma)\n",
    "states that among all possible tests, a likelihood ratio test\n",
    "maximizes the probability of detection for a given probability of false\n",
    "alarm.\n",
    "\n",
    "Another way to say the same thing is that among all possible tests, a likelihood ratio test\n",
    "maximizes **power** for a given **significance level**.\n",
    "\n",
    "We want a small probability of\n",
    "false alarm and a large probability of detection.\n",
    "\n",
    "With sample size $ t $ fixed, we can change our two probabilities by\n",
    "adjusting $ c $.\n",
    "\n",
    "A troublesome “that’s life” fact is that these two probabilities move in the same direction as we vary the critical value\n",
    "$ c $.\n",
    "\n",
    "Without specifying quantitative losses from making Type I and Type II errors, there is little that we can say\n",
    "about how we *should* trade off probabilities of the two types of mistakes.\n",
    "\n",
    "We do know that increasing sample size $ t $ improves\n",
    "statistical inference.\n",
    "\n",
    "Below we plot some informative figures that illustrate this.\n",
    "\n",
    "We also present a classical frequentist method for choosing a sample\n",
    "size $ t $.\n",
    "\n",
    "Let’s start with a case in which we fix the threshold $ c $ at\n",
    "$ 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77fe27",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "c = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b556e",
   "metadata": {},
   "source": [
    "Below we plot empirical distributions of logarithms of the cumulative\n",
    "likelihood ratios simulated above, which are generated by either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "Taking logarithms has no effect on calculating the probabilities because\n",
    "the log is a monotonic transformation.\n",
    "\n",
    "As $ t $ increases, the probabilities of making Type I and Type II\n",
    "errors both decrease, which is good.\n",
    "\n",
    "This is because most of the probability mass of log$ (L(w^t)) $\n",
    "moves toward $ -\\infty $ when $ g $ is the data generating\n",
    "process, while log$ (L(w^t)) $ goes to\n",
    "$ \\infty $ when data are generated by $ f $.\n",
    "\n",
    "That disparate behavior of log$ (L(w^t)) $ under $ f $ and $ q $\n",
    "is what makes it possible eventually to distinguish\n",
    "$ q=f $ from $ q=g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5ccdf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('distribution of $log(L(w^t))$ under f or under g', fontsize=15)\n",
    "\n",
    "for i, t in enumerate([1, 7, 14, 21]):\n",
    "    nr = i // 2\n",
    "    nc = i % 2\n",
    "\n",
    "    axs[nr, nc].axvline(np.log(c), color=\"k\", ls=\"--\")\n",
    "\n",
    "    hist_f, x_f = np.histogram(np.log(l_seq_f[:, t]), 200, density=True)\n",
    "    hist_g, x_g = np.histogram(np.log(l_seq_g[:, t]), 200, density=True)\n",
    "\n",
    "    axs[nr, nc].plot(x_f[1:], hist_f, label=\"dist under f\")\n",
    "    axs[nr, nc].plot(x_g[1:], hist_g, label=\"dist under g\")\n",
    "\n",
    "    for i, (x, hist, label) in enumerate(zip([x_f, x_g], [hist_f, hist_g], [\"Type I error\", \"Type II error\"])):\n",
    "        ind = x[1:] <= np.log(c) if i == 0 else x[1:] > np.log(c)\n",
    "        axs[nr, nc].fill_between(x[1:][ind], hist[ind], alpha=0.5, label=label)\n",
    "\n",
    "    axs[nr, nc].legend()\n",
    "    axs[nr, nc].set_title(f\"t={t}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ede6b",
   "metadata": {},
   "source": [
    "In the above graphs,\n",
    "\n",
    "- the blue areas are related to but not equal to probabilities $ \\alpha $ of a type I error because\n",
    "  they are integrals of $ \\log L_t $, not integrals of $ L_t $, over rejection region $ L_t < 1 $  \n",
    "- the orange areas are related to but not equal to probabilities $ \\beta $ of a type II error because\n",
    "  they are integrals of $ \\log L_t $, not integrals of $ L_t $, over acceptance region $ L_t > 1 $  \n",
    "\n",
    "\n",
    "When we hold $ c $ fixed at $ c=1 $, the following graph shows that\n",
    "\n",
    "- the probability of detection monotonically increases with increases in\n",
    "  $ t $  \n",
    "- the probability of a false alarm monotonically decreases with increases in $ t $.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17f968",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PD = np.empty(T)\n",
    "PFA = np.empty(T)\n",
    "\n",
    "for t in range(T):\n",
    "    PD[t] = np.sum(l_seq_g[:, t] < c) / N\n",
    "    PFA[t] = np.sum(l_seq_f[:, t] < c) / N\n",
    "\n",
    "plt.plot(range(T), PD, label=\"Probability of detection\")\n",
    "plt.plot(range(T), PFA, label=\"Probability of false alarm\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.title(\"$c=1$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e9d86",
   "metadata": {},
   "source": [
    "For a given sample size $ t $, the threshold $ c $ uniquely pins down probabilities\n",
    "of both types of error.\n",
    "\n",
    "If for a fixed $ t $ we now free up and move $ c $, we will sweep out the probability\n",
    "of detection as a function of the probability of false alarm.\n",
    "\n",
    "This produces a [receiver operating characteristic\n",
    "curve (ROC curve)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "\n",
    "Below, we plot receiver operating characteristic curves for different\n",
    "sample sizes $ t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4708b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PFA = np.arange(0, 100, 1)\n",
    "\n",
    "for t in range(1, 15, 4):\n",
    "    percentile = np.percentile(l_seq_f[:, t], PFA)\n",
    "    PD = [np.sum(l_seq_g[:, t] < p) / N for p in percentile]\n",
    "\n",
    "    plt.plot(PFA / 100, PD, label=f\"t={t}\")\n",
    "\n",
    "plt.scatter(0, 1, label=\"perfect detection\")\n",
    "plt.plot([0, 1], [0, 1], color='k', ls='--', label=\"random detection\")\n",
    "\n",
    "plt.arrow(0.5, 0.5, -0.15, 0.15, head_width=0.03)\n",
    "plt.text(0.35, 0.7, \"better\")\n",
    "plt.xlabel(\"Probability of false alarm\")\n",
    "plt.ylabel(\"Probability of detection\")\n",
    "plt.legend()\n",
    "plt.title(\"Receiver Operating Characteristic Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac69c6b",
   "metadata": {},
   "source": [
    "Notice that as $ t $ increases, we are assured a larger probability\n",
    "of detection and a smaller probability of false alarm associated with\n",
    "a given discrimination threshold $ c $.\n",
    "\n",
    "For a given sample size $ t $, both $ \\alpha $ and $ \\beta $ change as we vary $ c $.\n",
    "\n",
    "As we increase $ c $\n",
    "\n",
    "- $ \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\} $ increases  \n",
    "- $ \\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)>c\\mid q=g\\right\\} $ decreases  \n",
    "\n",
    "\n",
    "As $ t \\rightarrow + \\infty $, we approach the perfect detection\n",
    "curve that is indicated by a right angle hinging on the blue dot.\n",
    "\n",
    "For a given sample size $ t $, the discrimination threshold $ c $ determines a point on the receiver operating\n",
    "characteristic curve.\n",
    "\n",
    "It is up to the test designer to trade off probabilities of\n",
    "making the two types of errors.\n",
    "\n",
    "But we know how to choose the smallest sample size to achieve given targets for\n",
    "the probabilities.\n",
    "\n",
    "Typically, frequentists aim for a high probability of detection that\n",
    "respects an upper bound on the probability of false alarm.\n",
    "\n",
    "Below we show an example in which we fix the probability of false alarm at\n",
    "$ 0.05 $.\n",
    "\n",
    "The required sample size for making a decision is then determined by a\n",
    "target probability of detection, for example, $ 0.9 $, as depicted in the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43bce0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PFA = 0.05\n",
    "PD = np.empty(T)\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    c = np.percentile(l_seq_f[:, t], PFA * 100)\n",
    "    PD[t] = np.sum(l_seq_g[:, t] < c) / N\n",
    "\n",
    "plt.plot(range(T), PD)\n",
    "plt.axhline(0.9, color=\"k\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Probability of detection\")\n",
    "plt.title(f\"Probability of false alarm={PFA}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2007398",
   "metadata": {},
   "source": [
    "The United States Navy evidently used a procedure like this to select a sample size $ t $ for doing quality\n",
    "control tests during World War II.\n",
    "\n",
    "A Navy Captain who had been ordered to perform tests of this kind had doubts about it that he\n",
    "presented to Milton Friedman, as we describe in [this lecture](https://python.quantecon.org/wald_friedman.html).\n",
    "\n",
    "\n",
    "<a id='rel-entropy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368d59a",
   "metadata": {},
   "source": [
    "## Kullback–Leibler Divergence\n",
    "\n",
    "Now let’s consider a case in which neither $ g $ nor $ f $\n",
    "generates the data.\n",
    "\n",
    "Instead, a third distribution $ h $ does.\n",
    "\n",
    "Let’s study how accumulated likelihood ratios $ L $ behave\n",
    "when $ h $ governs the data.\n",
    "\n",
    "A key tool here is called **Kullback–Leibler divergence**.\n",
    "\n",
    "It is also called **relative entropy**.\n",
    "\n",
    "It measures how one probability distribution differs from another.\n",
    "\n",
    "In our application, we want to measure how much $ f $ or $ g $\n",
    "diverges from $ h $\n",
    "\n",
    "Two Kullback–Leibler divergences pertinent for us are $ K_f $\n",
    "and $ K_g $ defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{f} = D_{KL}\\bigl(h\\|f\\bigr) = KL(h, f)\n",
    "          &= E_{h}\\left[\\log\\frac{h(w)}{f(w)}\\right] \\\\\n",
    "          &= \\int \\log\\left(\\frac{h(w)}{f(w)}\\right)h(w)dw .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{g} = D_{KL}\\bigl(h\\|g\\bigr) = KL(h,g)\n",
    "          &= E_{h}\\left[\\log\\frac{h(w)}{g(w)}\\right] \\\\\n",
    "          &= \\int \\log\\left(\\frac{h(w)}{g(w)}\\right)h(w)dw .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let’s compute the Kullback–Leibler discrepancies by quadrature\n",
    "integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fad7a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL(f, g):\n",
    "    \"\"\"\n",
    "    Compute KL divergence KL(f, g)\n",
    "    \"\"\"\n",
    "    integrand = lambda w: f(w) * np.log(f(w) / g(w))\n",
    "    val, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c0219",
   "metadata": {},
   "source": [
    "Next we create a helper function to compute KL divergence with respect to a reference distribution $ h $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48869fe5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL_h(h, f, g):\n",
    "    \"\"\"\n",
    "    Compute KL divergence with reference distribution h\n",
    "    \"\"\"\n",
    "\n",
    "    Kf = compute_KL(h, f)\n",
    "    Kg = compute_KL(h, g)\n",
    "\n",
    "    return Kf, Kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77549ffe",
   "metadata": {},
   "source": [
    "\n",
    "<a id='kl-link'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283189e",
   "metadata": {},
   "source": [
    "### A helpful formula\n",
    "\n",
    "There is a mathematical relationship between likelihood ratios and KL divergence.\n",
    "\n",
    "When data is generated by distribution $ h $, the expected log likelihood ratio is:\n",
    "\n",
    "\n",
    "<a id='equation-eq-kl-likelihood-link'></a>\n",
    "$$\n",
    "\\frac{1}{t} E_{h}\\!\\bigl[\\log L_t\\bigr] = KL(h, g) - KL(h, f) = K_g - K_f \\tag{21.1}\n",
    "$$\n",
    "\n",
    "where $ L_t=\\prod_{j=1}^{t}\\frac{f(w_j)}{g(w_j)} $ is the likelihood ratio process.\n",
    "\n",
    "(For the proof, see [this note](https://nowak.ece.wisc.edu/ece830/ece830_fall11_lecture7.pdf).)\n",
    "\n",
    "Equation [(21.1)](#equation-eq-kl-likelihood-link) tells us that:\n",
    "\n",
    "- When $ K_g < K_f $ (i.e., $ g $ is closer to $ h $ than $ f $ is), the expected log likelihood ratio is negative, so $ L\\left(w^t\\right) \\rightarrow 0 $.  \n",
    "- When $ K_g > K_f $ (i.e., $ f $ is closer to $ h $ than $ g $ is), the expected log likelihood ratio is positive, so $ L\\left(w^t\\right) \\rightarrow + \\infty $.  \n",
    "\n",
    "\n",
    "Let’s verify this using simulation.\n",
    "\n",
    "In the simulation, we generate multiple paths using Beta distributions $ f $, $ g $, and $ h $, and compute the paths of $ \\log(L(w^t)) $.\n",
    "\n",
    "First, we write a function to compute the likelihood ratio process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9041e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_likelihood_ratios(sequences, f, g):\n",
    "    \"\"\"Compute likelihood ratios and cumulative products.\"\"\"\n",
    "    l_ratios = f(sequences) / g(sequences)\n",
    "    L_cumulative = np.cumprod(l_ratios, axis=1)\n",
    "    return l_ratios, L_cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede67d7",
   "metadata": {},
   "source": [
    "We consider three cases: (1) $ h $ is closer to $ f $, (2) $ f $ and $ g $ are approximately equidistant from $ h $, and (3) $ h $ is closer to $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5cdc2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"KL(h,g) > KL(h,f)\",\n",
    "        \"h_params\": (1.2, 1.1),\n",
    "        \"expected\": r\"$L_t \\to \\infty$\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KL(h,g) ≈ KL(h,f)\",\n",
    "        \"h_params\": (2, 1.35),\n",
    "        \"expected\": \"$L_t$ fluctuates\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KL(h,g) < KL(h,f)\", \n",
    "        \"h_params\": (3.5, 1.5),\n",
    "        \"expected\": r\"$L_t \\to 0$\"\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 12))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Define h\n",
    "    h = lambda x: p(x, scenario[\"h_params\"][0], \n",
    "                    scenario[\"h_params\"][1])\n",
    "    \n",
    "    # Compute KL divergences\n",
    "    Kf, Kg = compute_KL_h(h, f, g)\n",
    "    kl_diff = Kg - Kf\n",
    "    \n",
    "    # Simulate paths\n",
    "    N_paths = 100\n",
    "    T = 150\n",
    "\n",
    "    # Generate data from h\n",
    "    h_data = np.random.beta(scenario[\"h_params\"][0], \n",
    "                scenario[\"h_params\"][1], (N_paths, T))\n",
    "    l_ratios, l_cumulative = compute_likelihood_ratios(h_data, f, g)\n",
    "    log_l_cumulative = np.log(l_cumulative)\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax = axes[0, i]\n",
    "    x_range = np.linspace(0.001, 0.999, 200)\n",
    "    ax.plot(x_range, [f(x) for x in x_range], \n",
    "        'b-', label='f', linewidth=2)\n",
    "    ax.plot(x_range, [g(x) for x in x_range], \n",
    "        'r-', label='g', linewidth=2)\n",
    "    ax.plot(x_range, [h(x) for x in x_range], \n",
    "        'g--', label='h (data)', linewidth=2)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('density')\n",
    "    ax.set_title(scenario[\"name\"], fontsize=16)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot log likelihood ratio paths\n",
    "    ax = axes[1, i]\n",
    "    for j in range(min(20, N_paths)):\n",
    "        ax.plot(log_l_cumulative[j, :], alpha=0.3, color='purple')\n",
    "    \n",
    "    # Plot theoretical expectation\n",
    "    theory_line = kl_diff * np.arange(1, T+1)\n",
    "    ax.plot(theory_line, 'k--', linewidth=2, label=r'$t \\times (K_g - K_f)$')\n",
    "    \n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('$log L_t$')\n",
    "    ax.set_title(f'KL(h,f)={Kf:.3f}, KL(h,g)={Kg:.3f}\\n{scenario[\"expected\"]}', \n",
    "                 fontsize=16)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0298a6",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "- In the first figure, $ \\log L(w^t) $ diverges to $ \\infty $ because $ K_g > K_f $.  \n",
    "- In the second figure, we still have $ K_g > K_f $, but the difference is smaller, so $ L(w^t) $ diverges to infinity at a slower pace.  \n",
    "- In the last figure, $ \\log L(w^t) $ diverges to $ -\\infty $ because $ K_g < K_f $.  \n",
    "- The black dotted line, $ t \\left(KL(h,g) - KL(h, f)\\right) $, closely fits the paths verifying [(21.1)](#equation-eq-kl-likelihood-link).  \n",
    "\n",
    "\n",
    "These observations align with the theory.\n",
    "\n",
    "In the [next section](#hetero-agent), we will see an application of these ideas.\n",
    "\n",
    "\n",
    "<a id='hetero-agent'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b0b53",
   "metadata": {},
   "source": [
    "## Heterogeneous Beliefs and Financial Markets\n",
    "\n",
    "A likelihood ratio process lies behind  Lawrence  Blume and David Easley’s answer to their question\n",
    "‘‘If you’re so smart, why aren’t you rich?’’ [[Blume and Easley, 2006](https://python.quantecon.org/zreferences.html#id5)].\n",
    "\n",
    "Blume and Easley constructed  formal models to study how differences of opinions about probabilities governing risky income processes would influence outcomes and be reflected in prices of stocks, bonds, and insurance policies that individuals use to share and hedge risks.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">[[Alchian, 1950](https://python.quantecon.org/zreferences.html#id4)] and [[Friedman, 1953](https://python.quantecon.org/zreferences.html#id3)] can conjectured that, by rewarding traders with more realistic probability models,  competitive markets in financial securities put wealth in the hands of better informed traders and help\n",
    "make prices of risky assets  reflect realistic probability assessments.\n",
    "\n",
    "Here we’ll  provide an example that illustrates  basic components of Blume and Easley’s analysis.\n",
    "\n",
    "We’ll focus only on their analysis of an environment with complete markets in which trades in all conceivable risky securities are possible.\n",
    "\n",
    "We’ll study two alternative arrangements:\n",
    "\n",
    "- perfect socialism in which individuals surrender their endowments of consumption goods each period to a central planner who then dictatorially allocates those goods  \n",
    "- a decentralized system of competitive markets in which selfish price-taking individuals voluntarily trade with each other in competitive markets  \n",
    "\n",
    "\n",
    "The fundamental theorems of welfare economics will apply and assure us that these two arrangements end up producing exactly the same allocation of consumption goods to individuals **provided** that the social planner assigns  an appropriate set of **Pareto weights**.\n",
    "\n",
    "Let the random variable $ s_t \\in (0,1) $ at time $ t =0, 1, 2, \\ldots $ be distributed according to the same  Beta distribution  with parameters\n",
    "$ \\theta = \\{\\theta_1, \\theta_2\\} $.\n",
    "\n",
    "We’ll denote this  probability density as\n",
    "\n",
    "$$\n",
    "\\pi(s_t|\\theta)\n",
    "$$\n",
    "\n",
    "Below, we’ll often just write $ \\pi(s_t) $ instead of $ \\pi(s_t|\\theta) $ to save space.\n",
    "\n",
    "Let $ s_t \\equiv y_t^1 $ be the endowment of a nonstorable consumption good  that a person we’ll call “agent 1” receives at time $ t $.\n",
    "\n",
    "Let a history $ s^t = [s_t, s_{t-1}, \\ldots, s_0] $ be a sequence of i.i.d. random variables with joint distribution\n",
    "\n",
    "$$\n",
    "\\pi_t(s^t) = \\pi(s_t) \\pi(s_{t-1}) \\cdots \\pi(s_0)\n",
    "$$\n",
    "\n",
    "So in our example, the history $ s^t $ is a comprehensive record of agent $ 1 $’s endowments of the consumption good from time $ 0 $ up to time $ t $.\n",
    "\n",
    "If agent $ 1 $ were to live on an island by himself, agent $ 1 $’s consumption $ c^1(s_t) $ at time $ t $ is\n",
    "\n",
    "$$\n",
    "c^1(s_t) = y_t^1 = s_t.\n",
    "$$\n",
    "\n",
    "But in our model, agent 1 is not alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88d27f",
   "metadata": {},
   "source": [
    "### Nature and agents’ beliefs\n",
    "\n",
    "Nature draws i.i.d. sequences $ \\{s_t\\}_{t=0}^\\infty $ from $ \\pi_t(s^t) $.\n",
    "\n",
    "- so $ \\pi $ without a superscript is nature’s model  \n",
    "- but in addition to nature, there are other entities inside our model – artificial people that we call “agents”  \n",
    "- each agent has a sequence of probability distributions over $ s^t $ for $ t=0, \\ldots $  \n",
    "- agent $ i $ thinks that nature draws i.i.d. sequences $ \\{s_t\\}_{t=0}^\\infty $ from $ \\pi_t^i(s^t) $  \n",
    "  - agent $ i $ is mistaken unless $ \\pi_t^i(s^t) = \\pi_t(s^t) $  \n",
    "\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">A **rational expectations** model would set $ \\pi_t^i(s^t) = \\pi_t(s^t) $ for all agents $ i $.\n",
    "\n",
    "There are two agents named $ i=1 $ and $ i=2 $.\n",
    "\n",
    "At time $ t $, agent $ 1 $ receives an endowment\n",
    "\n",
    "$$\n",
    "y_t^1 = s_t\n",
    "$$\n",
    "\n",
    "of a nonstorable consumption good, while agent $ 2 $ receives an endowment of\n",
    "\n",
    "$$\n",
    "y_t^2 = 1 - s_t\n",
    "$$\n",
    "\n",
    "The aggregate endowment of the consumption good is\n",
    "\n",
    "$$\n",
    "y_t^1 + y_t^2 = 1\n",
    "$$\n",
    "\n",
    "at each date $ t \\geq 0 $.\n",
    "\n",
    "At date $ t $ agent $ i $ consumes $ c_t^i(s^t) $ of the good.\n",
    "\n",
    "A (non wasteful) feasible allocation of the aggregate endowment of $ 1 $ each period satisfies\n",
    "\n",
    "$$\n",
    "c_t^1 + c_t^2 = 1 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8669c3",
   "metadata": {},
   "source": [
    "### A social risk-sharing arrangement\n",
    "\n",
    "In order to share risks, a benevolent social planner will dictate a history-dependent consumption allocation in the form of a sequence of functions\n",
    "\n",
    "$$\n",
    "c_t^i = c_t^i(s^t)\n",
    "$$\n",
    "\n",
    "that satisfy\n",
    "\n",
    "\n",
    "<a id='equation-eq-feasibility'></a>\n",
    "$$\n",
    "c_t^1(s^t) + c_t^2(s^t) = 1 \\tag{21.2}\n",
    "$$\n",
    "\n",
    "for all $ s^t $ for all $ t \\geq 0 $.\n",
    "\n",
    "To design a socially optimal allocation, the social planner wants to know what agent $ 1 $ believes about the endowment sequence and how they feel about bearing risks.\n",
    "\n",
    "As for the endowment sequences, agent $ i $ believes that nature draws i.i.d. sequences from joint densities\n",
    "\n",
    "$$\n",
    "\\pi_t^i(s^t) = \\pi(s_t)^i \\pi^i(s_{t-1}) \\cdots \\pi^i(s_0)\n",
    "$$\n",
    "\n",
    "As for attitudes toward bearing risks, agent $ i $ has a one-period utility function\n",
    "\n",
    "$$\n",
    "u(c_t^i) = u(c_t^i) = \\ln (c_t^i)\n",
    "$$\n",
    "\n",
    "with marginal utility of consumption in period $ i $\n",
    "\n",
    "$$\n",
    "u'(c_t^i) = \\frac{1}{c_t^i}\n",
    "$$\n",
    "\n",
    "Putting its beliefs about its random endowment sequence and its attitudes toward bearing risks together, agent $ i $ has intertemporal utility function\n",
    "\n",
    "\n",
    "<a id='equation-eq-objectiveagenti'></a>\n",
    "$$\n",
    "V^i = \\sum_{t=0}^{\\infty} \\sum_{s^t} \\delta^t u(c_t^i(s^t)) \\pi_t^i(s^t) , \\tag{21.3}\n",
    "$$\n",
    "\n",
    "where $ \\delta \\in (0,1) $ is an intertemporal discount factor, and $ u(\\cdot) $ is a strictly increasing, concave one-period utility function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94518eaf",
   "metadata": {},
   "source": [
    "### The social planner’s allocation problem\n",
    "\n",
    "The benevolent dictator has all the information it requires to choose a consumption allocation that maximizes the social welfare criterion\n",
    "\n",
    "\n",
    "<a id='equation-eq-welfarew'></a>\n",
    "$$\n",
    "W = \\lambda V^1 + (1-\\lambda) V^2 \\tag{21.4}\n",
    "$$\n",
    "\n",
    "where $ \\lambda \\in [0,1] $ is a Pareto weight tells how much the planner likes agent $ 1 $ and $ 1 - \\lambda $ is a Pareto weight that tells how much the social planner likes agent $ 2 $.\n",
    "\n",
    "Setting $ \\lambda = .5 $ expresses ‘‘egalitarian’’ social preferences.\n",
    "\n",
    "Notice how social welfare criterion [(21.4)](#equation-eq-welfarew) takes into account both agents’ preferences as represented by formula [(21.3)](#equation-eq-objectiveagenti).\n",
    "\n",
    "This means that the social planner knows and respects\n",
    "\n",
    "- each agent’s  one period utility function $ u(\\cdot) = \\ln(\\cdot) $  \n",
    "- each agent $ i $’s probability model $ \\{\\pi_t^i(s^t)\\}_{t=0}^\\infty $  \n",
    "\n",
    "\n",
    "Consequently, we anticipate that these objects will appear in the social planner’s rule for allocating the aggregate endowment each period.\n",
    "\n",
    "First-order necessary conditions for maximizing welfare criterion [(21.4)](#equation-eq-welfarew) subject to the feasibility constraint [(21.2)](#equation-eq-feasibility) are\n",
    "\n",
    "$$\n",
    "\\frac{\\pi_t^2(s^t)}{\\pi_t^1(s^t)} \\frac{(1/c_t^2(s^t))}{(1/c_t^1(s^t))} = \\frac{\\lambda}{1 -\\lambda}\n",
    "$$\n",
    "\n",
    "which can be rearranged to become\n",
    "\n",
    "\n",
    "<a id='equation-eq-allocationrule0'></a>\n",
    "$$\n",
    "\\frac{c_t^1(s^t)}{c_t^2(s^t)} = \\frac{\\lambda}{1- \\lambda} l_t(s^t) \\tag{21.5}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "l_t(s^t) = \\frac{\\pi_t^1(s^t)}{\\pi_t^2(s^t)}\n",
    "$$\n",
    "\n",
    "is the likelihood ratio of agent 1’s joint density to agent 2’s joint density.\n",
    "\n",
    "Using\n",
    "\n",
    "$$\n",
    "c_t^1(s^t) + c_t^2(s^t) = 1\n",
    "$$\n",
    "\n",
    "we can rewrite allocation rule [(21.5)](#equation-eq-allocationrule0) as\n",
    "\n",
    "$$\n",
    "\\frac{c_t^1(s^t)}{1 - c_t^1(s^t)} = \\frac{\\lambda}{1-\\lambda} l_t(s^t)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "c_t^1(s^t) = \\frac{\\lambda}{1-\\lambda} l_t(s^t)(1 - c_t^1(s^t))\n",
    "$$\n",
    "\n",
    "which implies that the social planner’s allocation rule is\n",
    "\n",
    "\n",
    "<a id='equation-eq-allocationrule1'></a>\n",
    "$$\n",
    "c_t^1(s^t) = \\frac{\\lambda l_t(s^t)}{1-\\lambda + \\lambda l_t(s^t)} \\tag{21.6}\n",
    "$$\n",
    "\n",
    "If we define a temporary or **continuation Pareto weight** process as\n",
    "\n",
    "$$\n",
    "\\lambda_t(s^t) = \\frac{\\lambda l_t(s^t)}{1-\\lambda + \\lambda l_t(s^t)},\n",
    "$$\n",
    "\n",
    "then we can represent the social planner’s allocation rule as\n",
    "\n",
    "$$\n",
    "c_t^1(s^t) = \\lambda_t(s^t) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e5435",
   "metadata": {},
   "source": [
    "### If you’re so smart, $ \\ldots $\n",
    "\n",
    "Let’s compute some values   of limiting allocations [(21.6)](#equation-eq-allocationrule1) for some interesting possible limiting\n",
    "values of the likelihood ratio process $ l_t(s^t) $:\n",
    "\n",
    "$$\n",
    "l_\\infty (s^\\infty)= 1; \\quad c_\\infty^1 = \\lambda\n",
    "$$\n",
    "\n",
    "- In the above case, both agents are equally smart (or equally not smart) and the consumption allocation stays put at a $ \\lambda, 1 - \\lambda $ split between the two agents.  \n",
    "\n",
    "\n",
    "$$\n",
    "l_\\infty (s^\\infty) = 0; \\quad c_\\infty^1 = 0\n",
    "$$\n",
    "\n",
    "- In the above case, agent 2 is smarter than agent 1, and agent 1’s share of the aggregate endowment converges to zero.  \n",
    "\n",
    "\n",
    "$$\n",
    "l_\\infty (s^\\infty)= \\infty; \\quad c_\\infty^1 = 1\n",
    "$$\n",
    "\n",
    "- In the above case, agent 1 is smarter than agent 2, and agent 1’s share of the aggregate endowment converges to 1.  \n",
    "\n",
    "\n",
    "Soon we’ll do some simulations that will shed further light on possible outcomes.\n",
    "\n",
    "But before we do that, let’s take a detour and study some  “shadow prices” for the social planning problem that can readily be\n",
    "converted to “equilibrium prices” for a competitive equilibrium.\n",
    "\n",
    "Doing this will allow us to connect our analysis with an argument  of [[Alchian, 1950](https://python.quantecon.org/zreferences.html#id4)] and [[Friedman, 1953](https://python.quantecon.org/zreferences.html#id3)] that competitive market processes can make prices of risky assets better reflect realistic probability assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1cd7e",
   "metadata": {},
   "source": [
    "### Competitive Equilibrium Prices\n",
    "\n",
    "The two fundamental welfare theorems for general equilibrium models lead us to anticipate that there is  a connection between the allocation that solves the social planning problem we have been studying and the allocation in a  **competitive equilibrium**  with complete markets in history-contingent commodities.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For the two welfare theorems and their history, see   [https://en.wikipedia.org/wiki/Fundamental_theorems_of_welfare_economics](https://en.wikipedia.org/wiki/Fundamental_theorems_of_welfare_economics).\n",
    "\n",
    "Such a connection prevails for our model.\n",
    "\n",
    "We’ll sketch it now.\n",
    "\n",
    "In a competitive equilibrium, there is no social planner that dictatorially collects everybody’s endowments and then reallocates them.\n",
    "\n",
    "Instead, there is a comprehensive centralized   market that meets at one point in time.\n",
    "\n",
    "There are **prices** at which price-taking agents can buy or sell whatever goods that they want.\n",
    "\n",
    "Trade is multilateral in the sense that all that there is a “Walrasian auctioneer” who lives outside the model and whose job is to verify that\n",
    "each agent’s budget constraint is satisfied.\n",
    "\n",
    "That budget constraint involves the total value of the agent’s endowment stream and the total value of its consumption stream.\n",
    "\n",
    "Suppose that at time $ -1 $, before time $ 0 $ starts, agent  $ i $ can purchase one unit $ c_t(s^t) $ of  consumption at time $ t $ after history\n",
    "$ s^t $ at price $ p_t(s^t) $.\n",
    "\n",
    "Notice that there is (very long) **vector** of prices.\n",
    "\n",
    "We want to study how agents’ diverse beliefs influence equilibrium prices.\n",
    "\n",
    "Agent $ i $ faces a **single** intertemporal budget constraint\n",
    "\n",
    "\n",
    "<a id='equation-eq-budgeti'></a>\n",
    "$$\n",
    "\\sum_{t=0}\\sum_{s^t} p_t(s^t) c_t^i (y_t(s^t)) \\leq \\sum_{t=0}\\sum_{s^t} p_t(s^t) y_t^i (y_t(s^t)) \\tag{21.7}\n",
    "$$\n",
    "\n",
    "Agent $ i $ puts a Lagrange multiplier $ \\mu^i $ on [(21.7)](#equation-eq-budgeti) and once-and-for-all chooses a consumption plan $ \\{c^i_t(s^t)\\}_{t=0}^\\infty $\n",
    "to maximize criterion [(21.3)](#equation-eq-objectiveagenti) subject to budget constraint [(21.7)](#equation-eq-budgeti).\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For convenience, let’s remind ourselves of criterion [(21.3)](#equation-eq-objectiveagenti):\\\\\n",
    "\n",
    "\n",
    "$ V^i = \\sum_{t=0}^{\\infty} \\sum_{s^t} \\delta^t u_t(c_t^i(s^t)) \\pi_t^i(s^t) $\n",
    "\n",
    "First-order conditions for maximizing  with respect to $ c_t^i(s^t) $ are\n",
    "\n",
    "$$\n",
    "\\delta^t u'(c^i(s^t)) \\pi_t^i(s^t) = \\mu_i p_t(s^t) ,\n",
    "$$\n",
    "\n",
    "which we can rearrange to obtain\n",
    "\n",
    "\n",
    "<a id='equation-eq-priceequation1'></a>\n",
    "$$\n",
    "p_t(s^t) = \\frac{ \\delta^t \\pi_t^i(s^t)}{\\mu^i c^i(s^t)} \\tag{21.8}\n",
    "$$\n",
    "\n",
    "for $ i=1,2 $.\n",
    "\n",
    "If we divide equation [(21.8)](#equation-eq-priceequation1) for agent $ 1 $ by the appropriate  version of equation [(21.8)](#equation-eq-priceequation1) for agent 2, use\n",
    "$ c^2_t(s^t) = 1 - c^1_t(s^t) $, and do some algebra, we’ll obtain\n",
    "\n",
    "\n",
    "<a id='equation-eq-allocationce'></a>\n",
    "$$\n",
    "c_t^1(s^t) = \\frac{\\mu_1 l_t(s^t)}{\\mu_2 + \\mu_1 l_t(s^t)} . \\tag{21.9}\n",
    "$$\n",
    "\n",
    "We now engage in an extended “guess-and-verify” exercise that involves matching objects in our competitive equilibrium with objects in\n",
    "our social planning problem.\n",
    "\n",
    "- we’ll match consumption allocations in the planning problem with equilibrium consumption allocations in the competitive equilibrium  \n",
    "- we’ll match “shadow” prices in the planning problem with competitive equilibrium prices.  \n",
    "\n",
    "\n",
    "Notice that if we set $ \\mu_1 = \\lambda $ and $ \\mu_2 = 1 -\\lambda $, then  formula [(21.9)](#equation-eq-allocationce) agrees with formula\n",
    "[(21.6)](#equation-eq-allocationrule1).\n",
    "\n",
    "- doing this amounts to choosing a **numeraire** or normalization for the price system $ \\{p_t(s^t)\\}_{t=0}^\\infty $  \n",
    "\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For information about how a numeraire  must be chosen to pin down the absolute price level in a model like ours that determines only\n",
    "relative prices,   see [https://en.wikipedia.org/wiki/Numéraire](https://en.wikipedia.org/wiki/Num%C3%A9raire).\n",
    "\n",
    "If we substitute formula  [(21.9)](#equation-eq-allocationce) for $ c_t^1(s^t) $ into formula [(21.8)](#equation-eq-priceequation1) and rearrange, we obtain\n",
    "\n",
    "\n",
    "<a id='equation-eq-pformulafinal'></a>\n",
    "$$\n",
    "p_t(s^t) = \\frac{\\delta^t \\pi_t^2(s^t)}{1 - \\lambda + \\lambda l_t(s^t)} \\tag{21.10}\n",
    "$$\n",
    "\n",
    "According to formula [(21.10)](#equation-eq-pformulafinal), we have the following possible limiting cases:\n",
    "\n",
    "- when $ l_\\infty = 0 $, $ c_\\infty^2 = 0 $ and tails of competitive equilibrium prices reflect agent $ 2 $’s probability model $ \\pi_t^2(s^t) $  \n",
    "- when $ l_\\infty = 1 $, $ c_\\infty^1 = 0 $ and tails competitive equilibrium prices reflect agent $ 1 $’s probability model $ \\pi_t^2(s^t) $  \n",
    "- for small $ t $’s, competitive equilbrium prices reflect both agents’ probability models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0dd2f",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "Now let’s implement some simulations when agent $ 1 $ believes marginal density\n",
    "\n",
    "$$\n",
    "\\pi^1(s_t) = f(s_t)\n",
    "$$\n",
    "\n",
    "and agent $ 2 $ believes marginal density\n",
    "\n",
    "$$\n",
    "\\pi^2(s_t) = g(s_t)\n",
    "$$\n",
    "\n",
    "where $ f $ and $ g $ are Beta distributions like ones that  we used in earlier  sections of this lecture.\n",
    "\n",
    "Meanwhile, we’ll assume that  nature believes a  marginal density\n",
    "\n",
    "$$\n",
    "\\pi(s_t) = h(s_t)\n",
    "$$\n",
    "\n",
    "where $ h(s_t) $ is perhaps a  mixture of $ f $ and $ g $.\n",
    "\n",
    "Let’s  write a Python function that computes agent 1’s  consumption share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c404f33",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_blume_easley(sequences, f_belief=f, g_belief=g, λ=0.5):\n",
    "    \"\"\"Simulate Blume-Easley model consumption shares.\"\"\"\n",
    "    l_ratios, l_cumulative = compute_likelihood_ratios(sequences, f_belief, g_belief)\n",
    "    c1_share = λ * l_cumulative / (1 - λ + λ * l_cumulative)\n",
    "    return l_cumulative, c1_share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518bfc2b",
   "metadata": {},
   "source": [
    "Now let’s use this  function to generate sequences in which\n",
    "\n",
    "- nature draws from  $ f $ each period, or  \n",
    "- nature draws from  $ g $ each period, or  \n",
    "- or nature flips a fair coin each period  to decide whether  to draw from  $ f $ or $ g $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fec5d7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "λ = 0.5\n",
    "T = 100\n",
    "N = 10000\n",
    "\n",
    "# Nature follows f, g, or mixture\n",
    "s_seq_f = np.random.beta(F_a, F_b, (N, T))\n",
    "s_seq_g = np.random.beta(G_a, G_b, (N, T))\n",
    "\n",
    "h = jit(lambda x: 0.5 * f(x) + 0.5 * g(x))\n",
    "model_choices = np.random.rand(N, T) < 0.5\n",
    "s_seq_h = np.empty((N, T))\n",
    "s_seq_h[model_choices] = np.random.beta(F_a, F_b, size=model_choices.sum())\n",
    "s_seq_h[~model_choices] = np.random.beta(G_a, G_b, size=(~model_choices).sum())\n",
    "\n",
    "l_cum_f, c1_f = simulate_blume_easley(s_seq_f)\n",
    "l_cum_g, c1_g = simulate_blume_easley(s_seq_g)\n",
    "l_cum_h, c1_h = simulate_blume_easley(s_seq_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89b9b6",
   "metadata": {},
   "source": [
    "Before looking at the figure below, have some fun by guessing whether agent 1 or agent 2 will have a larger and larger consumption share as time passes in our three cases.\n",
    "\n",
    "To make better guesses,  let’s visualize instances of the likelihood ratio processes in  the three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9bf57",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "titles = [\"Nature = f\", \"Nature = g\", \"Nature = mixture\"]\n",
    "data_pairs = [(l_cum_f, c1_f), (l_cum_g, c1_g), (l_cum_h, c1_h)]\n",
    "\n",
    "for i, ((l_cum, c1), title) in enumerate(zip(data_pairs, titles)):\n",
    "    # Likelihood ratios\n",
    "    ax = axes[0, i]\n",
    "    for j in range(min(50, l_cum.shape[0])):\n",
    "        ax.plot(l_cum[j, :], alpha=0.3, color='blue')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('Likelihood ratio $l_t$')\n",
    "    ax.set_title(title)\n",
    "    ax.axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Consumption shares\n",
    "    ax = axes[1, i]\n",
    "    for j in range(min(50, c1.shape[0])):\n",
    "        ax.plot(c1[j, :], alpha=0.3, color='green')\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel(\"Agent 1's consumption share\")\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.axhline(y=λ, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2977106",
   "metadata": {},
   "source": [
    "In the left panel, nature chooses $ f $. Agent 1’s consumption reaches $ 1 $ very quickly.\n",
    "\n",
    "In the middle panel, nature chooses $ g $. Agent 1’s consumption ratio tends to move towards $ 0 $ but not as fast as in the first case.\n",
    "\n",
    "In the right panel, nature flips coins each period. We see a very similar pattern to the processes in the left panel.\n",
    "\n",
    "The figures in the top panel remind us of the discussion in [this section](#kl-link).\n",
    "\n",
    "We invite readers to revisit [that section](#rel-entropy) and try to infer the relationships among $ KL(f, g) $, $ KL(g, f) $, $ KL(h, f) $, and $ KL(h,g) $.\n",
    "\n",
    "Let’s compute values of KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945064e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "shares = [np.mean(c1_f[:, -1]), np.mean(c1_g[:, -1]), np.mean(c1_h[:, -1])]\n",
    "Kf_g, Kg_f = compute_KL(f, g), compute_KL(g, f)\n",
    "Kf_h, Kg_h = compute_KL_h(h, f, g)\n",
    "\n",
    "print(f\"Final shares: f={shares[0]:.3f}, g={shares[1]:.3f}, mix={shares[2]:.3f}\")\n",
    "print(f\"KL divergences: \\nKL(f,g)={Kf_g:.3f}, KL(g,f)={Kg_f:.3f}\")\n",
    "print(f\"KL(h,f)={Kf_h:.3f}, KL(h,g)={Kg_h:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75900d83",
   "metadata": {},
   "source": [
    "We find that $ KL(f,g) > KL(g,f) $ and $ KL(h,g) > KL(h,f) $.\n",
    "\n",
    "The first inequality tells us that the average “surprise” from having belief $ g $ when nature chooses $ f $ is greater than the “surprise” from having  belief $ f $ when nature chooses $ g $.\n",
    "\n",
    "This explains the difference between the first two panels we noted above.\n",
    "\n",
    "The second inequality tells us that agent 1’s belief distribution $ f $ is closer to nature’s pick than agent 2’s belief $ g $.\n",
    "\n",
    "To make this idea more concrete, let’s compare two cases:\n",
    "\n",
    "- agent 1’s belief distribution $ f $ is close to agent 2’s belief distribution $ g $;  \n",
    "- agent 1’s belief distribution $ f $ is far from agent 2’s belief distribution $ g $.  \n",
    "\n",
    "\n",
    "We use the two distributions visualized below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb1fc8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution_overlap(ax, x_range, f_vals, g_vals, \n",
    "                            f_label='f', g_label='g', \n",
    "                            f_color='blue', g_color='red'):\n",
    "    \"\"\"Plot two distributions with their overlap region.\"\"\"\n",
    "    ax.plot(x_range, f_vals, color=f_color, linewidth=2, label=f_label)\n",
    "    ax.plot(x_range, g_vals, color=g_color, linewidth=2, label=g_label)\n",
    "    \n",
    "    overlap = np.minimum(f_vals, g_vals)\n",
    "    ax.fill_between(x_range, 0, overlap, alpha=0.3, color='purple', label='Overlap')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    \n",
    "# Define close and far belief distributions\n",
    "f_close = jit(lambda x: p(x, 1, 1))\n",
    "g_close = jit(lambda x: p(x, 1.1, 1.05))\n",
    "\n",
    "f_far = jit(lambda x: p(x, 1, 1))\n",
    "g_far = jit(lambda x: p(x, 3, 1.2))\n",
    "\n",
    "# Visualize the belief distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "x_range = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "# Close beliefs\n",
    "f_close_vals = [f_close(x) for x in x_range]\n",
    "g_close_vals = [g_close(x) for x in x_range]\n",
    "plot_distribution_overlap(ax1, x_range, f_close_vals, g_close_vals,\n",
    "                         f_label='f (Beta(1, 1))', g_label='g (Beta(1.1, 1.05))')\n",
    "ax1.set_title(f'Close Beliefs')\n",
    "\n",
    "# Far beliefs\n",
    "f_far_vals = [f_far(x) for x in x_range]\n",
    "g_far_vals = [g_far(x) for x in x_range]\n",
    "plot_distribution_overlap(ax2, x_range, f_far_vals, g_far_vals,\n",
    "                         f_label='f (Beta(1, 1))', g_label='g (Beta(3, 1.2))')\n",
    "ax2.set_title(f'Far Beliefs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45206036",
   "metadata": {},
   "source": [
    "Let’s draw the same consumption ratio plots as above for agent 1.\n",
    "\n",
    "We replace the simulation paths with median and percentiles to make the figure cleaner.\n",
    "\n",
    "Staring at the figure below, can we infer the relation between $ KL(f,g) $ and $ KL(g,f) $?\n",
    "\n",
    "From the right panel, can we infer the relation between $ KL(h,g) $ and $ KL(h,f) $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acaee78",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "nature_params = {'close': [(1, 1), (1.1, 1.05), (2, 1.5)],\n",
    "                 'far':   [(1, 1), (3, 1.2),   (2, 1.5)]}\n",
    "nature_labels = [\"Nature = f\", \"Nature = g\", \"Nature = h\"]\n",
    "colors = {'close': 'blue', 'far': 'red'}\n",
    "\n",
    "threshold = 1e-5  # \"close to zero\" cutoff\n",
    "\n",
    "for row, (f_belief, g_belief, label) in enumerate([\n",
    "                        (f_close, g_close, 'close'),\n",
    "                        (f_far, g_far, 'far')]):\n",
    "    \n",
    "    for col, nature_label in enumerate(nature_labels):\n",
    "        params = nature_params[label][col]\n",
    "        s_seq = np.random.beta(params[0], params[1], (1000, 200))\n",
    "        _, c1 = simulate_blume_easley(s_seq, f_belief, g_belief, λ)\n",
    "        \n",
    "        median_c1 = np.median(c1, axis=0)\n",
    "        p10, p90 = np.percentile(c1, [10, 90], axis=0)\n",
    "        \n",
    "        ax = axes[row, col]\n",
    "        color = colors[label]\n",
    "        ax.plot(median_c1, color=color, linewidth=2, label='Median')\n",
    "        ax.fill_between(range(len(median_c1)), p10, p90, alpha=0.3, color=color, label='10–90%')\n",
    "        ax.set_xlabel('time')\n",
    "        ax.set_ylabel(\"Agent 1's share\")\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_title(nature_label)\n",
    "        ax.axhline(y=λ, color='gray', linestyle='--', alpha=0.5)\n",
    "        below = np.where(median_c1 < threshold)[0]\n",
    "        above = np.where(median_c1 > 1-threshold)[0]\n",
    "        if below.size > 0: first_zero = (below[0], True)\n",
    "        elif above.size > 0: first_zero = (above[0], False)\n",
    "        else: first_zero = None\n",
    "        if first_zero is not None:\n",
    "            ax.axvline(x=first_zero[0], color='black', linestyle='--',\n",
    "                       alpha=0.7, \n",
    "                       label=fr'Median $\\leq$ {threshold}' if first_zero[1]\n",
    "                       else fr'Median $\\geq$ 1-{threshold}')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c2ba2",
   "metadata": {},
   "source": [
    "Holding to our guesses, let’s calculate the four values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb1995",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Close case\n",
    "Kf_g, Kg_f = compute_KL(f_close, g_close), compute_KL(g_close, f_close)\n",
    "Kf_h, Kg_h = compute_KL_h(h, f_close, g_close)\n",
    "\n",
    "print(f\"KL divergences (close): \\nKL(f,g)={Kf_g:.3f}, KL(g,f)={Kg_f:.3f}\")\n",
    "print(f\"KL(h,f)={Kf_h:.3f}, KL(h,g)={Kg_h:.3f}\")\n",
    "\n",
    "# Far case\n",
    "Kf_g, Kg_f = compute_KL(f_far, g_far), compute_KL(g_far, f_far)\n",
    "Kf_h, Kg_h = compute_KL_h(h, f_far, g_far)\n",
    "\n",
    "print(f\"KL divergences (far): \\nKL(f,g)={Kf_g:.3f}, KL(g,f)={Kg_f:.3f}\")\n",
    "print(f\"KL(h,f)={Kf_h:.3f}, KL(h,g)={Kg_h:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157114e6",
   "metadata": {},
   "source": [
    "We find that in the first case, $ KL(f,g) \\approx KL(g,f) $ and both are relatively small, so although either agent 1 or agent  2 will eventually consume everything, convergence displaying in  first two panels on the top is pretty  slowly.\n",
    "\n",
    "In the first two panels at the bottom, we see convergence occurring faster (as indicated by the black dashed line) because the divergence gaps $ KL(f, g) $ and $ KL(g, f) $ are larger.\n",
    "\n",
    "Since $ KL(f,g) > KL(g,f) $, we  see faster convergence in  the first panel at the bottom when  nature chooses $ f $  than in the second panel where nature chooses $ g $.\n",
    "\n",
    "This ties in nicely with [(21.1)](#equation-eq-kl-likelihood-link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7e6c4",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and Classification\n",
    "\n",
    "This section discusses another application of likelihood ratio processes.\n",
    "\n",
    "We describe how a statistician can combine frequentist probabilities of type I and type II errors in order to\n",
    "\n",
    "- compute an anticipated frequency of  selecting a wrong model based on a sample length $ T $  \n",
    "- compute an anticipated error  rate in a classification problem  \n",
    "\n",
    "\n",
    "We consider a situation in which  nature generates data by mixing known densities $ f $ and $ g $ with known mixing\n",
    "parameter $ \\pi_{-1} \\in (0,1) $ so that the random variable $ w $ is drawn from the density\n",
    "\n",
    "$$\n",
    "h (w) = \\pi_{-1} f(w) + (1-\\pi_{-1}) g(w)\n",
    "$$\n",
    "\n",
    "We assume that the statistician knows the densities $ f $ and $ g $ and also the mixing parameter $ \\pi_{-1} $.\n",
    "\n",
    "Below, we’ll  set $ \\pi_{-1} = .5 $, although much of the analysis would follow through with other settings of $ \\pi_{-1} \\in (0,1) $.\n",
    "\n",
    "We assume that $ f $ and $ g $ both put positive probabilities on the same intervals of possible realizations of the random variable $ W $.\n",
    "\n",
    "In the simulations below, we specify that  $ f $ is a $ \\text{Beta}(1, 1) $ distribution and that  $ g $ is $ \\text{Beta}(3, 1.2) $ distribution.\n",
    "\n",
    "We consider two alternative timing protocols.\n",
    "\n",
    "- Timing protocol 1 is for   the model selection problem  \n",
    "- Timing protocol 2 is for the individual classification problem  \n",
    "\n",
    "\n",
    "**Timing Protocol 1:**  Nature flips a coin only **once** at time $ t=-1 $ and with probability $ \\pi_{-1} $  generates a sequence  $ \\{w_t\\}_{t=1}^T $\n",
    "of  IID  draws from  $ f $  and with probability $ 1-\\pi_{-1} $ generates a sequence  $ \\{w_t\\}_{t=1}^T $\n",
    "of  IID  draws from  $ g $.\n",
    "\n",
    "Let’s write some Python code that implements timing protocol 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1503f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def protocol_1(π_minus_1, T, N=1000):\n",
    "    \"\"\"\n",
    "    Simulate Protocol 1: \n",
    "    Nature decides once at t=-1 which model to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    # On-off coin flip for the true model\n",
    "    true_models_F = np.random.rand(N) < π_minus_1\n",
    "    \n",
    "    sequences = np.empty((N, T))\n",
    "    \n",
    "    n_f = np.sum(true_models_F)\n",
    "    n_g = N - n_f\n",
    "    if n_f > 0:\n",
    "        sequences[true_models_F, :] = np.random.beta(F_a, F_b, (n_f, T))\n",
    "    if n_g > 0:\n",
    "        sequences[~true_models_F, :] = np.random.beta(G_a, G_b, (n_g, T))\n",
    "    \n",
    "    return sequences, true_models_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef5434",
   "metadata": {},
   "source": [
    "**Timing Protocol 2.** Nature flips a coin **often**.  At each time $ t \\geq 0 $, nature flips a coin and with probability $ \\pi_{-1} $ draws $ w_t $ from $ f $ and with probability $ 1-\\pi_{-1} $ draws $ w_t $ from $ g $.\n",
    "\n",
    "Here is  Python code that we’ll use to implement timing protocol 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17c1cf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def protocol_2(π_minus_1, T, N=1000):\n",
    "    \"\"\"\n",
    "    Simulate Protocol 2: \n",
    "    Nature decides at each time step which model to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coin flips for each time t upto T\n",
    "    true_models_F = np.random.rand(N, T) < π_minus_1\n",
    "    \n",
    "    sequences = np.empty((N, T))\n",
    "    \n",
    "    n_f = np.sum(true_models_F)\n",
    "    n_g = N * T - n_f\n",
    "    if n_f > 0:\n",
    "        sequences[true_models_F] = np.random.beta(F_a, F_b, n_f)\n",
    "    if n_g > 0:\n",
    "        sequences[~true_models_F] = np.random.beta(G_a, G_b, n_g)\n",
    "    \n",
    "    return sequences, true_models_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63753fae",
   "metadata": {},
   "source": [
    "**Remark:** Under timing protocol 2, the $ \\{w_t\\}_{t=1}^T $ is a sequence of IID draws from $ h(w) $. Under timing protocol 1, the $ \\{w_t\\}_{t=1}^T $ is\n",
    "not IID.  It is **conditionally IID** – meaning that with probability $ \\pi_{-1} $ it is a sequence of IID draws from $ f(w) $ and with probability $ 1-\\pi_{-1} $ it is a sequence of IID draws from $ g(w) $. For more about this, see [this lecture about exchangeability](https://python.quantecon.org/exchangeable.html).\n",
    "\n",
    "We  again deploy a **likelihood ratio process** with time $ t $ component being the likelihood ratio\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "The **likelihood ratio process** for sequence $ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ is\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "For shorthand we’ll write $ L_t =  L(w^t) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c419f0f",
   "metadata": {},
   "source": [
    "### Model Selection Mistake Probability\n",
    "\n",
    "We first study  a problem that assumes  timing protocol 1.\n",
    "\n",
    "Consider a decision maker who wants to know whether model $ f $ or model $ g $ governs a data set of length $ T $ observations.\n",
    "\n",
    "The decision makers has observed a sequence $ \\{w_t\\}_{t=1}^T $.\n",
    "\n",
    "On the basis of that observed  sequence, a likelihood ratio test selects model $ f $ when\n",
    "$ L_T \\geq 1 $ and model $ g $ when  $ L_T < 1 $.\n",
    "\n",
    "When model $ f $ generates the data, the probability that the likelihood ratio test selects the wrong model is\n",
    "\n",
    "$$\n",
    "p_f = {\\rm Prob}\\left(L_T < 1\\Big| f\\right) = \\alpha_T .\n",
    "$$\n",
    "\n",
    "When model $ g $ generates the data, the probability that the likelihood ratio test selects the wrong model is\n",
    "\n",
    "$$\n",
    "p_g = {\\rm Prob}\\left(L_T \\geq 1 \\Big|g \\right) = \\beta_T.\n",
    "$$\n",
    "\n",
    "We can construct a probability that the likelihood ratio selects the wrong model by assigning a Bayesian prior probability of $ \\pi_{-1} = .5 $ that nature selects model $ f $ then  averaging $ p_f $ and $ p_g $ to form the Bayesian posterior probability of a detection error equal to\n",
    "\n",
    "\n",
    "<a id='equation-eq-detectionerrorprob'></a>\n",
    "$$\n",
    "p(\\textrm{wrong decision}) = {1 \\over 2} (\\alpha_T + \\beta_T) . \\tag{21.11}\n",
    "$$\n",
    "\n",
    "Now let’s simulate  timing protocol 1 and compute the error probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2c50c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "π_minus_1 = 0.5\n",
    "T_max = 30\n",
    "N_simulations = 10_000\n",
    "\n",
    "sequences_p1, true_models_p1 = protocol_1(\n",
    "                            π_minus_1, T_max, N_simulations)\n",
    "l_ratios_p1, L_cumulative_p1 = compute_likelihood_ratios(sequences_p1, f, g)\n",
    "\n",
    "# Compute error probabilities for different sample sizes\n",
    "T_range = np.arange(1, T_max + 1)\n",
    "\n",
    "# Boolean masks for true models\n",
    "mask_f = true_models_p1\n",
    "mask_g = ~true_models_p1\n",
    "\n",
    "# Select cumulative likelihoods for each model\n",
    "L_f = L_cumulative_p1[mask_f, :]\n",
    "L_g = L_cumulative_p1[mask_g, :]\n",
    "\n",
    "α_T = np.mean(L_f < 1, axis=0)\n",
    "β_T = np.mean(L_g >= 1, axis=0)\n",
    "\n",
    "error_prob = 0.5 * (α_T + β_T)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(T_range, α_T, 'b-', \n",
    "         label=r'$\\alpha_T$', linewidth=2)\n",
    "ax1.plot(T_range, β_T, 'r-', \n",
    "         label=r'$\\beta_T$', linewidth=2)\n",
    "ax1.set_xlabel('$T$')\n",
    "ax1.set_ylabel('error probability')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(T_range, error_prob, 'g-', \n",
    "         label=r'$\\frac{1}{2}(\\alpha_T+\\beta_T)$', linewidth=2)\n",
    "ax2.set_xlabel('$T$')\n",
    "ax2.set_ylabel('error probability')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At T={T_max}:\")\n",
    "print(f\"α_{T_max} = {α_T[-1]:.4f}\")\n",
    "print(f\"β_{T_max} = {β_T[-1]:.4f}\")\n",
    "print(f\"Model selection error probability = {error_prob[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb3a4f",
   "metadata": {},
   "source": [
    "Notice how the model selection  error probability approaches zero as $ T $ grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab91f39",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "We now consider a problem that assumes timing protocol 2.\n",
    "\n",
    "A decision maker wants to classify components of an observed sequence $ \\{w_t\\}_{t=1}^T $ as having been drawn from either $ f $ or $ g $.\n",
    "\n",
    "The decision maker uses the following classification rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_t  & \\ {\\rm is \\ from \\  f  \\ if \\ } l_t > 1 \\\\\n",
    "w_t  & \\ {\\rm is \\ from \\  g  \\ if \\ } l_t \\leq 1 . \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Under this rule, the expected misclassification rate is\n",
    "\n",
    "\n",
    "<a id='equation-eq-classerrorprob'></a>\n",
    "$$\n",
    "p(\\textrm{misclassification}) = {1 \\over 2} (\\tilde \\alpha_t + \\tilde \\beta_t) \\tag{21.12}\n",
    "$$\n",
    "\n",
    "where $ \\tilde \\alpha_t = {\\rm Prob}(l_t < 1 \\mid f) $ and $ \\tilde \\beta_t = {\\rm Prob}(l_t \\geq 1 \\mid g) $.\n",
    "\n",
    "Since for each $ t $, the decision boundary is the same, the decision boundary can be computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62220d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "root = brentq(lambda w: f(w) / g(w) - 1, 0.001, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056e4f8",
   "metadata": {},
   "source": [
    "we can plot the distributions of $ f $ and $ g $ and the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df26252",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "w_range = np.linspace(1e-5, 1-1e-5, 1000)\n",
    "f_values = [f(w) for w in w_range]\n",
    "g_values = [g(w) for w in w_range]\n",
    "ratio_values = [f(w)/g(w) for w in w_range]\n",
    "\n",
    "ax.plot(w_range, f_values, 'b-', \n",
    "        label=r'$f(w) \\sim Beta(1,1)$', linewidth=2)\n",
    "ax.plot(w_range, g_values, 'r-', \n",
    "        label=r'$g(w) \\sim Beta(3,1.2)$', linewidth=2)\n",
    "\n",
    "type1_prob = 1 - beta_dist.cdf(root, F_a, F_b)\n",
    "type2_prob = beta_dist.cdf(root, G_a, G_b)\n",
    "\n",
    "w_type1 = w_range[w_range >= root]\n",
    "f_type1 = [f(w) for w in w_type1]\n",
    "ax.fill_between(w_type1, 0, f_type1, alpha=0.3, color='blue', \n",
    "                label=fr'$\\tilde \\alpha_t = {type1_prob:.2f}$')\n",
    "\n",
    "w_type2 = w_range[w_range <= root]\n",
    "g_type2 = [g(w) for w in w_type2]\n",
    "ax.fill_between(w_type2, 0, g_type2, alpha=0.3, color='red', \n",
    "                label=fr'$\\tilde \\beta_t = {type2_prob:.2f}$')\n",
    "\n",
    "ax.axvline(root, color='green', linestyle='--', alpha=0.7, \n",
    "            label=f'decision boundary: $w=${root:.3f}')\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('probability density')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3ba55",
   "metadata": {},
   "source": [
    "To  the left of the  green vertical line  $ g < f $,  so $ l_t < 1 $; therefore a  $ w_t $ that falls to the left of the green line is classified as a type $ g $ individual.\n",
    "\n",
    "- The shaded orange area equals $ \\beta $ – the probability of classifying someone as a type $ g $ individual when it is really a type $ f $ individual.  \n",
    "\n",
    "\n",
    "To  the right of the  green vertical line $ g > f $, so $ l_t >1 $; therefore  a  $ w_t $ that falls to the right  of the green line is classified as a type $ f $ individual.\n",
    "\n",
    "- The shaded blue area equals $ \\alpha $ – the probability of classifying someone as a type $ f $ when it is really a type $ g $ individual.  \n",
    "\n",
    "\n",
    "This gives us clues about how to compute the theoretical classification error probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2d89b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Compute theoretical tilde α_t and tilde β_t\n",
    "def α_integrand(w):\n",
    "    \"\"\"Integrand for tilde α_t = P(l_t < 1 | f)\"\"\"\n",
    "    return f(w) if f(w) / g(w) < 1 else 0\n",
    "\n",
    "def β_integrand(w):\n",
    "    \"\"\"Integrand for tilde β_t = P(l_t >= 1 | g)\"\"\"\n",
    "    return g(w) if f(w) / g(w) >= 1 else 0\n",
    "\n",
    "# Compute the integrals\n",
    "α_theory, _ = quad(α_integrand, 0, 1, limit=100)\n",
    "β_theory, _ = quad(β_integrand, 0, 1, limit=100)\n",
    "\n",
    "theory_error = 0.5 * (α_theory + β_theory)\n",
    "\n",
    "print(f\"theoretical tilde α_t = {α_theory:.4f}\")\n",
    "print(f\"theoretical tilde β_t = {β_theory:.4f}\")\n",
    "print(f\"theoretical classification error probability = {theory_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c4925",
   "metadata": {},
   "source": [
    "Now we simulate timing protocol 2 and compute the classification error probability.\n",
    "\n",
    "In the next cell, we also compare the theoretical classification accuracy to the empirical classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ff32a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "accuracy = np.empty(T_max)\n",
    "\n",
    "sequences_p2, true_sources_p2 = protocol_2(\n",
    "                    π_minus_1, T_max, N_simulations)\n",
    "l_ratios_p2, _ = compute_likelihood_ratios(sequences_p2, f, g)\n",
    "\n",
    "for t in range(T_max):\n",
    "    predictions = (l_ratios_p2[:, t] >= 1)\n",
    "    actual = true_sources_p2[:, t]\n",
    "    accuracy[t] = np.mean(predictions == actual)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, T_max + 1), accuracy, \n",
    "                'b-', linewidth=2, label='empirical accuracy')\n",
    "plt.axhline(1 - theory_error, color='r', linestyle='--', \n",
    "                label=f'theoretical accuracy = {1 - theory_error:.4f}')\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a6ea1",
   "metadata": {},
   "source": [
    "Let’s watch decisions made by  the two timing protocols as more and more observations accrue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2c307",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "ax.plot(T_range, error_prob, linewidth=2, \n",
    "        label='Protocol 1')\n",
    "ax.plot(T_range, 1-accuracy, linestyle='--', linewidth=2, \n",
    "        label=f'Protocol 2')\n",
    "ax.set_ylabel('error probability')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d6b8e",
   "metadata": {},
   "source": [
    "From the figure above, we can see:\n",
    "\n",
    "- For both timing protocols, the error probability starts at the same level, subject to a little randomness.  \n",
    "- For timing protocol 1, the error probability decreases as the sample size increases because we are  making just **one** decision – i.e., selecting whether $ f $ or $ g $ governs  **all** individuals.  More data provides better evidence.  \n",
    "- For timing protocol 2, the error probability remains constant because we are making **many** decisions – one classification decision for each observation.  \n",
    "\n",
    "\n",
    "**Remark:** Think about how laws of large numbers are applied to compute error probabilities for the model selection problem and the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabef7b",
   "metadata": {},
   "source": [
    "## Measuring discrepancies between distributions\n",
    "\n",
    "A plausible guess is that  the ability of a likelihood ratio to distinguish  distributions $ f $ and $ g $ depends on how “different” they are.\n",
    "\n",
    "But how should we measure  discrepancies between distributions?\n",
    "\n",
    "We’ve already encountered one discrepancy measure – the Kullback-Leibler (KL) divergence.\n",
    "\n",
    "We now briefly explore two alternative discrepancy  measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b74dcc",
   "metadata": {},
   "source": [
    "### Chernoff entropy\n",
    "\n",
    "Chernoff entropy was motivated by an early application of  the [theory of large deviations](https://en.wikipedia.org/wiki/Large_deviations_theory).\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Large deviation theory provides refinements of the central limit theorem.\n",
    "\n",
    "The Chernoff entropy between probability densities $ f $ and $ g $ is defined as:\n",
    "\n",
    "$$\n",
    "C(f,g) = - \\log \\min_{\\phi \\in (0,1)} \\int f^\\phi(x) g^{1-\\phi}(x) dx\n",
    "$$\n",
    "\n",
    "An upper bound on model selection error probabilty is\n",
    "\n",
    "$$\n",
    "e^{-C(f,g)T} .\n",
    "$$\n",
    "\n",
    "Thus,    Chernoff entropy is  an upper bound on  the exponential  rate at which  the selection error probability falls as sample size $ T $ grows.\n",
    "\n",
    "Let’s compute Chernoff entropy numerically with some Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3710ec4f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def chernoff_integrand(ϕ, f, g):\n",
    "    \"\"\"\n",
    "    Compute the integrand for Chernoff entropy\n",
    "    \"\"\"\n",
    "    def integrand(w):\n",
    "        return f(w)**ϕ * g(w)**(1-ϕ)\n",
    "\n",
    "    result, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return result\n",
    "\n",
    "def compute_chernoff_entropy(f, g):\n",
    "    \"\"\"\n",
    "    Compute Chernoff entropy C(f,g)\n",
    "    \"\"\"\n",
    "    def objective(ϕ):\n",
    "        return chernoff_integrand(ϕ, f, g)\n",
    "    \n",
    "    # Find the minimum over ϕ in (0,1)\n",
    "    result = minimize_scalar(objective, \n",
    "                             # For numerical stability\n",
    "                             bounds=(1e-5, 1-1e-5), \n",
    "                             method='bounded')\n",
    "    min_value = result.fun\n",
    "    ϕ_optimal = result.x\n",
    "    \n",
    "    chernoff_entropy = -np.log(min_value)\n",
    "    return chernoff_entropy, ϕ_optimal\n",
    "\n",
    "C_fg, ϕ_optimal = compute_chernoff_entropy(f, g)\n",
    "print(f\"Chernoff entropy C(f,g) = {C_fg:.4f}\")\n",
    "print(f\"Optimal ϕ = {ϕ_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecaf67d",
   "metadata": {},
   "source": [
    "Now let’s examine how $ e^{-C(f,g)T} $ behaves as a function of $ T $ and compare it to the model selection error probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e0ad8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "T_range = np.arange(1, T_max+1)\n",
    "chernoff_bound = np.exp(-C_fg * T_range)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.semilogy(T_range, chernoff_bound, 'r-', linewidth=2, \n",
    "           label=f'$e^{{-C(f,g)T}}$')\n",
    "ax.semilogy(T_range, error_prob, 'b-', linewidth=2, \n",
    "           label='Model selection error probability')\n",
    "\n",
    "ax.set_xlabel('T')\n",
    "ax.set_ylabel('error probability (log scale)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03c5e5",
   "metadata": {},
   "source": [
    "Evidently, $ e^{-C(f,g)T} $ is an upper bound on the error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63826e91",
   "metadata": {},
   "source": [
    "### Jensen-Shannon divergence\n",
    "\n",
    "The [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) is another  divergence measure.\n",
    "\n",
    "For probability densities $ f $ and $ g $, the **Jensen-Shannon divergence** is defined as:\n",
    "\n",
    "\n",
    "<a id='equation-eq-compute-js'></a>\n",
    "$$\n",
    "D(f,g) = \\frac{1}{2} KL(f, m) + \\frac{1}{2} KL(g, m) \\tag{21.13}\n",
    "$$\n",
    "\n",
    "where $ m = \\frac{1}{2}(f+g) $ is a mixture of $ f $ and $ g $.\n",
    "\n",
    "Below we compute Jensen-Shannon divergence numerically with some Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9c061",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_JS(f, g):\n",
    "    \"\"\"\n",
    "    Compute Jensen-Shannon divergence\n",
    "    \"\"\"\n",
    "    def m(w):\n",
    "        return 0.5 * (f(w) + g(w))\n",
    "    \n",
    "    js_div = 0.5 * compute_KL(f, m) + 0.5 * compute_KL(g, m)\n",
    "    return js_div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e106b",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">We studied KL divergence in the [section above](#rel-entropy) with respect to a reference distribution $ h $.\n",
    "\n",
    "Recall that  KL divergence $ KL(f, g) $ measures expected excess surprisal from using misspecified model $ g $ instead $ f $ when $ f $ is the true model.\n",
    "\n",
    "Because in general $ KL(f, g) \\neq KL(g, f) $, KL divergence is not symmetric, but Jensen-Shannon divergence is symmetric.\n",
    "\n",
    "(In fact, the square root of the Jensen-Shannon divergence is a metric referred to as the Jensen-Shannon distance.)\n",
    "\n",
    "As [(21.13)](#equation-eq-compute-js) shows, the Jensen-Shannon divergence computes average of the KL divergence of $ f $ and $ g $ with respect to a particular reference distribution $ m $ defined below the equation.\n",
    "\n",
    "Now let’s create a comparison table showing KL divergence, Jensen-Shannon divergence, and Chernoff entropy for a set of pairs of Beta distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994e6d7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "distribution_pairs = [\n",
    "    # (f_params, g_params)\n",
    "    ((1, 1), (0.1, 0.2)),\n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (0.3, 0.4)),\n",
    "    ((1, 1), (0.5, 0.5)),\n",
    "    ((1, 1), (0.7, 0.6)),\n",
    "    ((1, 1), (0.9, 0.8)),\n",
    "    ((1, 1), (1.1, 1.05)),\n",
    "    ((1, 1), (1.2, 1.1)),\n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),\n",
    "    ((1, 1), (2.5, 1.8)),\n",
    "    ((1, 1), (3, 1.2)),\n",
    "    ((1, 1), (4, 1)),\n",
    "    ((1, 1), (5, 1))\n",
    "]\n",
    "\n",
    "# Create comparison table\n",
    "results = []\n",
    "for i, ((f_a, f_b), (g_a, g_b)) in enumerate(distribution_pairs):\n",
    "    # Define the density functions\n",
    "    f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "    g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "    \n",
    "    # Compute measures\n",
    "    kl_fg = compute_KL(f, g)\n",
    "    kl_gf = compute_KL(g, f)\n",
    "    js_div = compute_JS(f, g)\n",
    "    chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "    \n",
    "    results.append({\n",
    "        'Pair (f, g)': f\"\\\\text{{Beta}}({f_a},{f_b}), \\\\text{{Beta}}({g_a},{g_b})\",\n",
    "        'KL(f, g)': f\"{kl_fg:.4f}\",\n",
    "        'KL(g, f)': f\"{kl_gf:.4f}\",\n",
    "        'JS': f\"{js_div:.4f}\",\n",
    "        'C': f\"{chernoff_ent:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by JS divergence\n",
    "df['JS_numeric'] = df['JS'].astype(float)\n",
    "df = df.sort_values('JS_numeric').drop('JS_numeric', axis=1)\n",
    "\n",
    "# Generate LaTeX table manually\n",
    "columns = ' & '.join([f'\\\\text{{{col}}}' for col in df.columns])\n",
    "rows = ' \\\\\\\\\\n'.join(\n",
    "    [' & '.join([f'{val}' for val in row]) \n",
    "     for row in df.values])\n",
    "\n",
    "latex_code = rf\"\"\"\n",
    "\\begin{{array}}{{lcccc}}\n",
    "{columns} \\\\\n",
    "\\hline\n",
    "{rows}\n",
    "\\end{{array}}\n",
    "\"\"\"\n",
    "\n",
    "display(Math(latex_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c896524",
   "metadata": {},
   "source": [
    "The above  table indicates how  Jensen-Shannon divergence,  and Chernoff entropy, and  KL divergence covary as we alter $ f $ and $ g $.\n",
    "\n",
    "Let’s also visualize how these diverge measures covary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2eeca",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "kl_fg_values = [float(result['KL(f, g)']) for result in results]\n",
    "js_values = [float(result['JS']) for result in results]\n",
    "chernoff_values = [float(result['C']) for result in results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# JS divergence and KL divergence\n",
    "axes[0].scatter(kl_fg_values, js_values, alpha=0.7, s=60)\n",
    "axes[0].set_xlabel('KL divergence KL(f, g)')\n",
    "axes[0].set_ylabel('JS divergence')\n",
    "axes[0].set_title('JS divergence and KL divergence')\n",
    "\n",
    "# Chernoff Entropy and JS divergence\n",
    "axes[1].scatter(js_values, chernoff_values, alpha=0.7, s=60)\n",
    "axes[1].set_xlabel('JS divergence')\n",
    "axes[1].set_ylabel('Chernoff entropy')\n",
    "axes[1].set_title('Chernoff entropy and JS divergence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e55b3",
   "metadata": {},
   "source": [
    "To make the comparison more concrete, let’s plot the distributions and the divergence measures for a few pairs of distributions.\n",
    "\n",
    "Note that the numbers on the title changes with the area of the overlaps of two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e194e65",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_dist_diff():\n",
    "    \"\"\"\n",
    "    Plot overlap of two distributions and divergence measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chose a subset of Beta distribution parameters\n",
    "    param_grid = [\n",
    "        ((1, 1), (1, 1)),   \n",
    "        ((1, 1), (1.5, 1.2)),\n",
    "        ((1, 1), (2, 1.5)),  \n",
    "        ((1, 1), (3, 1.2)),  \n",
    "        ((1, 1), (5, 1)),\n",
    "        ((1, 1), (0.3, 0.3))\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    \n",
    "    divergence_data = []\n",
    "    \n",
    "    for i, ((f_a, f_b), (g_a, g_b)) in enumerate(param_grid):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Create density functions\n",
    "        f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "        g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "        \n",
    "        # Compute divergence measures\n",
    "        kl_fg = compute_KL(f, g)\n",
    "        js_div = compute_JS(f, g) \n",
    "        chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "        \n",
    "        divergence_data.append({\n",
    "            'f_params': (f_a, f_b),\n",
    "            'g_params': (g_a, g_b),\n",
    "            'kl_fg': kl_fg,\n",
    "            'js_div': js_div,\n",
    "            'chernoff': chernoff_ent\n",
    "        })\n",
    "        \n",
    "        # Plot distributions\n",
    "        x_range = np.linspace(0, 1, 200)\n",
    "        f_vals = [f(x) for x in x_range]\n",
    "        g_vals = [g(x) for x in x_range]\n",
    "        \n",
    "        axes[row, col].plot(x_range, f_vals, 'b-', linewidth=2, \n",
    "                           label=f'f ~ Beta({f_a},{f_b})')\n",
    "        axes[row, col].plot(x_range, g_vals, 'r-', linewidth=2, \n",
    "                           label=f'g ~ Beta({g_a},{g_b})')\n",
    "        \n",
    "        # Fill overlap region\n",
    "        overlap = np.minimum(f_vals, g_vals)\n",
    "        axes[row, col].fill_between(x_range, 0, overlap, alpha=0.3, \n",
    "                                   color='purple', label='overlap')\n",
    "        \n",
    "        # Add divergence information\n",
    "        axes[row, col].set_title(\n",
    "            f'KL(f, g)={kl_fg:.3f}, JS={js_div:.3f}, C={chernoff_ent:.3f}',\n",
    "            fontsize=12)\n",
    "        axes[row, col].legend(fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return divergence_data\n",
    "\n",
    "divergence_data = plot_dist_diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b5e0d",
   "metadata": {},
   "source": [
    "### Error probability and divergence measures\n",
    "\n",
    "Now let’s return to our guess that the error probability at large sample sizes is related to the Chernoff entropy  between two distributions.\n",
    "\n",
    "We verify this by computing the correlation between the log of the error probability at $ T=50 $ under Timing Protocol 1 and the divergence measures.\n",
    "\n",
    "In the simulation below, nature draws $ N / 2 $ sequences from $ g $ and $ N/2 $ sequences from $ f $.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Nature does this rather than flipping a fair coin to decide whether to draw from $ g $ or $ f $ once and for all before each simulation of length $ T $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb355dab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters for simulation\n",
    "T_large = 50\n",
    "N_sims = 5000\n",
    "N_half = N_sims // 2\n",
    "\n",
    "# Initialize arrays\n",
    "n_pairs = len(distribution_pairs)\n",
    "kl_fg_vals = np.zeros(n_pairs)\n",
    "kl_gf_vals = np.zeros(n_pairs) \n",
    "js_vals = np.zeros(n_pairs)\n",
    "chernoff_vals = np.zeros(n_pairs)\n",
    "error_probs = np.zeros(n_pairs)\n",
    "pair_names = []\n",
    "\n",
    "for i, ((f_a, f_b), (g_a, g_b)) in enumerate(distribution_pairs):\n",
    "    # Create density functions\n",
    "    f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "    g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "\n",
    "    # Compute divergence measures\n",
    "    kl_fg_vals[i] = compute_KL(f, g)\n",
    "    kl_gf_vals[i] = compute_KL(g, f)\n",
    "    js_vals[i] = compute_JS(f, g)\n",
    "    chernoff_vals[i], _ = compute_chernoff_entropy(f, g)\n",
    "\n",
    "    # Generate samples\n",
    "    sequences_f = np.random.beta(f_a, f_b, (N_half, T_large))\n",
    "    sequences_g = np.random.beta(g_a, g_b, (N_half, T_large))\n",
    "\n",
    "    # Compute likelihood ratios and cumulative products\n",
    "    _, L_cumulative_f = compute_likelihood_ratios(sequences_f, f, g)\n",
    "    _, L_cumulative_g = compute_likelihood_ratios(sequences_g, f, g)\n",
    "    \n",
    "    # Get final values\n",
    "    L_cumulative_f = L_cumulative_f[:, -1]\n",
    "    L_cumulative_g = L_cumulative_g[:, -1]\n",
    "\n",
    "    # Calculate error probabilities\n",
    "    error_probs[i] = 0.5 * (np.mean(L_cumulative_f < 1) + \n",
    "                            np.mean(L_cumulative_g >= 1))\n",
    "    pair_names.append(f\"Beta({f_a},{f_b}) and Beta({g_a},{g_b})\")\n",
    "\n",
    "cor_data =  {\n",
    "    'kl_fg': kl_fg_vals,\n",
    "    'kl_gf': kl_gf_vals,\n",
    "    'js': js_vals, \n",
    "    'chernoff': chernoff_vals,\n",
    "    'error_prob': error_probs,\n",
    "    'names': pair_names,\n",
    "    'T': T_large}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17a103",
   "metadata": {},
   "source": [
    "Now let’s visualize the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464391e8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_error_divergence(data):\n",
    "    \"\"\"\n",
    "    Plot correlations between error probability and divergence measures.\n",
    "    \"\"\"\n",
    "    # Filter out near-zero error probabilities for log scale\n",
    "    nonzero_mask = data['error_prob'] > 1e-6\n",
    "    log_error = np.log(data['error_prob'][nonzero_mask])\n",
    "    js_vals = data['js'][nonzero_mask]\n",
    "    chernoff_vals = data['chernoff'][nonzero_mask]\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # function for plotting correlation\n",
    "    def plot_correlation(ax, x_vals, x_label, color):\n",
    "        ax.scatter(x_vals, log_error, alpha=0.7, s=60, color=color)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(f'log(Error probability) at T={data[\"T\"]}')\n",
    "        \n",
    "        # Calculate correlation and trend line\n",
    "        corr = np.corrcoef(x_vals, log_error)[0, 1]\n",
    "        z = np.polyfit(x_vals, log_error, 2)\n",
    "        x_trend = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "        ax.plot(x_trend, np.poly1d(z)(x_trend), \n",
    "                \"r--\", alpha=0.8, linewidth=2)\n",
    "        ax.set_title(f'Log error probability and {x_label}\\n'\n",
    "                     f'Correlation = {corr:.3f}')\n",
    "    \n",
    "    # Plot both correlations\n",
    "    plot_correlation(ax1, js_vals, 'JS divergence', 'C0')\n",
    "    plot_correlation(ax2, chernoff_vals, 'Chernoff entropy', 'C1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_error_divergence(cor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f95dc",
   "metadata": {},
   "source": [
    "Evidently, Chernoff entropy and Jensen-Shannon entropy each covary tightly with the model selection error probability.\n",
    "\n",
    "We’ll encounter related ideas in [A Problem that Stumped Milton Friedman](https://python.quantecon.org/wald_friedman.html) very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2fcd7",
   "metadata": {},
   "source": [
    "## Related Lectures\n",
    "\n",
    "Likelihood processes play an important role in Bayesian learning, as described in [Likelihood Ratio Processes and Bayesian Learning](https://python.quantecon.org/likelihood_bayes.html)\n",
    "and as applied in [Job Search VII: Search with Learning](https://python.quantecon.org/odu.html).\n",
    "\n",
    "Likelihood ratio processes appear again in [Additive and Multiplicative Functionals](https://python-advanced.quantecon.org/additive_functionals.html), which contains another illustration\n",
    "of the **peculiar property** of likelihood ratio processes described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e813be",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea8e39",
   "metadata": {},
   "source": [
    "## Exercise 21.1\n",
    "\n",
    "Consider the setting where nature generates data from a third density $ h $.\n",
    "\n",
    "Let $ \\{w_t\\}_{t=1}^T $ be IID draws from $ h $, and let $ L_t = L(w^t) $ be the likelihood ratio process defined as in the lecture.\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$\n",
    "\\frac{1}{t} E_h[\\log L_t] = K_g - K_f\n",
    "$$\n",
    "\n",
    "with finite $ K_g, K_f < \\infty $, $ E_h |\\log f(W)| < \\infty $ and $ E_h |\\log g(W)| < \\infty $.\n",
    "\n",
    "*Hint:* Start by expressing $ \\log L_t $ as a sum of $ \\log \\ell(w_i) $ terms and compare with the definition of $ K_f $ and $ K_g $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea7585",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 21.1](https://python.quantecon.org/#lr_ex1)\n",
    "\n",
    "Since $ w_1, \\ldots, w_t $ are IID draws from $ h $, we can write\n",
    "\n",
    "$$\n",
    "\\log L_t = \\log \\prod_{i=1}^t \\ell(w_i) = \\sum_{i=1}^t \\log \\ell(w_i) = \\sum_{i=1}^t \\log \\frac{f(w_i)}{g(w_i)}\n",
    "$$\n",
    "\n",
    "Taking the expectation under $ h $\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] \n",
    "= E_h\\left[\\sum_{i=1}^t \\log \\frac{f(w_i)}{g(w_i)}\\right] \n",
    "= \\sum_{i=1}^t E_h\\left[\\log \\frac{f(w_i)}{g(w_i)}\\right]\n",
    "$$\n",
    "\n",
    "Since the $ w_i $ are identically distributed\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] = t \\cdot E_h\\left[\\log \\frac{f(w)}{g(w)}\\right]\n",
    "$$\n",
    "\n",
    "where $ w \\sim h $.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\frac{1}{t} E_h[\\log L_t] = E_h\\left[\\log \\frac{f(w)}{g(w)}\\right] = E_h[\\log f(w)] - E_h[\\log g(w)]\n",
    "$$\n",
    "\n",
    "Now, from the definition of Kullback-Leibler divergence\n",
    "\n",
    "$$\n",
    "K_f = KL(h, f) = \\int h(w) \\log \\frac{h(w)}{f(w)} dw = E_h[\\log h(w)] - E_h[\\log f(w)]\n",
    "$$\n",
    "\n",
    "This gives us\n",
    "\n",
    "$$\n",
    "E_h[\\log f(w)] = E_h[\\log h(w)] - K_f\n",
    "$$\n",
    "\n",
    "Similarly\n",
    "\n",
    "$$\n",
    "E_h[\\log g(w)] = E_h[\\log h(w)] - K_g\n",
    "$$\n",
    "\n",
    "Substituting back\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{t} E_h[\\log L_t] &= E_h[\\log f(w)] - E_h[\\log g(w)] \\\\\n",
    "&= [E_h[\\log h(w)] - K_f] - [E_h[\\log h(w)] - K_g] \\\\\n",
    "&= K_g - K_f\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e13618",
   "metadata": {},
   "source": [
    "## Exercise 21.2\n",
    "\n",
    "Building on Exercise 21.1, use the result to explain what happens to $ L_t $ as $ t \\to \\infty $ in the following cases:\n",
    "\n",
    "1. When $ K_g > K_f $ (i.e., $ f $ is “closer” to $ h $ than $ g $ is)  \n",
    "1. When $ K_g < K_f $ (i.e., $ g $ is “closer” to $ h $ than $ f $ is)  \n",
    "\n",
    "\n",
    "Relate your answer to the simulation results shown in the [Kullback-Leibler Divergence](#rel-entropy) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad234ec1",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 21.2](https://python.quantecon.org/#lr_ex2)\n",
    "\n",
    "From Exercise 21.1, we know that:\n",
    "\n",
    "$$\n",
    "\\frac{1}{t} E_h[\\log L_t] = K_g - K_f\n",
    "$$\n",
    "\n",
    "**Case 1:** When $ K_g > K_f $\n",
    "\n",
    "Here, $ f $ is “closer” to $ h $ than $ g $ is. Since $ K_g - K_f > 0 $\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] = t \\cdot (K_g - K_f) \\to +\\infty \\text{ as } t \\to \\infty\n",
    "$$\n",
    "\n",
    "By the Law of Large Numbers, $ \\frac{1}{t} \\log L_t \\to K_g - K_f > 0 $ almost surely.\n",
    "\n",
    "Therefore $ L_t \\to +\\infty $ almost surely.\n",
    "\n",
    "**Case 2:** When $ K_g < K_f $\n",
    "\n",
    "Here, $ g $ is “closer” to $ h $ than $ f $ is. Since $ K_g - K_f < 0 $\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] = t \\cdot (K_g - K_f) \\to -\\infty \\text{ as } t \\to \\infty\n",
    "$$\n",
    "\n",
    "Therefore by similar reasoning $ L_t \\to 0 $ almost surely."
   ]
  }
 ],
 "metadata": {
  "date": 1753917525.4646416,
  "filename": "likelihood_ratio_process.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Likelihood Ratio Processes"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}