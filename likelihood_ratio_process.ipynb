{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f292c1",
   "metadata": {},
   "source": [
    "\n",
    "<a id='likelihood-ratio-process'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a5ddc",
   "metadata": {},
   "source": [
    "# Likelihood Ratio Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d12e2",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Likelihood Ratio Processes](#Likelihood-Ratio-Processes)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Likelihood Ratio Process](#Likelihood-Ratio-Process)  \n",
    "  - [Nature Permanently Draws from Density g](#Nature-Permanently-Draws-from-Density-g)  \n",
    "  - [Peculiar Property](#Peculiar-Property)  \n",
    "  - [Nature Permanently Draws from Density f](#Nature-Permanently-Draws-from-Density-f)  \n",
    "  - [Likelihood Ratio Test](#Likelihood-Ratio-Test)  \n",
    "  - [Kullback–Leibler Divergence](#Kullback–Leibler-Divergence)  \n",
    "  - [Hypothesis Testing and Classification](#Hypothesis-Testing-and-Classification)  \n",
    "  - [Model Selection Mistake Probability](#Model-Selection-Mistake-Probability)  \n",
    "  - [Classification](#Classification)  \n",
    "  - [Sequels](#Sequels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9e41c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture describes likelihood ratio processes and some of their uses.\n",
    "\n",
    "We’ll study the same  setting that is also used in  [this lecture on exchangeability](https://python.quantecon.org/exchangeable.html).\n",
    "\n",
    "Among  things that we’ll learn  are\n",
    "\n",
    "- How a likelihood ratio process is a key ingredient in frequentist hypothesis testing  \n",
    "- How a **receiver operator characteristic curve** summarizes information about a false alarm probability and power in frequentist hypothesis testing  \n",
    "- How a  statistician can combine frequentist probabilities of type I and type II errors to form posterior probabilities of erroneous model selection or missclassification of individuals  \n",
    "- How during World War II the United States Navy devised a decision rule that Captain Garret L. Schyler challenged, a topic to be studied in  [this lecture](https://python.quantecon.org/wald_friedman.html)  \n",
    "- A peculiar property of likelihood ratio processes  \n",
    "\n",
    "\n",
    "Let’s start  by importing some Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf76015",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #set default figure size\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import beta as beta_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59bf89",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Process\n",
    "\n",
    "A nonnegative random variable $ W $ has one of two probability density functions, either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "Before the beginning of time, nature once and for all decides whether she will draw a sequence of IID draws from either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "We will sometimes let $ q $ be the density that nature chose once and for all, so\n",
    "that $ q $ is either $ f $ or $ g $, permanently.\n",
    "\n",
    "Nature knows which density it permanently draws from, but we the observers do not.\n",
    "\n",
    "We know both $ f $ and $ g $ but we don’t know which density nature\n",
    "chose.\n",
    "\n",
    "But we want to know.\n",
    "\n",
    "To do that, we use observations.\n",
    "\n",
    "We observe a sequence $ \\{w_t\\}_{t=1}^T $ of $ T $ IID draws that we know came from either $ f $ or $ g $.\n",
    "\n",
    "We want to use these observations to infer whether nature chose $ f $ or $ g $.\n",
    "\n",
    "A **likelihood ratio process** is a useful tool for this task.\n",
    "\n",
    "To begin, we define key component of a likelihood ratio process, namely, the time $ t $ likelihood ratio  as the random variable\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "We assume that $ f $ and $ g $ both put positive probabilities on the\n",
    "same intervals of possible realizations of the random variable $ W $.\n",
    "\n",
    "That means that under the $ g $ density,  $ \\ell (w_t)=\n",
    "\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)} $\n",
    "is a nonnegative  random variable with mean $ 1 $.\n",
    "\n",
    "A **likelihood ratio process** for sequence\n",
    "$ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ is defined as\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "where $ w^t=\\{ w_1,\\dots,w_t\\} $ is a history of\n",
    "observations up to and including time $ t $.\n",
    "\n",
    "Sometimes for shorthand we’ll write $ L_t =  L(w^t) $.\n",
    "\n",
    "Notice that the likelihood process satisfies the *recursion*\n",
    "\n",
    "$$\n",
    "L(w^t) = \\ell (w_t) L (w^{t-1}) .\n",
    "$$\n",
    "\n",
    "The likelihood ratio and its logarithm are key tools for making\n",
    "inferences using a classic frequentist approach due to Neyman and\n",
    "Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id256)].\n",
    "\n",
    "To help us appreciate how things work, the following Python code evaluates $ f $ and $ g $ as two different\n",
    "beta distributions, then computes and simulates an associated likelihood\n",
    "ratio process by generating a sequence $ w^t $ from one of the two\n",
    "probability distributions, for example, a sequence of  IID draws from $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164ac58",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters in the two beta distributions.\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# The two density functions.\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf3c95",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate(a, b, T=50, N=500):\n",
    "    '''\n",
    "    Generate N sets of T observations of the likelihood ratio,\n",
    "    return as N x T matrix.\n",
    "\n",
    "    '''\n",
    "\n",
    "    l_arr = np.empty((N, T))\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        for j in range(T):\n",
    "            w = np.random.beta(a, b)\n",
    "            l_arr[i, j] = f(w) / g(w)\n",
    "\n",
    "    return l_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f08cbc",
   "metadata": {},
   "source": [
    "\n",
    "<a id='nature-likeli'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b741af9",
   "metadata": {},
   "source": [
    "## Nature Permanently Draws from Density g\n",
    "\n",
    "We first simulate the likelihood ratio process when nature permanently\n",
    "draws from $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915bc03",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d24ca",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_g.shape\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    plt.plot(range(T), l_seq_g[i, :], color='b', lw=0.8, alpha=0.5)\n",
    "\n",
    "plt.ylim([0, 3])\n",
    "plt.title(\"$L(w^{t})$ paths\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25023110",
   "metadata": {},
   "source": [
    "Evidently, as sample length $ T $ grows, most probability mass\n",
    "shifts toward zero\n",
    "\n",
    "To see it this more clearly clearly, we plot over time the fraction of\n",
    "paths $ L\\left(w^{t}\\right) $ that fall in the interval\n",
    "$ \\left[0, 0.01\\right] $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14a768",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(T), np.sum(l_seq_g <= 0.01, axis=0) / N)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ef898",
   "metadata": {},
   "source": [
    "Despite the evident convergence of most probability mass to a\n",
    "very small interval near $ 0 $,  the unconditional mean of\n",
    "$ L\\left(w^t\\right) $ under probability density $ g $ is\n",
    "identically $ 1 $ for all $ t $.\n",
    "\n",
    "To verify this assertion, first notice that as mentioned earlier the unconditional mean\n",
    "$ E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right] $ is $ 1 $ for\n",
    "all $ t $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right]  &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=\\int f\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=1,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which immediately implies\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[L\\left(w^{1}\\right)\\bigm|q=g\\right]  &=E\\left[\\ell \\left(w_{1}\\right)\\bigm|q=g\\right]\\\\\n",
    "    &=1.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because $ L(w^t) = \\ell(w_t) L(w^{t-1}) $ and\n",
    "$ \\{w_t\\}_{t=1}^t $ is an IID sequence, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]  &=E\\left[L\\left(w^{t-1}\\right)\\ell \\left(w_{t}\\right)\\bigm|q=g\\right] \\\\\n",
    "    &=E\\left[L\\left(w^{t-1}\\right)E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g,w^{t-1}\\right]\\bigm|q=g\\right] \\\\\n",
    "    &=E\\left[L\\left(w^{t-1}\\right)E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right]\\bigm|q=g\\right] \\\\\n",
    "    &=E\\left[L\\left(w^{t-1}\\right)\\bigm|q=g\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for any $ t \\geq 1 $.\n",
    "\n",
    "Mathematical induction implies\n",
    "$ E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]=1 $ for all\n",
    "$ t \\geq 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10402f69",
   "metadata": {},
   "source": [
    "## Peculiar Property\n",
    "\n",
    "How can $ E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]=1 $ possibly be true when most  probability mass of the likelihood\n",
    "ratio process is piling up near $ 0 $ as\n",
    "$ t \\rightarrow + \\infty $?\n",
    "\n",
    "The answer is that as $ t \\rightarrow + \\infty $, the\n",
    "distribution of $ L_t $ becomes more and more fat-tailed:\n",
    "enough  mass shifts to larger and larger values of $ L_t $ to make\n",
    "the mean of $ L_t $ continue to be one despite most of the probability mass piling up\n",
    "near $ 0 $.\n",
    "\n",
    "To illustrate this peculiar property, we simulate many paths and\n",
    "calculate the unconditional mean of $ L\\left(w^t\\right) $ by\n",
    "averaging across these many paths at each $ t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9d2d7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b, N=50000)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7420bdc",
   "metadata": {},
   "source": [
    "It would be useful to use simulations to verify that  unconditional means\n",
    "$ E\\left[L\\left(w^{t}\\right)\\right] $ equal unity by averaging across sample\n",
    "paths.\n",
    "\n",
    "But it would be too computer-time-consuming for us to that  here simply by applying a standard Monte Carlo simulation approach.\n",
    "\n",
    "The reason is that the distribution of $ L\\left(w^{t}\\right) $ is extremely skewed for large values of  $ t $.\n",
    "\n",
    "Because the probability density in the right tail is close to $ 0 $,  it just takes too much computer time to sample enough points from the right tail.\n",
    "\n",
    "We explain the problem in more detail  in [this lecture](https://python.quantecon.org/imp_sample.html).\n",
    "\n",
    "There we describe a way to an alternative way to compute the mean of a likelihood ratio by computing the mean of a *different* random variable by sampling from  a *different* probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21c47b",
   "metadata": {},
   "source": [
    "## Nature Permanently Draws from Density f\n",
    "\n",
    "Now suppose that before time $ 0 $ nature permanently decided to draw repeatedly from density $ f $.\n",
    "\n",
    "While the mean of the likelihood ratio $ \\ell \\left(w_{t}\\right) $ under density\n",
    "$ g $ is $ 1 $, its mean under the density $ f $ exceeds one.\n",
    "\n",
    "To see this, we compute\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=f\\right]  &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}f\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=\\int \\ell \\left(w_{t}\\right)^{2}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=E\\left[\\ell \\left(w_{t}\\right)^{2}\\mid q=g\\right] \\\\\n",
    "    &=E\\left[\\ell \\left(w_{t}\\right)\\mid q=g\\right]^{2}+Var\\left(\\ell \\left(w_{t}\\right)\\mid q=g\\right) \\\\\n",
    "    &>E\\left[\\ell \\left(w_{t}\\right)\\mid q=g\\right]^{2} = 1 \\\\\n",
    "       \\end{aligned}\n",
    "$$\n",
    "\n",
    "This in turn implies that the unconditional mean of the likelihood ratio process $ L(w^t) $\n",
    "diverges toward $ + \\infty $.\n",
    "\n",
    "Simulations below confirm this conclusion.\n",
    "\n",
    "Please note the scale of the $ y $ axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509df96",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_f = simulate(F_a, F_b, N=50000)\n",
    "l_seq_f = np.cumprod(l_arr_f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61925fa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_f.shape\n",
    "plt.plot(range(T), np.mean(l_seq_f, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9bfc43",
   "metadata": {},
   "source": [
    "We also plot the probability that $ L\\left(w^t\\right) $ falls into\n",
    "the interval $ [10000, \\infty) $ as a function of time and watch how\n",
    "fast probability mass diverges  to $ +\\infty $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b7b92",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(T), np.sum(l_seq_f > 10000, axis=0) / N)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3b63e",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Test\n",
    "\n",
    "We now describe how to employ the machinery\n",
    "of Neyman and Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id256)] to test the hypothesis that  history $ w^t $ is generated by repeated\n",
    "IID draws from density $ g $.\n",
    "\n",
    "Denote $ q $ as the data generating process, so that\n",
    "$ q=f \\text{ or } g $.\n",
    "\n",
    "Upon observing a sample $ \\{W_i\\}_{i=1}^t $, we want to decide\n",
    "whether nature is drawing from $ g $ or from $ f $ by performing  a (frequentist)\n",
    "hypothesis test.\n",
    "\n",
    "We specify\n",
    "\n",
    "- Null hypothesis $ H_0 $: $ q=f $,  \n",
    "- Alternative hypothesis $ H_1 $: $ q=g $.  \n",
    "\n",
    "\n",
    "Neyman and Pearson proved that the best way to test this hypothesis is to use a **likelihood ratio test** that takes the\n",
    "form:\n",
    "\n",
    "- accept $ H_0 $ if $ L(W^t) > c $,  \n",
    "- reject $ H_0 $ if $ L(W^t) < c $,  \n",
    "\n",
    "\n",
    "where $ c $ is a given  discrimination threshold.\n",
    "\n",
    "Setting $ c =1 $ is a common choice.\n",
    "\n",
    "We’ll discuss consequences of other choices of $ c $ below.\n",
    "\n",
    "This test is *best* in the sense that it is  **uniformly most powerful**.\n",
    "\n",
    "To understand what this means, we have to define probabilities of two important events that\n",
    "allow us to characterize a test associated with a given\n",
    "threshold $ c $.\n",
    "\n",
    "The two probabilities are:\n",
    "\n",
    "- Probability of a Type I error in which we reject $ H_0 $ when it is true:  \n",
    "  $$\n",
    "  \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\}\n",
    "  $$\n",
    "- Probability of a Type II error in which we accept $ H_0 $ when it is false:  \n",
    "  $$\n",
    "  \\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)>c\\mid q=g\\right\\}\n",
    "  $$\n",
    "\n",
    "\n",
    "These two probabilities underly  the following two concepts:\n",
    "\n",
    "- Probability of false alarm (= significance level = probability of\n",
    "  Type I error):  \n",
    "  $$\n",
    "  \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\}\n",
    "  $$\n",
    "- Probability of detection (= power = 1 minus probability\n",
    "  of Type II error):  \n",
    "  $$\n",
    "  1-\\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=g\\right\\}\n",
    "  $$\n",
    "\n",
    "\n",
    "The [Neyman-Pearson\n",
    "Lemma](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma)\n",
    "states that among all possible tests, a likelihood ratio test\n",
    "maximizes the probability of detection for a given probability of false\n",
    "alarm.\n",
    "\n",
    "Another way to say the same thing is that  among all possible tests, a likelihood ratio test\n",
    "maximizes **power** for a given **significance level**.\n",
    "\n",
    "We want a small probability of\n",
    "false alarm and a large probability of detection.\n",
    "\n",
    "With sample size $ t $ fixed, we can change our two probabilities by\n",
    "adjusting $ c $.\n",
    "\n",
    "A troublesome “that’s life” fact is that these two probabilities  move in the same direction as we vary the critical value\n",
    "$ c $.\n",
    "\n",
    "Without specifying quantitative losses from making Type I and Type II errors, there is little that we can say\n",
    "about how we *should*  trade off probabilities of the two types of mistakes.\n",
    "\n",
    "We do know that increasing sample size $ t $ improves\n",
    "statistical inference.\n",
    "\n",
    "Below we plot some informative figures that illustrate this.\n",
    "\n",
    "We also present a classical frequentist method for choosing a sample\n",
    "size $ t $.\n",
    "\n",
    "Let’s start with a case in which we fix the threshold $ c $ at\n",
    "$ 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf3748",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "c = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb3652",
   "metadata": {},
   "source": [
    "Below we plot empirical distributions of logarithms of the cumulative\n",
    "likelihood ratios simulated above, which are generated by either\n",
    "$ f $ or $ g $.\n",
    "\n",
    "Taking logarithms has no effect on calculating the probabilities because\n",
    "the log  is a monotonic transformation.\n",
    "\n",
    "As $ t $ increases, the probabilities of making Type I and Type II\n",
    "errors both decrease, which is good.\n",
    "\n",
    "This is because most of the probability mass of log$ (L(w^t)) $\n",
    "moves toward $ -\\infty $ when $ g $ is the data generating\n",
    "process, ; while log$ (L(w^t)) $ goes to\n",
    "$ \\infty $ when data are generated by $ f $.\n",
    "\n",
    "That disparate  behavior of log$ (L(w^t)) $ under $ f $ and $ q $\n",
    "is what makes it possible eventually to distinguish\n",
    "$ q=f $ from $ q=g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637cfd39",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('distribution of $log(L(w^t))$ under f or under g', fontsize=15)\n",
    "\n",
    "for i, t in enumerate([1, 7, 14, 21]):\n",
    "    nr = i // 2\n",
    "    nc = i % 2\n",
    "\n",
    "    axs[nr, nc].axvline(np.log(c), color=\"k\", ls=\"--\")\n",
    "\n",
    "    hist_f, x_f = np.histogram(np.log(l_seq_f[:, t]), 200, density=True)\n",
    "    hist_g, x_g = np.histogram(np.log(l_seq_g[:, t]), 200, density=True)\n",
    "\n",
    "    axs[nr, nc].plot(x_f[1:], hist_f, label=\"dist under f\")\n",
    "    axs[nr, nc].plot(x_g[1:], hist_g, label=\"dist under g\")\n",
    "\n",
    "    for i, (x, hist, label) in enumerate(zip([x_f, x_g], [hist_f, hist_g], [\"Type I error\", \"Type II error\"])):\n",
    "        ind = x[1:] <= np.log(c) if i == 0 else x[1:] > np.log(c)\n",
    "        axs[nr, nc].fill_between(x[1:][ind], hist[ind], alpha=0.5, label=label)\n",
    "\n",
    "    axs[nr, nc].legend()\n",
    "    axs[nr, nc].set_title(f\"t={t}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6cb5c2",
   "metadata": {},
   "source": [
    "In the above graphs,\n",
    "\n",
    "- the blue areas are related to but not equal to probabilities $ \\alpha $ of a type I error because\n",
    "  they are integrals of $ \\log L_t $, not integrals of $ L_t $, over  rejection region $ L_t < 1 $  \n",
    "- the orange areas are related to but not equal to probabilities $ \\beta $ of a type II error because\n",
    "  they are integrals of $ \\log L_t $, not integrals of $ L_t $, over  acceptance region $ L_t > 1 $  \n",
    "\n",
    "\n",
    "When we hold $ c $ fixed at $ c=1 $, the following graph shows  that\n",
    "\n",
    "- the probability of detection monotonically increases with increases in\n",
    "  $ t $  \n",
    "- the probability of a false alarm monotonically decreases with increases in $ t $.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c6c38",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PD = np.empty(T)\n",
    "PFA = np.empty(T)\n",
    "\n",
    "for t in range(T):\n",
    "    PD[t] = np.sum(l_seq_g[:, t] < c) / N\n",
    "    PFA[t] = np.sum(l_seq_f[:, t] < c) / N\n",
    "\n",
    "plt.plot(range(T), PD, label=\"Probability of detection\")\n",
    "plt.plot(range(T), PFA, label=\"Probability of false alarm\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.title(\"$c=1$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd2b09",
   "metadata": {},
   "source": [
    "For a given sample size $ t $,  the threshold $ c $ uniquely pins down  probabilities\n",
    "of both types of error.\n",
    "\n",
    "If for a fixed $ t $ we now free up and move $ c $, we will sweep out the probability\n",
    "of detection as a function of the probability of false alarm.\n",
    "\n",
    "This produces  a [receiver operating characteristic\n",
    "curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "\n",
    "Below, we plot receiver operating characteristic curves for different\n",
    "sample sizes $ t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea63b6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PFA = np.arange(0, 100, 1)\n",
    "\n",
    "for t in range(1, 15, 4):\n",
    "    percentile = np.percentile(l_seq_f[:, t], PFA)\n",
    "    PD = [np.sum(l_seq_g[:, t] < p) / N for p in percentile]\n",
    "\n",
    "    plt.plot(PFA / 100, PD, label=f\"t={t}\")\n",
    "\n",
    "plt.scatter(0, 1, label=\"perfect detection\")\n",
    "plt.plot([0, 1], [0, 1], color='k', ls='--', label=\"random detection\")\n",
    "\n",
    "plt.arrow(0.5, 0.5, -0.15, 0.15, head_width=0.03)\n",
    "plt.text(0.35, 0.7, \"better\")\n",
    "plt.xlabel(\"Probability of false alarm\")\n",
    "plt.ylabel(\"Probability of detection\")\n",
    "plt.legend()\n",
    "plt.title(\"Receiver Operating Characteristic Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6709105",
   "metadata": {},
   "source": [
    "Notice that as $ t $ increases, we are assured a larger probability\n",
    "of detection and a smaller probability of false alarm associated with\n",
    "a given discrimination threshold $ c $.\n",
    "\n",
    "As $ t \\rightarrow + \\infty $, we approach the perfect detection\n",
    "curve that is indicated by a right angle hinging on the blue dot.\n",
    "\n",
    "For a given sample size $ t $, the discrimination threshold $ c $ determines a point on the receiver operating\n",
    "characteristic curve.\n",
    "\n",
    "It is up to the test designer to trade off probabilities of\n",
    "making the two types of errors.\n",
    "\n",
    "But we know how to choose the smallest sample size to achieve given targets for\n",
    "the probabilities.\n",
    "\n",
    "Typically, frequentists aim for a high probability of detection that\n",
    "respects an upper bound on the probability of false alarm.\n",
    "\n",
    "Below we show an example in which we fix the probability of false alarm at\n",
    "$ 0.05 $.\n",
    "\n",
    "The required sample size for making a decision is then determined by a\n",
    "target probability of detection, for example, $ 0.9 $, as depicted in the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f04a3e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PFA = 0.05\n",
    "PD = np.empty(T)\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    c = np.percentile(l_seq_f[:, t], PFA * 100)\n",
    "    PD[t] = np.sum(l_seq_g[:, t] < c) / N\n",
    "\n",
    "plt.plot(range(T), PD)\n",
    "plt.axhline(0.9, color=\"k\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Probability of detection\")\n",
    "plt.title(f\"Probability of false alarm={PFA}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba717b9",
   "metadata": {},
   "source": [
    "The United States Navy evidently used a procedure like this to select a sample size $ t $ for doing quality\n",
    "control tests during World War II.\n",
    "\n",
    "A Navy Captain who had been ordered to perform tests of this kind had doubts about it that he\n",
    "presented to Milton Friedman, as we describe in  [this lecture](https://python.quantecon.org/wald_friedman.html).\n",
    "\n",
    "\n",
    "<a id='rel-entropy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582dccc",
   "metadata": {},
   "source": [
    "## Kullback–Leibler Divergence\n",
    "\n",
    "Now let’s consider a case in which neither $ g $ nor $ f $\n",
    "generates the data.\n",
    "\n",
    "Instead, a third distribution $ h $ does.\n",
    "\n",
    "Let’s watch how how the cumulated likelihood ratios $ f/g $ behave\n",
    "when $ h $ governs the data.\n",
    "\n",
    "A key tool here is called **Kullback–Leibler divergence**.\n",
    "\n",
    "It is also called **relative entropy**.\n",
    "\n",
    "It measures how one probability distribution differs from another.\n",
    "\n",
    "In our application, we want to measure how $ f $ or $ g $\n",
    "diverges from $ h $\n",
    "\n",
    "The two Kullback–Leibler divergences pertinent for us are $ K_f $\n",
    "and $ K_g $ defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{f}   &=E_{h}\\left[\\log\\left(\\frac{f\\left(w\\right)}{h\\left(w\\right)}\\right)\\frac{f\\left(w\\right)}{h\\left(w\\right)}\\right] \\\\\n",
    "    &=\\int\\log\\left(\\frac{f\\left(w\\right)}{h\\left(w\\right)}\\right)\\frac{f\\left(w\\right)}{h\\left(w\\right)}h\\left(w\\right)dw \\\\\n",
    "    &=\\int\\log\\left(\\frac{f\\left(w\\right)}{h\\left(w\\right)}\\right)f\\left(w\\right)dw\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{g}   &=E_{h}\\left[\\log\\left(\\frac{g\\left(w\\right)}{h\\left(w\\right)}\\right)\\frac{g\\left(w\\right)}{h\\left(w\\right)}\\right] \\\\\n",
    "    &=\\int\\log\\left(\\frac{g\\left(w\\right)}{h\\left(w\\right)}\\right)\\frac{g\\left(w\\right)}{h\\left(w\\right)}h\\left(w\\right)dw \\\\\n",
    "    &=\\int\\log\\left(\\frac{g\\left(w\\right)}{h\\left(w\\right)}\\right)g\\left(w\\right)dw\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When $ K_g < K_f $, $ g $ is closer to $ h $ than $ f $\n",
    "is.\n",
    "\n",
    "- In that case we’ll find that $ L\\left(w^t\\right) \\rightarrow 0 $.  \n",
    "\n",
    "\n",
    "When $ K_g > K_f $, $ f $ is closer to $ h $ than $ g $\n",
    "is.\n",
    "\n",
    "- In that case we’ll find that\n",
    "  $ L\\left(w^t\\right) \\rightarrow + \\infty $  \n",
    "\n",
    "\n",
    "We’ll now experiment with an $ h $ is also a beta distribution\n",
    "\n",
    "We’ll start by setting parameters $ G_a $ and $ G_b $ so that\n",
    "$ h $ is closer to $ g $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601763e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "H_a, H_b = 3.5, 1.8\n",
    "\n",
    "h = jit(lambda x: p(x, H_a, H_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f187d7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 1, 100)\n",
    "plt.plot(x_range, f(x_range), label='f')\n",
    "plt.plot(x_range, g(x_range), label='g')\n",
    "plt.plot(x_range, h(x_range), label='h')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcb7ab",
   "metadata": {},
   "source": [
    "Let’s compute the Kullback–Leibler discrepancies by quadrature\n",
    "integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401af3fa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def KL_integrand(w, q, h):\n",
    "\n",
    "    m = q(w) / h(w)\n",
    "\n",
    "    return np.log(m) * q(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfe5ba",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL(h, f, g):\n",
    "\n",
    "    Kf, _ = quad(KL_integrand, 0, 1, args=(f, h))\n",
    "    Kg, _ = quad(KL_integrand, 0, 1, args=(g, h))\n",
    "\n",
    "    return Kf, Kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac1de7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Kf, Kg = compute_KL(h, f, g)\n",
    "Kf, Kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb520c2f",
   "metadata": {},
   "source": [
    "We have $ K_g < K_f $.\n",
    "\n",
    "Next, we can verify our conjecture about $ L\\left(w^t\\right) $ by\n",
    "simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383cf6c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_h = simulate(H_a, H_b)\n",
    "l_seq_h = np.cumprod(l_arr_h, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf778336",
   "metadata": {},
   "source": [
    "The figure below plots over time the fraction of paths\n",
    "$ L\\left(w^t\\right) $ that fall in the interval $ [0,0.01] $.\n",
    "\n",
    "Notice that, as expected,  it converges to 1  when $ g $ is closer to\n",
    "$ h $ than $ f $ is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61070e7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_h.shape\n",
    "plt.plot(range(T), np.sum(l_seq_h <= 0.01, axis=0) / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da42e1",
   "metadata": {},
   "source": [
    "We can also try an $ h $ that is closer to $ f $ than is\n",
    "$ g $ so that now $ K_g $ is larger than $ K_f $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e94b7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "H_a, H_b = 1.2, 1.2\n",
    "h = jit(lambda x: p(x, H_a, H_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7670fa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Kf, Kg = compute_KL(h, f, g)\n",
    "Kf, Kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c102a4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_h = simulate(H_a, H_b)\n",
    "l_seq_h = np.cumprod(l_arr_h, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf7592",
   "metadata": {},
   "source": [
    "Now probability mass of $ L\\left(w^t\\right) $ falling above\n",
    "$ 10000 $ diverges to $ +\\infty $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea4879",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_h.shape\n",
    "plt.plot(range(T), np.sum(l_seq_h > 10000, axis=0) / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c106b1",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and Classification\n",
    "\n",
    "We now describe how a  statistician can combine frequentist probabilities of type I and type II errors in order to\n",
    "\n",
    "- compute an anticipated frequency of  selecting a wrong model based on a sample length $ T $  \n",
    "- compute an anticipated error  rate in a classification problem  \n",
    "\n",
    "\n",
    "We consider a situation in which  nature generates data by mixing known densities $ f $ and $ g $ with known mixing\n",
    "parameter $ \\pi_{-1} \\in (0,1) $ so that the random variable $ w $ is drawn from the density\n",
    "\n",
    "$$\n",
    "h (w) = \\pi_{-1} f(w) + (1-\\pi_{-1}) g(w)\n",
    "$$\n",
    "\n",
    "We assume that the statistician knows the densities $ f $ and $ g $ and also the mixing parameter $ \\pi_{-1} $.\n",
    "\n",
    "Below, we’ll  set $ \\pi_{-1} = .5 $, although much of the analysis would follow through with other settings of $ \\pi_{-1} \\in (0,1) $.\n",
    "\n",
    "We assume that $ f $ and $ g $ both put positive probabilities on the same intervals of possible realizations of the random variable $ W $.\n",
    "\n",
    "In the simulations below, we specify that  $ f $ is a $ \\text{Beta}(1, 1) $ distribution and that  $ g $ is $ \\text{Beta}(3, 1.2) $ distribution,\n",
    "just as we did often earlier in this lecture.\n",
    "\n",
    "We consider two alternative timing protocols.\n",
    "\n",
    "- Timing protocol 1 is for   the model selection problem  \n",
    "- Timing protocol 2 is for the individual classification problem  \n",
    "\n",
    "\n",
    "**Protocol 1:**  Nature flips a coin once at time $ t=-1 $ and with probability $ \\pi_{-1} $  generates a sequence  $ \\{w_t\\}_{t=1}^T $\n",
    "of  IID  draws from  $ f $  and with probability $ 1-\\pi_{-1} $ generates a sequence  $ \\{w_t\\}_{t=1}^T $\n",
    "of  IID  draws from  $ g $.\n",
    "\n",
    "Let’s write some Python code that implements timing protocol 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f9912",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def protocol_1(π_minus_1, T, N=1000):\n",
    "    \"\"\"\n",
    "    Simulate Protocol 1: \n",
    "    Nature decides once at t=-1 which model to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    # On-off coin flip for the true model\n",
    "    true_models_F = np.random.rand(N) < π_minus_1\n",
    "    \n",
    "    sequences = np.empty((N, T))\n",
    "    \n",
    "    n_f = np.sum(true_models_F)\n",
    "    n_g = N - n_f\n",
    "    if n_f > 0:\n",
    "        sequences[true_models_F, :] = np.random.beta(F_a, F_b, (n_f, T))\n",
    "    if n_g > 0:\n",
    "        sequences[~true_models_F, :] = np.random.beta(G_a, G_b, (n_g, T))\n",
    "    \n",
    "    return sequences, true_models_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cf35e",
   "metadata": {},
   "source": [
    "**Protocol 2.** At each time $ t \\geq 0 $, nature flips a coin and with probability $ \\pi_{-1} $ draws $ w_t $ from $ f $ and with probability $ 1-\\pi_{-1} $ draws $ w_t $ from $ g $.\n",
    "\n",
    "Here is  Python code that we’ll use to implement timing protocol 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e72f64",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def protocol_2(π_minus_1, T, N=1000):\n",
    "    \"\"\"\n",
    "    Simulate Protocol 2: \n",
    "    Nature decides at each time step which model to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coin flips for each time t upto T\n",
    "    true_models_F = np.random.rand(N, T) < π_minus_1\n",
    "    \n",
    "    sequences = np.empty((N, T))\n",
    "    \n",
    "    n_f = np.sum(true_models_F)\n",
    "    n_g = N * T - n_f\n",
    "    if n_f > 0:\n",
    "        sequences[true_models_F] = np.random.beta(F_a, F_b, n_f)\n",
    "    if n_g > 0:\n",
    "        sequences[~true_models_F] = np.random.beta(G_a, G_b, n_g)\n",
    "    \n",
    "    return sequences, true_models_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38fe4b",
   "metadata": {},
   "source": [
    "**Remark:** Under protocol 2, the $ \\{w_t\\}_{t=1}^T $ is a sequence of IID draws from $ h(w) $. Under protocol 1, the the $ \\{w_t\\}_{t=1}^T $ is\n",
    "not IID.  It is **conditionally IID** – meaning that with probability $ \\pi_{-1} $ it is a sequence of IID draws from $ f(w) $ and with probability $ 1-\\pi_{-1} $ it is a sequence of IID draws from $ g(w) $. For more about this, see [this lecture about exchangeability](https://python.quantecon.org/exchangeable.html).\n",
    "\n",
    "We  again deploy a **likelihood ratio process** with time $ t $ component being the likelihood ratio\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "The **likelihood ratio process** for sequence $ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ is\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "For shorthand we’ll write $ L_t =  L(w^t) $.\n",
    "\n",
    "In the next cell, we write the likelihood ratio calculation that we have done [previously](#nature-likeli) into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f6483",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_likelihood_ratios(sequences):\n",
    "    \"\"\"\n",
    "    Compute likelihood ratios for given sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    l_ratios = f(sequences) / g(sequences) \n",
    "    L_cumulative = np.cumprod(l_ratios, axis=1)\n",
    "    return l_ratios, L_cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822736",
   "metadata": {},
   "source": [
    "## Model Selection Mistake Probability\n",
    "\n",
    "We first study  a problem that assumes  timing protocol 1.\n",
    "\n",
    "Consider a decision maker who wants to know whether model $ f $ or model $ g $ governs a data set of length $ T $ observations.\n",
    "\n",
    "The decision makers has observed a sequence $ \\{w_t\\}_{t=1}^T $.\n",
    "\n",
    "On the basis of that observed  sequence, a likelihood ratio test selects model $ f $ when\n",
    "$ L_T \\geq 1 $ and model $ g $ when  $ L_T < 1 $.\n",
    "\n",
    "When model $ f $ generates the data, the probability that the likelihood ratio test selects the wrong model is\n",
    "\n",
    "$$\n",
    "p_f = {\\rm Prob}\\left(L_T < 1\\Big| f\\right) = \\alpha_T .\n",
    "$$\n",
    "\n",
    "When model $ g $ generates the data, the probability that the likelihood ratio test selects the wrong model is\n",
    "\n",
    "$$\n",
    "p_g = {\\rm Prob}\\left(L_T \\geq 1 \\Big|g \\right) = \\beta_T.\n",
    "$$\n",
    "\n",
    "We can construct a probability that the likelihood ratio selects the wrong model by assigning a Bayesian prior probability of $ \\pi_{-1} = .5 $ that nature selects model $ f $ then  averaging $ p_f $ and $ p_g $ to form the Bayesian posterior probability of a detection error equal to\n",
    "\n",
    "\n",
    "<a id='equation-eq-detectionerrorprob'></a>\n",
    "$$\n",
    "p(\\textrm{wrong decision}) = {1 \\over 2} (\\alpha_T + \\beta_T) . \\tag{21.1}\n",
    "$$\n",
    "\n",
    "Now let’s simulate the protocol 1 and compute the error probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc45cc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "π_minus_1 = 0.5\n",
    "T_max = 30\n",
    "N_simulations = 10_000\n",
    "\n",
    "sequences_p1, true_models_p1 = protocol_1(\n",
    "                            π_minus_1, T_max, N_simulations)\n",
    "l_ratios_p1, L_cumulative_p1 = compute_likelihood_ratios(sequences_p1)\n",
    "\n",
    "# Compute error probabilities for different sample sizes\n",
    "T_range = np.arange(1, T_max + 1)\n",
    "\n",
    "# Boolean masks for true models\n",
    "mask_f = true_models_p1\n",
    "mask_g = ~true_models_p1\n",
    "\n",
    "# Select cumulative likelihoods for each model\n",
    "L_f = L_cumulative_p1[mask_f, :]\n",
    "L_g = L_cumulative_p1[mask_g, :]\n",
    "\n",
    "α_T = np.mean(L_f < 1, axis=0)\n",
    "β_T = np.mean(L_g >= 1, axis=0)\n",
    "\n",
    "error_prob = 0.5 * (α_T + β_T)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(T_range, α_T, 'b-', \n",
    "         label=r'$\\alpha_T$', linewidth=2)\n",
    "ax1.plot(T_range, β_T, 'r-', \n",
    "         label=r'$\\beta_T$', linewidth=2)\n",
    "ax1.set_xlabel('$T$')\n",
    "ax1.set_ylabel('error probability')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(T_range, error_prob, 'g-', \n",
    "         label=r'$\\frac{1}{2}(\\alpha_T+\\beta_T)$', linewidth=2)\n",
    "ax2.set_xlabel('$T$')\n",
    "ax2.set_ylabel('error probability')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At T={T_max}:\")\n",
    "print(f\"α_{T_max} = {α_T[-1]:.4f}\")\n",
    "print(f\"β_{T_max} = {β_T[-1]:.4f}\")\n",
    "print(f\"Model selection error probability = {error_prob[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d1831",
   "metadata": {},
   "source": [
    "Notice how the model selection  error probability approaches zero as $ T $ grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a937e2",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We now consider a problem that assumes timing protocol 2.\n",
    "\n",
    "A decision maker wants to classify components of an observed sequence $ \\{w_t\\}_{t=1}^T $ as having been drawn from either $ f $ or $ g $.\n",
    "\n",
    "The decision maker uses the following classification rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_t  & \\ {\\rm is \\ from \\  f  \\ if \\ } l_t > 1 \\\\\n",
    "w_t  & \\ {\\rm is \\ from \\  g  \\ if \\ } l_t \\leq 1 . \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Under this rule, the expected misclassification rate is\n",
    "\n",
    "\n",
    "<a id='equation-eq-classerrorprob'></a>\n",
    "$$\n",
    "p(\\textrm{misclassification}) = {1 \\over 2} (\\tilde \\alpha_t + \\tilde \\beta_t) \\tag{21.2}\n",
    "$$\n",
    "\n",
    "where $ \\tilde \\alpha_t = {\\rm Prob}(l_t < 1 \\mid f) $ and $ \\tilde \\beta_t = {\\rm Prob}(l_t \\geq 1 \\mid g) $.\n",
    "\n",
    "Now let’s simulate protocol 2 and compute the classification error probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f4f3d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sequences_p2, true_sources_p2 = protocol_2(\n",
    "                    π_minus_1, T_max, N_simulations)\n",
    "l_ratios_p2, _ = compute_likelihood_ratios(sequences_p2)\n",
    "\n",
    "# Find decision boundary where f(w) = g(w)\n",
    "root = brentq(lambda w: f(w) / g(w) - 1, 0.001, 0.999)\n",
    "\n",
    "# Compute theoretical tilde α_t and tilde β_t\n",
    "def α_integrand(w):\n",
    "    \"\"\"Integrand for tilde α_t = P(l_t < 1 | f)\"\"\"\n",
    "    return f(w) if f(w) / g(w) < 1 else 0\n",
    "\n",
    "def β_integrand(w):\n",
    "    \"\"\"Integrand for tilde β_t = P(l_t >= 1 | g)\"\"\"\n",
    "    return g(w) if f(w) / g(w) >= 1 else 0\n",
    "\n",
    "# Compute the integrals\n",
    "α_theory, _ = quad(α_integrand, 0, 1, limit=100)\n",
    "β_theory, _ = quad(β_integrand, 0, 1, limit=100)\n",
    "\n",
    "theory_error = 0.5 * (α_theory + β_theory)\n",
    "\n",
    "print(f\"theoretical tilde α_t = {α_theory:.4f}\")\n",
    "print(f\"theoretical tilde β_t = {β_theory:.4f}\")\n",
    "print(f\"theoretical classification error probability = {theory_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b18b8f",
   "metadata": {},
   "source": [
    "Since for each $ t $, the decision boundary is the same, we can plot the distributions of $ f $ and $ g $ and the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf8a43",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "w_range = np.linspace(0.001, 0.999, 1000)\n",
    "f_values = [f(w) for w in w_range]\n",
    "g_values = [g(w) for w in w_range]\n",
    "ratio_values = [f(w)/g(w) for w in w_range]\n",
    "\n",
    "ax.plot(w_range, f_values, 'b-', \n",
    "        label=r'$f(w) \\sim Beta(1,1)$', linewidth=2)\n",
    "ax.plot(w_range, g_values, 'r-', \n",
    "        label=r'$g(w) \\sim Beta(3,1.2)$', linewidth=2)\n",
    "\n",
    "type1_prob = 1 - beta_dist.cdf(root, F_a, F_b)\n",
    "type2_prob = beta_dist.cdf(root, G_a, G_b)\n",
    "\n",
    "w_type1 = w_range[w_range >= root]\n",
    "f_type1 = [f(w) for w in w_type1]\n",
    "ax.fill_between(w_type1, 0, f_type1, alpha=0.3, color='blue', \n",
    "                label=fr'$\\tilde \\alpha_t = {type1_prob:.2f}$')\n",
    "\n",
    "w_type2 = w_range[w_range <= root]\n",
    "g_type2 = [g(w) for w in w_type2]\n",
    "ax.fill_between(w_type2, 0, g_type2, alpha=0.3, color='red', \n",
    "                label=fr'$\\tilde \\beta_t = {type2_prob:.2f}$')\n",
    "\n",
    "ax.axvline(root, color='green', linestyle='--', alpha=0.7, \n",
    "            label=f'decision boundary: $w=${root:.3f}')\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('probability density')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72800e1b",
   "metadata": {},
   "source": [
    "To  the left of the  green vertical line  $ f < g $,  so $ l_t < 1 $; therefore a  $ w_t $ that falls to the left of the green line is classified as a type $ g $ individual.\n",
    "\n",
    "- The shaded orange area equals $ \\beta $ – the probability of classifying someone as a type $ g $ individual when it is really a type $ f $ individual.  \n",
    "\n",
    "\n",
    "To  the right of the  green vertical line $ g > f $, so $ l_t >1 $; therefore  a  $ w_t $ that falls to the right  of the green line is classified as a type $ f $ individual.\n",
    "\n",
    "- The shaded blue area equals $ \\alpha $ – the probability of classifying someone as a type $ f $ when it is really a type $ g $ individual.  \n",
    "\n",
    "\n",
    "Let’s see the classification algorithm performs in  simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0723ce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "accuracy = np.empty(T_max)\n",
    "\n",
    "for t in range(T_max):\n",
    "    predictions = (l_ratios_p2[:, t] >= 1)\n",
    "    actual = true_sources_p2[:, t]\n",
    "    accuracy[t] = np.mean(predictions == actual)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, T_max + 1), accuracy, \n",
    "                'b-', linewidth=2, label='empirical accuracy')\n",
    "plt.axhline(1 - theory_error, color='r', linestyle='--', \n",
    "                label=f'theoretical accuracy = {1 - theory_error:.4f}')\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2f6b2",
   "metadata": {},
   "source": [
    "Let’s watch decisions made by  the two protocols as more and more observations accrue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6c789",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "ax.plot(T_range, error_prob, linewidth=2, \n",
    "        label='Protocol 1')\n",
    "ax.plot(T_range, 1-accuracy, linestyle='--', linewidth=2, \n",
    "        label=f'Protocol 2')\n",
    "ax.set_ylabel('error probability')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216f75f",
   "metadata": {},
   "source": [
    "From the figure above, we can see:\n",
    "\n",
    "- For both protocols, the error probability starts at the same level, subject to a little randomness.  \n",
    "- For protocol 1, the error probability decreases as the sample size increaes because we are just making **one** decision – i.e., selecting whether $ f $ or $ g $ governs  **all** individuals.  More data provides better evidence.  \n",
    "- For protocol 2, the error probability remains constant because we are making **many** decisions – one classification decision for each observation.  \n",
    "\n",
    "\n",
    "**Remark:** Think about how laws of large numbers are applied to compute error probabilities for the model selection problem and the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b33f",
   "metadata": {},
   "source": [
    "## Sequels\n",
    "\n",
    "Likelihood processes play an important role in Bayesian learning, as described in [Likelihood Ratio Processes and Bayesian Learning](https://python.quantecon.org/likelihood_bayes.html)\n",
    "and as applied in [Job Search VII: Search with Learning](https://python.quantecon.org/odu.html).\n",
    "\n",
    "Likelihood ratio processes appear again in [Additive and Multiplicative Functionals](https://python-advanced.quantecon.org/additive_functionals.html), which contains another illustration\n",
    "of the **peculiar property** of likelihood ratio processes described above."
   ]
  }
 ],
 "metadata": {
  "date": 1752710911.429514,
  "filename": "likelihood_ratio_process.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Likelihood Ratio Processes"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}