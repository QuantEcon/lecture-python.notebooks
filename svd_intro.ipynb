{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f258efba",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "In addition to regular packages contained in Anaconda by default, this lecture also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d06d4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead72bca",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049807f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **singular value decomposition** is a work-horse in applications of least squares projection that\n",
    "form  foundations for important machine learning methods.\n",
    "\n",
    "This lecture describes the singular value decomposition and two of its uses:\n",
    "\n",
    "- principal components analysis (PCA)  \n",
    "- dynamic mode decomposition (DMD)  \n",
    "\n",
    "\n",
    "Each of these can be thought of as a data-reduction procedure  designed to capture salient patterns by projecting data onto a limited set of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750be57",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "Let $ X $ be an $ m \\times n $ matrix of rank $ p $.\n",
    "\n",
    "Necessarily, $ p \\leq \\min(m,n) $.\n",
    "\n",
    "In this lecture, weâ€™ll think of $ X $ as a matrix of **data**.\n",
    "\n",
    "- each column is an **individual** â€“ a time period or person, depending on the application  \n",
    "- each row is a **random variable** describing an attribute of a time period or a person, depending on the application  \n",
    "\n",
    "\n",
    "Weâ€™ll be interested in  two  cases\n",
    "\n",
    "- A **short and fat** case in which $ m << n $, so that there are many more columns (individuals) than rows (attributes).  \n",
    "- A  **tall and skinny** case in which $ m >> n $, so that there are many more rows  (attributes) than columns (individuals).  \n",
    "\n",
    "\n",
    "Weâ€™ll apply a **singular value decomposition** of $ X $ in both situations.\n",
    "\n",
    "In the first case in which there are many more individuals $ n $ than attributes $ m $, we learn sample moments of  a joint distribution  by taking averages  across observations of functions of the observations.\n",
    "\n",
    "In this $ m < < n $ case,  weâ€™ll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the $ m > > n $  case in which there are many more attributes $ m $ than individuals $ n $, weâ€™ll proceed in a different way.\n",
    "\n",
    "Weâ€™ll again use a **singular value decomposition**,  but now to construct a **dynamic mode decomposition** (DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519681a",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $ m \\times n $ matrix $ X $ of rank $ p \\leq \\min(m,n) $ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ U $ is an $ m \\times m $ matrix whose columns are eigenvectors of $ X^T X $  \n",
    "- $ V $ is an $ n \\times n $ matrix whose columns are eigenvectors of $ X X^T $  \n",
    "- $ \\Sigma $ is an $ m \\times n $ matrix in which the first $ p $ places on its main diagonal are positive numbers $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_p $ called **singular values**; remaining entries of $ \\Sigma $ are all zero  \n",
    "- The $ p $ singular values are square roots of the eigenvalues of the $ m \\times m $ matrix  $ X X^T $ and the $ n \\times n $ matrix $ X^T X $  \n",
    "- When $ U $ is a complex valued matrix, $ U^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ U $, meaning that\n",
    "  $ U_{ij}^T $ is the complex conjugate of $ U_{ji} $.  \n",
    "- Similarly, when $ V $ is a complex valued matrix, $ V^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ V $  \n",
    "\n",
    "\n",
    "In what is called a **full** SVD, the  shapes of $ U $, $ \\Sigma $, and $ V $ are $ \\left(m, m\\right) $, $ \\left(m, n\\right) $, $ \\left(n, n\\right) $, respectively.\n",
    "\n",
    "There is also an alternative shape convention called an **economy** or **reduced** SVD .\n",
    "\n",
    "Thus, note that because we assume that $ X $ has rank $ p $, there are only $ p $ nonzero singular values, where $ p=\\textrm{rank}(X)\\leq\\min\\left(m, n\\right) $.\n",
    "\n",
    "A **reduced** SVD uses this fact to express $ U $, $ \\Sigma $, and $ V $ as matrices with shapes $ \\left(m, p\\right) $, $ \\left(p, p\\right) $, $ \\left( n, p\\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f39f42",
   "metadata": {},
   "source": [
    "## Properties of Full and Reduced SVDâ€™s\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    "[https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "For a full SVD,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But these properties donâ€™t hold for a  **reduced** SVD.\n",
    "\n",
    "Which properties hold depend on whether we are in a **tall-skinny** case or a **short-fat** case.\n",
    "\n",
    "- In a **tall-skinny** case in which $ m > > n $, for a **reduced** SVD  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  \\neq I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- In a **short-fat** case in which $ m < < n $, for a **reduced** SVD  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V \\neq I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When we study Dynamic Mode Decomposition below, we shall want to remember this caveat because sometimes weâ€™ll be using reduced SVDâ€™s to compute key objects.\n",
    "\n",
    "Letâ€™s do an  exercise  to compare **full** and **reduced** SVDâ€™s.\n",
    "\n",
    "To review,\n",
    "\n",
    "- in a **full** SVD  \n",
    "  - $ U $ is $ m \\times m $  \n",
    "  - $ \\Sigma $ is $ m \\times n $  \n",
    "  - $ V $ is $ n \\times n $  \n",
    "- in a **reduced** SVD  \n",
    "  - $ U $ is $ m \\times p $  \n",
    "  - $ \\Sigma $ is $ p\\times p $  \n",
    "  - $ V $ is $ n \\times p $  \n",
    "\n",
    "\n",
    "First, letâ€™s study a case in which $ m = 5 > n = 2 $.\n",
    "\n",
    "(This is a small example of the **tall-skinny** case that will concern us when we study **Dynamic Mode Decompositions** below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4351c66",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1da7c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c6db4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "print('rank of X - '), rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd447e1",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "\n",
    "- Where $ U $ is constructed via a full SVD, $ U^T U = I_{p\\times p} $ and  $ U U^T = I_{m \\times m} $  \n",
    "- Where $ \\hat U $ is constructed via a reduced SVD, although $ \\hat U^T \\hat U = I_{p\\times p} $ it happens that  $ \\hat U \\hat U^T \\neq I_{m \\times m} $  \n",
    "\n",
    "\n",
    "We illustrate these properties for our example with the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e5fc2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "UTU = U.T@U\n",
    "UUT = U@U.T\n",
    "print('UUT, UTU = '), UUT, UTU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b952097",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "UhatUhatT = Uhat@Uhat.T\n",
    "UhatTUhat = Uhat.T@Uhat\n",
    "print('UhatUhatT, UhatTUhat= '), UhatUhatT, UhatTUhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce9a3f",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "The cells above illustrate application of the  `fullmatrices=True` and `full-matrices=False` options.\n",
    "Using `full-matrices=False` returns a reduced singular value decomposition.\n",
    "\n",
    "This option implements an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius\n",
    "norm of the discrepancy between the approximating matrix and the matrix being approximated.\n",
    "\n",
    "Optimality in this sense is  established in the celebrated Eckartâ€“Young theorem. See [https://en.wikipedia.org/wiki/Low-rank_approximation](https://en.wikipedia.org/wiki/Low-rank_approximation).\n",
    "\n",
    "When we study Dynamic Mode Decompositions below, it  will be important for us to remember the preceding properties of full and reduced SVDâ€™s in such tall-skinny cases.\n",
    "\n",
    "Now letâ€™s turn to a short-fat case.\n",
    "\n",
    "To illustrate this case,  weâ€™ll set $ m = 2 < 5 = n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63433366",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2,5)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30d901",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e5525",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "print('rank X = '), rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b964cd",
   "metadata": {},
   "source": [
    "## Digression:  Polar Decomposition\n",
    "\n",
    "A singular value decomposition (SVD) of $ X $ is related to a **polar decomposition** of $ X $\n",
    "\n",
    "$$\n",
    "X  = SQ\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " S & = U\\Sigma U^T \\cr\n",
    "Q & = U V^T \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and $ S $ is evidently a symmetric matrix and $ Q $ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8d104",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "Letâ€™s begin with a case in which $ n >> m $, so that we have many  more individuals $ n $ than attributes $ m $.\n",
    "\n",
    "The  matrix $ X $ is **short and fat**  in an  $ n >> m $ case as opposed to a **tall and skinny** case with $ m > > n $ to be discussed later.\n",
    "\n",
    "We regard  $ X $ as an  $ m \\times n $ matrix of **data**:\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ j = 1, \\ldots, n $ the column vector $ X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix} $ is a  vector of observations on variables $ \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix} $.\n",
    "\n",
    "In a **time series** setting, we would think of columns $ j $ as indexing different **times** at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "In a **cross section** setting, we would think of columns $ j $ as indexing different **individuals** for  which random variables are observed, while rows index different **attributes**.\n",
    "\n",
    "The number of positive singular values equals the rank of  matrix $ X $.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $ \\Sigma $ of into a vector $ \\sigma_R $.\n",
    "\n",
    "Set all other entries of $ \\Sigma $ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1197f",
   "metadata": {},
   "source": [
    "## Relationship of PCA to SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $ X $, first construct  the  SVD of the data matrix $ X $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca1'></a>\n",
    "$$\n",
    "X = U \\Sigma V^T = \\sigma_1 U_1 V_1^T + \\sigma_2 U_2 V_2^T + \\cdots + \\sigma_p U_p V_p^T \\tag{7.1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^T = \\begin{bmatrix}V_1^T\\\\V_2^T\\\\\\ldots\\\\V_n^T\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In equation [(7.1)](#equation-eq-pca1), each of the $ m \\times n $ matrices $ U_{j}V_{j}^T $ is evidently\n",
    "of rank $ 1 $.\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca2'></a>\n",
    "$$\n",
    "X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^T\\\\U_{21}V_{1}^T\\\\\\cdots\\\\U_{m1}V_{1}^T\\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^T\\\\U_{22}V_{2}^T\\\\\\cdots\\\\U_{m2}V_{2}^T\\\\\\end{pmatrix}+\\ldots + \\sigma_p\\begin{pmatrix}U_{1p}V_{p}^T\\\\U_{2p}V_{p}^T\\\\\\cdots\\\\U_{mp}V_{p}^T\\\\\\end{pmatrix} \\tag{7.2}\n",
    "$$\n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation [(7.2)](#equation-eq-pca2) in\n",
    "a time series context:\n",
    "\n",
    "- $ \\textrm{for each} \\   k=1, \\ldots, n $, the object $ \\lbrace V_{kj} \\rbrace_{j=1}^n $ is a time series   for the $ k $th **principal component**  \n",
    "- $ U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m $\n",
    "  is a vector of **loadings** of variables $ X_i $ on the $ k $th principal component,  $ i=1, \\ldots, m $  \n",
    "- $ \\sigma_k $ for each $ k=1, \\ldots, p $ is the strength of $ k $th **principal component**, where strength means contribution to the overall covariance of $ X $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835ccb3",
   "metadata": {},
   "source": [
    "## PCA with Eigenvalues and Eigenvectors\n",
    "\n",
    "We now  use an eigen decomposition of a sample covariance matrix to do PCA.\n",
    "\n",
    "Let $ X_{m \\times n} $ be our $ m \\times n $ data matrix.\n",
    "\n",
    "Letâ€™s assume that sample means of all variables are zero.\n",
    "\n",
    "We can assure  this  by **pre-processing** the data by subtracting sample means.\n",
    "\n",
    "Define a sample covariance matrix $ \\Omega $ as\n",
    "\n",
    "$$\n",
    "\\Omega = XX^T\n",
    "$$\n",
    "\n",
    "Then use an eigen decomposition to represent $ \\Omega $ as follows:\n",
    "\n",
    "$$\n",
    "\\Omega =P\\Lambda P^T\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ P $ is $ mÃ—m $ matrix of eigenvectors of $ \\Omega $  \n",
    "- $ \\Lambda $ is a diagonal matrix of eigenvalues of $ \\Omega $  \n",
    "\n",
    "\n",
    "We can then represent $ X $ as\n",
    "\n",
    "$$\n",
    "X=P\\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon = P^{-1} X\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda .\n",
    "$$\n",
    "\n",
    "We can verify that\n",
    "\n",
    "\n",
    "<a id='equation-eq-xxo'></a>\n",
    "$$\n",
    "XX^T=P\\Lambda P^T . \\tag{7.3}\n",
    "$$\n",
    "\n",
    "It follows that we can represent the data matrix $ X $  as\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we had obtained earlier through the SVD, we first note that $ \\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j $.\n",
    "\n",
    "Now define  $ \\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}} $,\n",
    "which  implies that $ \\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^T=1 $.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m} ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which  agrees with\n",
    "\n",
    "$$\n",
    "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}\n",
    "$$\n",
    "\n",
    "provided that  we set\n",
    "\n",
    "- $ U_j=P_j $ (a vector of  loadings of variables on principal component $ j $)  \n",
    "- $ {V_k}^{T}=\\tilde{\\epsilon_k} $ (the $ k $th principal component)  \n",
    "\n",
    "\n",
    "Because  there are alternative algorithms for  computing  $ P $ and $ U $ for  given a data matrix $ X $, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We can resolve such ambiguities about  $ U $ and $ P $ by\n",
    "\n",
    "1. sorting eigenvalues and singular values in descending order  \n",
    "1. imposing positive diagonals on $ P $ and $ U $ and adjusting signs in $ V^T $ accordingly  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49951c4f",
   "metadata": {},
   "source": [
    "## Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider an  SVD of an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "\n",
    "<a id='equation-eq-xxcompare'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "XX^T&=U\\Sigma V^TV\\Sigma^T U^T\\cr\n",
    "&\\equiv U\\Sigma\\Sigma^TU^T\\cr\n",
    "&\\equiv U\\Lambda U^T\n",
    "\\end{aligned} \\tag{7.4}\n",
    "$$\n",
    "\n",
    "Compare representation [(7.4)](#equation-eq-xxcompare) with equation [(7.3)](#equation-eq-xxo) above.\n",
    "\n",
    "Evidently, $ U $ in the SVD is the matrix $ P $  of\n",
    "eigenvectors of $ XX^T $ and $ \\Sigma \\Sigma^T $ is the matrix $ \\Lambda $ of eigenvalues.\n",
    "\n",
    "Second, letâ€™s compute\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^TX &=V\\Sigma^T U^TU\\Sigma V^T\\\\\n",
    "&=V\\Sigma^T{\\Sigma}V^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the matrix $ V $ in the SVD is the matrix of eigenvectors of $ X^TX $\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "X X^T = P \\Lambda P^T\n",
    "$$\n",
    "\n",
    "where $ P $ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $ X $, we know that\n",
    "\n",
    "$$\n",
    "X X^T = U \\Sigma \\Sigma^T U^T\n",
    "$$\n",
    "\n",
    "where $ U $ is an orthonal matrix.\n",
    "\n",
    "Thus, $ P = U $ and we have the representation of $ X $\n",
    "\n",
    "$$\n",
    "X = P \\epsilon = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "U^T X = \\Sigma V^T = \\epsilon\n",
    "$$\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$\n",
    "\\epsilon \\epsilon^T = \\Sigma V^T V \\Sigma^T = \\Sigma \\Sigma^T = \\Lambda ,\n",
    "$$\n",
    "\n",
    "so that everything fits together.\n",
    "\n",
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ba3e6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Î© = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        ðœ†, P = LA.eigh(self.Î©)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(ðœ†.size), key=lambda x: ðœ†[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.ðœ† = ðœ†[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Î› = np.diag(self.ðœ†)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.ðœ†) / self.ðœ†.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.ðœ– = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        ðœ– = self.ðœ–[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ðœ–\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, ðœŽ, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(ðœŽ.size), key=lambda x: ðœŽ[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.ðœŽ = ðœŽ[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Î£ = np.zeros((self.m, self.n))\n",
    "        self.Î£[:d, :d] = np.diag(self.ðœŽ)\n",
    "\n",
    "        ðœŽ_sq = self.ðœŽ ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(ðœŽ_sq) / ðœŽ_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Î£ = self.Î£[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Î£ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        ðœ– = self.ðœ–[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ðœ–\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Î£ = self.Î£[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Î£ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b06dd",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8248a5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'Î» = {da.Î»}\\n')\n",
    "    print(f'Ïƒ^2 = {da.Ïƒ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.Îµ.T)\n",
    "    axs[0].set_title('Îµ')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.Î»))\n",
    "    axs[1].set_title('$V^T*\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e139af4",
   "metadata": {},
   "source": [
    "For an example  PCA applied to analyzing the structure of intelligence tests see this lecture [Multivariable Normal Distribution](https://python.quantecon.org/multivariate_normal.html).\n",
    "\n",
    "Look at  parts of that lecture that describe and illustrate the classic factor analysis model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5d75b",
   "metadata": {},
   "source": [
    "## Dynamic Mode Decomposition (DMD)\n",
    "\n",
    "We turn to the **tall and skinny** case  associated with **Dynamic Mode Decomposition**, the case in  which $ m >>n $.\n",
    "\n",
    "Here an $ m \\times n $  data matrix $ \\tilde X $ contains many more attributes $ m $ than individuals $ n $.\n",
    "\n",
    "This\n",
    "\n",
    "Dynamic mode decomposition was introduced by [[Sch10](https://python.quantecon.org/zreferences.html#id16)],\n",
    "\n",
    "You can read more about Dynamic Mode Decomposition here [[KBBWP16](https://python.quantecon.org/zreferences.html#id24)] and here [[BK19](https://python.quantecon.org/zreferences.html#id25)] (section 7.2).\n",
    "\n",
    "We want to fit a **first-order vector autoregression**\n",
    "\n",
    "\n",
    "<a id='equation-eq-varfirstorder'></a>\n",
    "$$\n",
    "X_{t+1} = A X_t + C \\epsilon_{t+1} \\tag{7.5}\n",
    "$$\n",
    "\n",
    "where $ \\epsilon_{t+1} $ is the time $ t+1 $ instance of an i.i.d. $ m \\times 1 $ random vector with mean vector\n",
    "zero and identity  covariance matrix and\n",
    "\n",
    "where\n",
    "the $ m \\times 1 $ vector $ X_t $ is\n",
    "\n",
    "\n",
    "<a id='equation-eq-xvector'></a>\n",
    "$$\n",
    "X_t = \\begin{bmatrix}  X_{1,t} & X_{2,t} & \\cdots & X_{m,t}     \\end{bmatrix}^T \\tag{7.6}\n",
    "$$\n",
    "\n",
    "and where $ T $ again denotes complex transposition and $ X_{i,t} $ is an observation on variable $ i $ at time $ t $.\n",
    "\n",
    "We want to fit equation [(7.5)](#equation-eq-varfirstorder).\n",
    "\n",
    "Our data are organized in   an $ m \\times (n+1) $ matrix  $ \\tilde X $\n",
    "\n",
    "$$\n",
    "\\tilde X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n \\mid X_{n+1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ t = 1, \\ldots, n +1 $,  the $ m \\times 1 $ vector $ X_t $ is given by [(7.6)](#equation-eq-xvector).\n",
    "\n",
    "Thus, we want to estimate a  system  [(7.5)](#equation-eq-varfirstorder) that consists of $ m $ least squares regressions of **everything** on one lagged value of **everything**.\n",
    "\n",
    "The $ i $â€™th equation of [(7.5)](#equation-eq-varfirstorder) is a regression of $ X_{i,t+1} $ on the vector $ X_t $.\n",
    "\n",
    "We proceed as follows.\n",
    "\n",
    "From $ \\tilde X $,  we  form two $ m \\times n $ matrices\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_{n}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "X' =  \\begin{bmatrix} X_2 \\mid X_3 \\mid \\cdots \\mid X_{n+1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here $ ' $ does not indicate matrix transposition but instead is part of the name of the matrix $ X' $.\n",
    "\n",
    "In forming $ X $ and $ X' $, we have in each case  dropped a column from $ \\tilde X $,  the last column in the case of $ X $, and  the first column in the case of $ X' $.\n",
    "\n",
    "Evidently, $ X $ and $ X' $ are both $ m \\times  n $ matrices.\n",
    "\n",
    "We denote the rank of $ X $ as $ p \\leq \\min(m, n) $.\n",
    "\n",
    "Two possible cases are\n",
    "\n",
    "- $ n > > m $, so that we have many more time series  observations $ n $ than variables $ m $  \n",
    "- $ m > > n $, so that we have many more variables $ m $ than time series observations $ n $  \n",
    "\n",
    "\n",
    "At a general level that includes both of these special cases, a common formula describes the least squares estimator $ \\hat A $ of $ A $ for both cases.\n",
    "\n",
    "But some important  details differ.\n",
    "\n",
    "The common formula is\n",
    "\n",
    "\n",
    "<a id='equation-eq-commona'></a>\n",
    "$$\n",
    "\\hat A = X' X^+ \\tag{7.7}\n",
    "$$\n",
    "\n",
    "where $ X^+ $ is the pseudo-inverse of $ X $.\n",
    "\n",
    "Applicable formulas for the pseudo-inverse differ for our two cases.\n",
    "\n",
    "**Short-Fat Case:**\n",
    "\n",
    "When $ n > > m $, so that we have many more time series  observations $ n $ than variables $ m $ and when\n",
    "$ X $ has linearly independent **rows**, $ X X^T $ has an inverse and the pseudo-inverse $ X^+ $ is\n",
    "\n",
    "$$\n",
    "X^+ = X^T (X X^T)^{-1}\n",
    "$$\n",
    "\n",
    "Here $ X^+ $ is a **right-inverse** that verifies $ X X^+ = I_{m \\times m} $.\n",
    "\n",
    "In this case, our formula [(7.7)](#equation-eq-commona) for the least-squares estimator of the population matrix of regression coefficients  $ A $ becomes\n",
    "\n",
    "\n",
    "<a id='equation-eq-ahatform101'></a>\n",
    "$$\n",
    "\\hat A = X' X^T (X X^T)^{-1} \\tag{7.8}\n",
    "$$\n",
    "\n",
    "This  formula for least-squares regression coefficients widely used in econometrics.\n",
    "\n",
    "For example, it is used  to estimate vector autorgressions.\n",
    "\n",
    "The right side of formula [(7.8)](#equation-eq-ahatform101) is proportional to the empirical cross second moment matrix of $ X_{t+1} $ and $ X_t $ times the inverse\n",
    "of the second moment matrix of $ X_t $.\n",
    "\n",
    "**Tall-Skinny Case:**\n",
    "\n",
    "When $ m > > n $, so that we have many more attributes $ m $ than time series observations $ n $ and when $ X $ has linearly independent **columns**,\n",
    "$ X^T X $ has an inverse and the pseudo-inverse $ X^+ $ is\n",
    "\n",
    "$$\n",
    "X^+ = (X^T X)^{-1} X^T\n",
    "$$\n",
    "\n",
    "Here  $ X^+ $ is a **left-inverse** that verifies $ X^+ X = I_{n \\times n} $.\n",
    "\n",
    "In this case, our formula  [(7.7)](#equation-eq-commona) for a least-squares estimator of $ A $ becomes\n",
    "\n",
    "\n",
    "<a id='equation-eq-hataversion0'></a>\n",
    "$$\n",
    "\\hat A = X' (X^T X)^{-1} X^T \\tag{7.9}\n",
    "$$\n",
    "\n",
    "Please compare formulas [(7.8)](#equation-eq-ahatform101) and [(7.9)](#equation-eq-hataversion0) for $ \\hat A $.\n",
    "\n",
    "Here we are interested in formula [(7.9)](#equation-eq-hataversion0).\n",
    "\n",
    "The $ i $th  row of $ \\hat A $ is an $ m \\times 1 $ vector of regression coefficients of $ X_{i,t+1} $ on $ X_{j,t}, j = 1, \\ldots, m $.\n",
    "\n",
    "If we use formula [(7.9)](#equation-eq-hataversion0) to calculate $ \\hat A X $ we find that\n",
    "\n",
    "$$\n",
    "\\hat A X = X'\n",
    "$$\n",
    "\n",
    "so that the regression equation **fits perfectly**.\n",
    "\n",
    "This is the usual outcome in an **underdetermined least-squares** model.\n",
    "\n",
    "To reiterate, in our **tall-skinny** case  in which we have a number $ n $ of observations   that is small relative to the number $ m $ of\n",
    "attributes that appear in the vector $ X_t $,  we want to fit equation [(7.5)](#equation-eq-varfirstorder).\n",
    "\n",
    "To  offer  ideas about how we can efficiently calculate the pseudo-inverse $ X^+ $, as our  estimator $ \\hat A $ of $ A $ we form an  $ m \\times m $ matrix that  solves the least-squares best-fit problem\n",
    "\n",
    "\n",
    "<a id='equation-eq-alseqn'></a>\n",
    "$$\n",
    "\\hat A = \\textrm{argmin}_{\\check A} || X' - \\check  A X ||_F \\tag{7.10}\n",
    "$$\n",
    "\n",
    "where $ || \\cdot ||_F $ denotes the Frobenius (or Euclidean) norm of a matrix.\n",
    "\n",
    "The Frobenius norm is defined as\n",
    "\n",
    "$$\n",
    "||A||_F = \\sqrt{ \\sum_{i=1}^m \\sum_{j=1}^m |A_{ij}|^2 }\n",
    "$$\n",
    "\n",
    "The minimizer of the right side of equation [(7.10)](#equation-eq-alseqn) is\n",
    "\n",
    "\n",
    "<a id='equation-eq-hataform'></a>\n",
    "$$\n",
    "\\hat A =  X'  X^{+} \\tag{7.11}\n",
    "$$\n",
    "\n",
    "where the (possibly huge) $ n \\times m $ matrix $ X^{+} = (X^T X)^{-1} X^T $ is again a pseudo-inverse of $ X $.\n",
    "\n",
    "For some situations that we are interested in, $ X^T X $ can be close to singular, a situation that can make some numerical algorithms  be error-prone.\n",
    "\n",
    "To acknowledge that possibility, weâ€™ll use  efficient algorithms for computing and for constructing reduced rank approximations of  $ \\hat A $ in formula [(7.9)](#equation-eq-hataversion0).\n",
    "\n",
    "The $ i $th  row of $ \\hat A $ is an $ m \\times 1 $ vector of regression coefficients of $ X_{i,t+1} $ on $ X_{j,t}, j = 1, \\ldots, m $.\n",
    "\n",
    "An efficient way to compute the pseudo-inverse $ X^+ $ is to start with  a singular value decomposition\n",
    "\n",
    "\n",
    "<a id='equation-eq-svddmd'></a>\n",
    "$$\n",
    "X =  U \\Sigma  V^T \\tag{7.12}\n",
    "$$\n",
    "\n",
    "where we remind ourselves that for a **reduced** SVD, $ X $ is an $ m \\times n $ matrix of data, $ U $ is an $ m \\times p $ matrix, $ \\Sigma $  is a $ p \\times p $ matrix, and $ V is an $n \\times p\\$ matrix.\n",
    "\n",
    "We can    efficiently  construct the pertinent pseudo-inverse $ X^+ $\n",
    "by recognizing the following string of equalities.\n",
    "\n",
    "\n",
    "<a id='equation-eq-efficientpseudoinverse'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^{+} & = (X^T X)^{-1} X^T \\\\\n",
    "  & = (V \\Sigma U^T U \\Sigma V^T)^{-1} V \\Sigma U^T \\\\\n",
    "  & = (V \\Sigma \\Sigma V^T)^{-1} V \\Sigma U^T \\\\\n",
    "  & = V \\Sigma^{-1} \\Sigma^{-1} V^T V \\Sigma U^T \\\\\n",
    "  & = V \\Sigma^{-1} U^T \n",
    "\\end{aligned} \\tag{7.13}\n",
    "$$\n",
    "\n",
    "(Since we are in the $ m > > n $ case in which $ V^T V = I_{p \\times p} $ in a reduced SVD, we can use the preceding\n",
    "string of equalities for a reduced SVD as well as for a full SVD.)\n",
    "\n",
    "Thus, we shall  construct a pseudo-inverse $ X^+ $  of $ X $ by using\n",
    "a singular value decomposition of $ X $ in equation [(7.12)](#equation-eq-svddmd)  to compute\n",
    "\n",
    "\n",
    "<a id='equation-eq-xplusformula'></a>\n",
    "$$\n",
    "X^{+} =  V \\Sigma^{-1}  U^T \\tag{7.14}\n",
    "$$\n",
    "\n",
    "where the matrix $ \\Sigma^{-1} $ is constructed by replacing each non-zero element of $ \\Sigma $ with $ \\sigma_j^{-1} $.\n",
    "\n",
    "We can  use formula [(7.14)](#equation-eq-xplusformula)   together with formula [(7.11)](#equation-eq-hataform) to compute the matrix  $ \\hat A $ of regression coefficients.\n",
    "\n",
    "Thus, our  estimator $ \\hat A = X' X^+ $ of the $ m \\times m $ matrix of coefficients $ A $    is\n",
    "\n",
    "\n",
    "<a id='equation-eq-ahatsvdformula'></a>\n",
    "$$\n",
    "\\hat A = X' V \\Sigma^{-1}  U^T \\tag{7.15}\n",
    "$$\n",
    "\n",
    "Weâ€™ll eventually use **dynamic mode decomposition** to compute a rank $ r $ approximation to $ \\hat A $,\n",
    "where $ r <  p $.\n",
    "\n",
    "**Remark:** In our Python code, weâ€™ll sometimes use  a reduced SVD.\n",
    "\n",
    "Next, we describe alternative representations of our first-order linear dynamic system.\n",
    "\n",
    "**Guide to three representations:** In practice, weâ€™ll be interested in Representation 3.  We present the first 2 in order to set the stage for some intermediate steps that might help us understand what is under the hood of Representation 3.  In applications, weâ€™ll use only a small  subset of the DMD to approximate dynamics.  To to that, weâ€™ll want to be using the  reduced  SVDâ€™s affiliated with representation 3, not the full SVDâ€™s affiliated with Representations 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8457d",
   "metadata": {},
   "source": [
    "## Representation 1\n",
    "\n",
    "In this representation, we shall use a **full** SVD of $ X $.\n",
    "\n",
    "We use the $ m $  **columns** of $ U $, and thus the $ m $ **rows** of $ U^T $,  to define   a $ m \\times 1 $  vector $ \\tilde b_t $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildexdef2'></a>\n",
    "$$\n",
    "\\tilde b_t = U^T X_t . \\tag{7.16}\n",
    "$$\n",
    "\n",
    "The original  data $ X_t $ can be represented as\n",
    "\n",
    "\n",
    "<a id='equation-eq-xdecoder'></a>\n",
    "$$\n",
    "X_t = U \\tilde b_t \\tag{7.17}\n",
    "$$\n",
    "\n",
    "(Here we use $ b $ to remind ourselves that we are creating a **basis** vector.)\n",
    "\n",
    "Since we are now using a **full** SVD, $ U U^T = I_{m \\times m} $.\n",
    "\n",
    "So it follows from equation [(7.16)](#equation-eq-tildexdef2) that we can reconstruct  $ X_t $ from $ \\tilde b_t $.\n",
    "\n",
    "In particular,\n",
    "\n",
    "- Equation [(7.16)](#equation-eq-tildexdef2) serves as an **encoder** that  **rotates** the $ m \\times 1 $ vector $ X_t $ to become an $ m \\times 1 $ vector $ \\tilde b_t $  \n",
    "- Equation [(7.17)](#equation-eq-xdecoder) serves as a **decoder** that **reconstructs** the $ m \\times 1 $ vector $ X_t $ by rotating  the $ m \\times 1 $ vector $ \\tilde b_t $  \n",
    "\n",
    "\n",
    "Define a  transition matrix for an $ m \\times 1 $ basis vector  $ \\tilde b_t $ by\n",
    "\n",
    "\n",
    "<a id='equation-eq-atilde0'></a>\n",
    "$$\n",
    "\\tilde A = U^T \\hat A U \\tag{7.18}\n",
    "$$\n",
    "\n",
    "We can  recover $ \\hat A $ from\n",
    "\n",
    "$$\n",
    "\\hat A = U \\tilde A U^T\n",
    "$$\n",
    "\n",
    "Dynamics of the  $ m \\times 1 $ basis vector $ \\tilde b_t $ are governed by\n",
    "\n",
    "$$\n",
    "\\tilde b_{t+1} = \\tilde A \\tilde b_t\n",
    "$$\n",
    "\n",
    "To construct forecasts $ \\overline X_t $ of  future values of $ X_t $ conditional on $ X_1 $, we can apply  decoders\n",
    "(i.e., rotators) to both sides of this\n",
    "equation and deduce\n",
    "\n",
    "$$\n",
    "\\overline X_{t+1} = U \\tilde A^t U^T X_1\n",
    "$$\n",
    "\n",
    "where we use $ \\overline X_{t+1}, t \\geq 1 $ to denote a forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e509607",
   "metadata": {},
   "source": [
    "## Representation 2\n",
    "\n",
    "This representation is related to  one originally proposed by  [[Sch10](https://python.quantecon.org/zreferences.html#id16)].\n",
    "\n",
    "It can be regarded as an intermediate step to  a related   representation 3 to be presented later\n",
    "\n",
    "As with Representation 1, we continue to\n",
    "\n",
    "- use a **full** SVD and **not** a reduced SVD  \n",
    "\n",
    "\n",
    "As we observed and illustrated  earlier in this lecture\n",
    "\n",
    "- (a) for a full SVD $ U U^T = I_{m \\times m} $ and $ U^T U = I_{p \\times p} $ are both identity matrices  \n",
    "- (b)  for  a reduced SVD of $ X $, $ U^T U $ is not an identity matrix.  \n",
    "\n",
    "\n",
    "As we shall see later, a full SVD is  too confining for what we ultimately want to do, namely,  situations in which  $ U^T U $ is **not** an identity matrix because we  use a reduced SVD of $ X $.\n",
    "\n",
    "But for now, letâ€™s proceed under the assumption that we are using a full SVD so that  both of the  preceding two  requirements (a) and (b) are satisfied.\n",
    "\n",
    "Form an eigendecomposition of the $ m \\times m $ matrix $ \\tilde A = U^T \\hat A U $ defined in equation [(7.18)](#equation-eq-atilde0):\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaeigen'></a>\n",
    "$$\n",
    "\\tilde A = W \\Lambda W^{-1} \\tag{7.19}\n",
    "$$\n",
    "\n",
    "where $ \\Lambda $ is a diagonal matrix of eigenvalues and $ W $ is an $ m \\times m $\n",
    "matrix whose columns are eigenvectors  corresponding to rows (eigenvalues) in\n",
    "$ \\Lambda $.\n",
    "\n",
    "When $ U U^T = I_{m \\times m} $, as is true with a full SVD of $ X $, it follows that\n",
    "\n",
    "\n",
    "<a id='equation-eq-eqeigahat'></a>\n",
    "$$\n",
    "\\hat A = U \\tilde A U^T = U W \\Lambda W^{-1} U^T \\tag{7.20}\n",
    "$$\n",
    "\n",
    "According to equation [(7.20)](#equation-eq-eqeigahat), the diagonal matrix $ \\Lambda $ contains eigenvalues of\n",
    "$ \\hat A $ and corresponding eigenvectors of $ \\hat A $ are columns of the matrix $ UW $.\n",
    "\n",
    "It follows that the systematic (i.e., not random) parts of the $ X_t $ dynamics captured by our first-order vector autoregressions   are described by\n",
    "\n",
    "$$\n",
    "X_{t+1} = U W \\Lambda W^{-1} U^T  X_t\n",
    "$$\n",
    "\n",
    "Multiplying both sides of the above equation by $ W^{-1} U^T $ gives\n",
    "\n",
    "$$\n",
    "W^{-1} U^T X_{t+1} = \\Lambda W^{-1} U^T X_t\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\hat b_{t+1} = \\Lambda \\hat b_t\n",
    "$$\n",
    "\n",
    "where our **encoder**  is now\n",
    "\n",
    "$$\n",
    "\\hat b_t = W^{-1} U^T X_t\n",
    "$$\n",
    "\n",
    "and our **decoder** is\n",
    "\n",
    "$$\n",
    "X_t = U W \\hat b_t\n",
    "$$\n",
    "\n",
    "We can use this representation to construct a predictor $ \\overline X_{t+1} $ of $ X_{t+1} $ conditional on $ X_1 $  via:\n",
    "\n",
    "\n",
    "<a id='equation-eq-dssebookrepr'></a>\n",
    "$$\n",
    "\\overline X_{t+1} = U W \\Lambda^t W^{-1} U^T X_1 \\tag{7.21}\n",
    "$$\n",
    "\n",
    "In effect,\n",
    "[[Sch10](https://python.quantecon.org/zreferences.html#id16)] defined an $ m \\times m $ matrix $ \\Phi_s $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-phisfull'></a>\n",
    "$$\n",
    "\\Phi_s = UW \\tag{7.22}\n",
    "$$\n",
    "\n",
    "and a generalized inverse\n",
    "\n",
    "\n",
    "<a id='equation-eq-phisfullinv'></a>\n",
    "$$\n",
    "\\Phi_s^+ = W^{-1}U^T \\tag{7.23}\n",
    "$$\n",
    "\n",
    "[[Sch10](https://python.quantecon.org/zreferences.html#id16)] then  represented equation [(7.21)](#equation-eq-dssebookrepr) as\n",
    "\n",
    "\n",
    "<a id='equation-eq-schmidrep'></a>\n",
    "$$\n",
    "\\overline X_{t+1} = \\Phi_s \\Lambda^t \\Phi_s^+ X_1 \\tag{7.24}\n",
    "$$\n",
    "\n",
    "Components of the  basis vector $ \\hat b_t = W^{-1} U^T X_t \\equiv \\Phi_s^+ X_t $ are often  called DMD **modes**, or sometimes also\n",
    "DMD **projected modes**.\n",
    "\n",
    "To understand why they are called **projected modes**, notice that\n",
    "\n",
    "$$\n",
    "\\Phi_s^+ = ( \\Phi_s^T \\Phi_s)^{-1} \\Phi_s^T\n",
    "$$\n",
    "\n",
    "so that the $ m \\times p $ matrix\n",
    "\n",
    "$$\n",
    "\\hat b =  \\Phi_s^+ X\n",
    "$$\n",
    "\n",
    "is a matrix of regression coefficients of the $ m \\times n $ matrix $ X $ on the $ m \\times p $ matrix $ \\Phi_s $.\n",
    "\n",
    "Weâ€™ll say more about this interpretation in a related context when we discuss representation 3.\n",
    "\n",
    "We turn next  to an alternative  representation suggested by  Tu et al. [[TRL+14](https://python.quantecon.org/zreferences.html#id25)].\n",
    "\n",
    "It is more appropriate to use this alternative representation  when, as in practice is typically the case, we use a reduced SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1d051",
   "metadata": {},
   "source": [
    "## Representation 3\n",
    "\n",
    "Departing from the procedures used to construct  Representations 1 and 2, each of which deployed a **full** SVD, we now use a **reduced** SVD.\n",
    "\n",
    "Again, we let  $ p \\leq \\textrm{min}(m,n) $ be the rank of $ X $.\n",
    "\n",
    "Construct a **reduced** SVD\n",
    "\n",
    "$$\n",
    "X = \\tilde U \\tilde \\Sigma \\tilde V^T,\n",
    "$$\n",
    "\n",
    "where now $ \\tilde U $ is $ m \\times p $, $ \\tilde \\Sigma $ is $ p \\times p $, and $ \\tilde V^T $ is $ p \\times n $.\n",
    "\n",
    "Our minimum-norm least-squares estimator  approximator of  $ A $ now has representation\n",
    "\n",
    "\n",
    "<a id='equation-eq-ahatwithtildes'></a>\n",
    "$$\n",
    "\\hat A = X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T \\tag{7.25}\n",
    "$$\n",
    "\n",
    "Paralleling a step in Representation 1, define a  transition matrix for a rotated $ p \\times 1 $ state $ \\tilde b_t $ by\n",
    "\n",
    "\n",
    "<a id='equation-eq-atildered'></a>\n",
    "$$\n",
    "\\tilde A =\\tilde  U^T \\hat A \\tilde U \\tag{7.26}\n",
    "$$\n",
    "\n",
    "**Interpretation as projection coefficients**\n",
    "\n",
    "[[BK22](https://python.quantecon.org/zreferences.html#id39)] remark that $ \\tilde A $  can be interpreted in terms of a projection of $ \\hat A $ onto the $ p $ modes in $ \\tilde U $.\n",
    "\n",
    "To verify this, first note that, because  $ \\tilde U^T \\tilde U = I $, it follows that\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaverify'></a>\n",
    "$$\n",
    "\\tilde A = \\tilde U^T \\hat A \\tilde U = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T \\tilde U \n",
    "= \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T \\tag{7.27}\n",
    "$$\n",
    "\n",
    "Next, weâ€™ll just  compute the regression coefficients in a projection of $ \\hat A $ on $ \\tilde U $ using the\n",
    "standard least-square formula\n",
    "\n",
    "$$\n",
    "(\\tilde U^T \\tilde U)^{-1} \\tilde U^T \\hat A = (\\tilde U^T \\tilde U)^{-1} \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T = \n",
    "\\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T  = \\tilde A .\n",
    "$$\n",
    "\n",
    "Note that because we are now working with a reduced SVD,  $ \\tilde U \\tilde U^T \\neq I $.\n",
    "\n",
    "Consequently,\n",
    "\n",
    "$$\n",
    "\\hat A \\neq \\tilde U \\tilde A \\tilde U^T,\n",
    "$$\n",
    "\n",
    "and we canâ€™t simply  recover $ \\hat A $ from  $ \\tilde A $ and $ \\tilde U $.\n",
    "\n",
    "Nevertheless, we  hope for the best and proceed to construct an eigendecomposition of the\n",
    "$ p \\times p $ matrix $ \\tilde A $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaeigenred'></a>\n",
    "$$\n",
    "\\tilde A =  \\tilde  W  \\Lambda \\tilde  W^{-1} . \\tag{7.28}\n",
    "$$\n",
    "\n",
    "Mimicking our procedure in Representation 2, we cross our fingers and compute an $ m \\times p $ matrix\n",
    "\n",
    "\n",
    "<a id='equation-eq-phisred'></a>\n",
    "$$\n",
    "\\tilde \\Phi_s = \\tilde U \\tilde W \\tag{7.29}\n",
    "$$\n",
    "\n",
    "that  corresponds to [(7.22)](#equation-eq-phisfull) for a full SVD.\n",
    "\n",
    "At this point, where $ \\hat A $ is given by formula [(7.25)](#equation-eq-ahatwithtildes) it is interesting to compute $ \\hat A \\tilde  \\Phi_s $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat A \\tilde \\Phi_s & = (X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T) (\\tilde U \\tilde W) \\\\\n",
    "  & = X' \\tilde V \\tilde \\Sigma^{-1} \\tilde  W \\\\\n",
    "  & \\neq (\\tilde U \\tilde  W) \\Lambda \\\\\n",
    "  & = \\tilde \\Phi_s \\Lambda\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "That\n",
    "$ \\hat A \\tilde \\Phi_s \\neq \\tilde \\Phi_s \\Lambda $ means, that unlike the  corresponding situation in Representation 2, columns of $ \\tilde \\Phi_s = \\tilde U \\tilde  W $\n",
    "are **not** eigenvectors of $ \\hat A $ corresponding to eigenvalues  on the diagonal of matix $ \\Lambda $.\n",
    "\n",
    "But in a quest for eigenvectors of $ \\hat A $ that we **can** compute with a reduced SVD,  letâ€™s define  the $ m \\times p $ matrix\n",
    "$ \\Phi $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-phiformula'></a>\n",
    "$$\n",
    "\\Phi \\equiv \\hat A \\tilde \\Phi_s = X' \\tilde V \\tilde \\Sigma^{-1}  \\tilde  W \\tag{7.30}\n",
    "$$\n",
    "\n",
    "It turns out that columns of $ \\Phi $ **are** eigenvectors of $ \\hat A $.\n",
    "\n",
    "This is\n",
    "a consequence of a  result established by Tu et al. [[TRL+14](https://python.quantecon.org/zreferences.html#id25)], which we now present.\n",
    "\n",
    "**Proposition** The $ p $ columns of $ \\Phi $ are eigenvectors of $ \\hat A $.\n",
    "\n",
    "**Proof:** From formula [(7.30)](#equation-eq-phiformula) we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\hat A \\Phi & =  (X' \\tilde  V \\tilde  \\Sigma^{-1} \\tilde  U^T) (X' \\tilde  V \\Sigma^{-1} \\tilde  W) \\cr\n",
    "  & = X' \\tilde V \\tilde  \\Sigma^{-1} \\tilde A \\tilde  W \\cr\n",
    "  & = X' \\tilde  V \\tilde  \\Sigma^{-1}\\tilde  W \\Lambda \\cr\n",
    "  & = \\Phi \\Lambda \n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, we  have deduced  that\n",
    "\n",
    "\n",
    "<a id='equation-eq-aphilambda'></a>\n",
    "$$\n",
    "\\hat A \\Phi = \\Phi \\Lambda \\tag{7.31}\n",
    "$$\n",
    "\n",
    "Let $ \\phi_i $ be the $ i $th  column of $ \\Phi $ and $ \\lambda_i $ be the corresponding $ i $ eigenvalue of $ \\tilde A $ from decomposition [(7.28)](#equation-eq-tildeaeigenred).\n",
    "\n",
    "Writing out the $ m \\times 1 $ vectors on both sides of  equation [(7.31)](#equation-eq-aphilambda) and equating them gives\n",
    "\n",
    "$$\n",
    "\\hat A \\phi_i = \\lambda_i \\phi_i .\n",
    "$$\n",
    "\n",
    "This equation confirms that  $ \\phi_i $ is an eigenvector of $ \\hat A $ that corresponds to eigenvalue  $ \\lambda_i $ of both  $ \\tilde A $ and $ \\hat A $.\n",
    "\n",
    "This concludes the proof.\n",
    "\n",
    "Also see [[BK22](https://python.quantecon.org/zreferences.html#id39)] (p. 238)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9699adf",
   "metadata": {},
   "source": [
    "### Decoder of  $ X $ as a linear projection\n",
    "\n",
    "From  eigendecomposition [(7.31)](#equation-eq-aphilambda) we can represent $ \\hat A $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-aform12'></a>\n",
    "$$\n",
    "\\hat A = \\Phi \\Lambda \\Phi^+ . \\tag{7.32}\n",
    "$$\n",
    "\n",
    "From formula [(7.32)](#equation-eq-aform12) we can deduce the reduced dimension dynamics\n",
    "\n",
    "$$\n",
    "\\check b_{t+1} = \\Lambda \\check b_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-eq-decoder102'></a>\n",
    "$$\n",
    "\\check b_t  = \\Phi^+ X_t \\tag{7.33}\n",
    "$$\n",
    "\n",
    "Since the $ m \\times p $ matrix $ \\Phi $ has $ p $ linearly independent columns, the generalized inverse of $ \\Phi $ is\n",
    "\n",
    "$$\n",
    "\\Phi^{+} = (\\Phi^T \\Phi)^{-1} \\Phi^T\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkbform'></a>\n",
    "$$\n",
    "\\check b = (\\Phi^T \\Phi)^{-1} \\Phi^T X \\tag{7.34}\n",
    "$$\n",
    "\n",
    "The $ p \\times n $  matrix $ \\check b $  is recognizable as a  matrix of least squares regression coefficients of the $ m \\times n $  matrix\n",
    "$ X $ on the $ m \\times p $ matrix $ \\Phi $ and consequently\n",
    "\n",
    "\n",
    "<a id='equation-eq-xcheck'></a>\n",
    "$$\n",
    "\\check X = \\Phi \\check b \\tag{7.35}\n",
    "$$\n",
    "\n",
    "is an $ m \\times n $ matrix of least squares projections of $ X $ on $ \\Phi $.\n",
    "\n",
    "By virtue of least-squares projection theory discussed here [https://python-advanced.quantecon.org/orth_proj.html](https://python-advanced.quantecon.org/orth_proj.html),\n",
    "we can represent $ X $ as the sum of the projection $ \\check X $ of $ X $ on $ \\Phi $  plus a matrix of errors.\n",
    "\n",
    "To verify this, note that the least squares projection $ \\check X $ is related to $ X $ by\n",
    "\n",
    "$$\n",
    "X = \\check X + \\epsilon\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-xbcheck'></a>\n",
    "$$\n",
    "X = \\Phi \\check b + \\epsilon \\tag{7.36}\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is an $ m \\times n $ matrix of least squares errors satisfying the least squares\n",
    "orthogonality conditions $ \\epsilon^T \\Phi =0 $ or\n",
    "\n",
    "\n",
    "<a id='equation-eq-orthls'></a>\n",
    "$$\n",
    "(X - \\Phi \\check b)^T \\Phi = 0_{m \\times p} \\tag{7.37}\n",
    "$$\n",
    "\n",
    "Rearranging  the orthogonality conditions [(7.37)](#equation-eq-orthls) gives $ X^T \\Phi = \\check b \\Phi^T \\Phi $,\n",
    "which implies formula [(7.34)](#equation-eq-checkbform)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d2578",
   "metadata": {},
   "source": [
    "### A useful approximation\n",
    "\n",
    "There is a useful  way to approximate  the $ p \\times 1 $ vector $ \\check b_t $ instead of using  formula\n",
    "[(7.33)](#equation-eq-decoder102).\n",
    "\n",
    "In particular, the following argument adapted from [[BK22](https://python.quantecon.org/zreferences.html#id39)] (page 240) provides a computationally efficient way\n",
    "to approximate $ \\check b_t $.\n",
    "\n",
    "For convenience, weâ€™ll do this first for time $ t=1 $.\n",
    "\n",
    "For $ t=1 $, from equation [(7.36)](#equation-eq-xbcheck) we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-x1proj'></a>\n",
    "$$\n",
    "\\check X_1 = \\Phi \\check b_1 \\tag{7.38}\n",
    "$$\n",
    "\n",
    "where $ \\check b_1 $ is a $ p \\times 1 $ vector.\n",
    "\n",
    "Recall from representation 1 above that  $ X_1 =  U \\tilde b_1 $, where $ \\tilde b_1 $ is a time $ 1 $  basis vector for representation 1 and $ U $ is from a full SVD of $ X $.\n",
    "\n",
    "It  then follows from equation [(7.36)](#equation-eq-xbcheck) that\n",
    "\n",
    "$$\n",
    "U \\tilde b_1 = X' \\tilde V \\tilde \\Sigma^{-1} \\tilde  W \\check b_1 + \\epsilon_1\n",
    "$$\n",
    "\n",
    "where $ \\epsilon_1 $ is a least-squares error vector from equation [(7.36)](#equation-eq-xbcheck).\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\tilde b_1 = U^T X' V \\tilde \\Sigma^{-1} \\tilde W \\check b_1 + U^T \\epsilon_1\n",
    "$$\n",
    "\n",
    "Replacing the error term $ U^T \\epsilon_1 $ by zero, and replacing $ U $ from a full SVD of $ X $ with\n",
    "$ \\tilde U $ from a reduced SVD,  we obtain  an approximation $ \\hat b_1 $ to $ \\tilde b_1 $:\n",
    "\n",
    "$$\n",
    "\\hat b_1 = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde  W \\check b_1\n",
    "$$\n",
    "\n",
    "Recall that  from equation [(7.27)](#equation-eq-tildeaverify),  $ \\tilde A = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} $.\n",
    "\n",
    "It then follows  that\n",
    "\n",
    "$$\n",
    "\\hat  b_1 = \\tilde   A \\tilde W \\check b_1\n",
    "$$\n",
    "\n",
    "and therefore, by the  eigendecomposition  [(7.28)](#equation-eq-tildeaeigenred) of $ \\tilde A $, we have\n",
    "\n",
    "$$\n",
    "\\hat b_1 = \\tilde W \\Lambda \\check b_1\n",
    "$$\n",
    "\n",
    "Consequently,\n",
    "\n",
    "$$\n",
    "\\hat b_1 = ( \\tilde W \\Lambda)^{-1} \\tilde b_1\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-beqnsmall'></a>\n",
    "$$\n",
    "\\hat b_1 = ( \\tilde W \\Lambda)^{-1} \\tilde U^T X_1 , \\tag{7.39}\n",
    "$$\n",
    "\n",
    "which is  computationally efficient approximation to  the following instance of  equation [(7.33)](#equation-eq-decoder102) for  the initial vector $ \\check b_1 $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-bphieqn'></a>\n",
    "$$\n",
    "\\check b_1= \\Phi^{+} X_1 \\tag{7.40}\n",
    "$$\n",
    "\n",
    "(To highlight that [(7.39)](#equation-eq-beqnsmall) is an approximation, users of  DMD sometimes call  components of the  basis vector $ \\check b_t  = \\Phi^+ X_t $  the  **exact** DMD modes.)\n",
    "\n",
    "Conditional on $ X_t $, we can compute our decoded $ \\check X_{t+j},   j = 1, 2, \\ldots $  from\n",
    "either\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkxevoln'></a>\n",
    "$$\n",
    "\\check X_{t+j} = \\Phi \\Lambda^j \\Phi^{+} X_t \\tag{7.41}\n",
    "$$\n",
    "\n",
    "or  use the approximation\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkxevoln2'></a>\n",
    "$$\n",
    "\\hat X_{t+j} = \\Phi \\Lambda^j (W \\Lambda)^{-1}  \\tilde U^T X_t . \\tag{7.42}\n",
    "$$\n",
    "\n",
    "We can then use $ \\check X_{t+j} $ or $ \\hat X_{t+j} $ to forecast $ X_{t+j} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0d994",
   "metadata": {},
   "source": [
    "### Using Fewer Modes\n",
    "\n",
    "In applications, weâ€™ll actually want to just a few modes, often three or less.\n",
    "\n",
    "Some of the preceding formulas assume that we have retained all $ p $ modes associated with the positive\n",
    "singular values of $ X $.\n",
    "\n",
    "We can  adjust our  formulas to describe a situation in which we instead retain only\n",
    "the $ r < p $ largest singular values.\n",
    "\n",
    "In that case, we simply replace $ \\tilde \\Sigma $ with the appropriate $ r\\times r $ matrix of singular values,\n",
    "$ \\tilde U $ with the $ m \\times r $ matrix  whose columns correspond to the $ r $ largest singular values,\n",
    "and $ \\tilde V $ with the $ n \\times r $ matrix whose columns correspond to the $ r $ largest  singular values.\n",
    "\n",
    "Counterparts of all of the salient formulas above then apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49077664",
   "metadata": {},
   "source": [
    "## Source for Some Python Code\n",
    "\n",
    "You can find a Python implementation of DMD here:\n",
    "\n",
    "[https://mathlab.github.io/PyDMD/](https://mathlab.github.io/PyDMD/)"
   ]
  }
 ],
 "metadata": {
  "date": 1658714239.6678421,
  "filename": "svd_intro.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Singular Value Decomposition (SVD)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}