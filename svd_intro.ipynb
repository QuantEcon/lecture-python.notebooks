{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f258efba",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "In addition to regular packages contained in Anaconda by default, this lecture also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d06d4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead72bca",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049807f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **singular value decomposition** is a work-horse in applications of least squares projection that\n",
    "form  foundations for important machine learning methods.\n",
    "\n",
    "This lecture describes the singular value decomposition and two of its uses:\n",
    "\n",
    "- principal components analysis (PCA)  \n",
    "- dynamic mode decomposition (DMD)  \n",
    "\n",
    "\n",
    "Each of these can be thought of as a data-reduction procedure  designed to capture salient patterns by projecting data onto a limited set of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750be57",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "Let $ X $ be an $ m \\times n $ matrix of rank $ p $.\n",
    "\n",
    "Necessarily, $ p \\leq \\min(m,n) $.\n",
    "\n",
    "In this lecture, we’ll think of $ X $ as a matrix of **data**.\n",
    "\n",
    "- each column is an **individual** – a time period or person, depending on the application  \n",
    "- each row is a **random variable** describing an attribute of a time period or a person, depending on the application  \n",
    "\n",
    "\n",
    "We’ll be interested in  two  cases\n",
    "\n",
    "- A **short and fat** case in which $ m << n $, so that there are many more columns (individuals) than rows (attributes).  \n",
    "- A  **tall and skinny** case in which $ m >> n $, so that there are many more rows  (attributes) than columns (individuals).  \n",
    "\n",
    "\n",
    "We’ll apply a **singular value decomposition** of $ X $ in both situations.\n",
    "\n",
    "In the first case in which there are many more individuals $ n $ than attributes $ m $, we learn sample moments of  a joint distribution  by taking averages  across observations of functions of the observations.\n",
    "\n",
    "In this $ m < < n $ case,  we’ll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the $ m > > n $  case in which there are many more attributes $ m $ than individuals $ n $, we’ll proceed in a different way.\n",
    "\n",
    "We’ll again use a **singular value decomposition**,  but now to construct a **dynamic mode decomposition** (DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519681a",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $ m \\times n $ matrix $ X $ of rank $ p \\leq \\min(m,n) $ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ U $ is an $ m \\times m $ matrix whose columns are eigenvectors of $ X^T X $  \n",
    "- $ V $ is an $ n \\times n $ matrix whose columns are eigenvectors of $ X X^T $  \n",
    "- $ \\Sigma $ is an $ m \\times n $ matrix in which the first $ p $ places on its main diagonal are positive numbers $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_p $ called **singular values**; remaining entries of $ \\Sigma $ are all zero  \n",
    "- The $ p $ singular values are square roots of the eigenvalues of the $ m \\times m $ matrix  $ X X^T $ and the $ n \\times n $ matrix $ X^T X $  \n",
    "- When $ U $ is a complex valued matrix, $ U^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ U $, meaning that\n",
    "  $ U_{ij}^T $ is the complex conjugate of $ U_{ji} $.  \n",
    "- Similarly, when $ V $ is a complex valued matrix, $ V^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ V $  \n",
    "\n",
    "\n",
    "In what is called a **full** SVD, the  shapes of $ U $, $ \\Sigma $, and $ V $ are $ \\left(m, m\\right) $, $ \\left(m, n\\right) $, $ \\left(n, n\\right) $, respectively.\n",
    "\n",
    "There is also an alternative shape convention called an **economy** or **reduced** SVD .\n",
    "\n",
    "Thus, note that because we assume that $ X $ has rank $ p $, there are only $ p $ nonzero singular values, where $ p=\\textrm{rank}(X)\\leq\\min\\left(m, n\\right) $.\n",
    "\n",
    "A **reduced** SVD uses this fact to express $ U $, $ \\Sigma $, and $ V $ as matrices with shapes $ \\left(m, p\\right) $, $ \\left(p, p\\right) $, $ \\left( n, p\\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f39f42",
   "metadata": {},
   "source": [
    "## Properties of Full and Reduced SVD’s\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    "[https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "For a full SVD,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But these properties don’t hold for a  **reduced** SVD.\n",
    "\n",
    "Which properties hold depend on whether we are in a **tall-skinny** case or a **short-fat** case.\n",
    "\n",
    "- In a **tall-skinny** case in which $ m > > n $, for a **reduced** SVD  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  \\neq I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- In a **short-fat** case in which $ m < < n $, for a **reduced** SVD  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V \\neq I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When we study Dynamic Mode Decomposition below, we shall want to remember this caveat because sometimes we’ll be using reduced SVD’s to compute key objects.\n",
    "\n",
    "Let’s do an  exercise  to compare **full** and **reduced** SVD’s.\n",
    "\n",
    "To review,\n",
    "\n",
    "- in a **full** SVD  \n",
    "  - $ U $ is $ m \\times m $  \n",
    "  - $ \\Sigma $ is $ m \\times n $  \n",
    "  - $ V $ is $ n \\times n $  \n",
    "- in a **reduced** SVD  \n",
    "  - $ U $ is $ m \\times p $  \n",
    "  - $ \\Sigma $ is $ p\\times p $  \n",
    "  - $ V $ is $ n \\times p $  \n",
    "\n",
    "\n",
    "First, let’s study a case in which $ m = 5 > n = 2 $.\n",
    "\n",
    "(This is a small example of the **tall-skinny** case that will concern us when we study **Dynamic Mode Decompositions** below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4351c66",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1da7c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c6db4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "print('rank of X - '), rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd447e1",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "\n",
    "- Where $ U $ is constructed via a full SVD, $ U^T U = I_{p\\times p} $ and  $ U U^T = I_{m \\times m} $  \n",
    "- Where $ \\hat U $ is constructed via a reduced SVD, although $ \\hat U^T \\hat U = I_{p\\times p} $ it happens that  $ \\hat U \\hat U^T \\neq I_{m \\times m} $  \n",
    "\n",
    "\n",
    "We illustrate these properties for our example with the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e5fc2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "UTU = U.T@U\n",
    "UUT = U@U.T\n",
    "print('UUT, UTU = '), UUT, UTU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b952097",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "UhatUhatT = Uhat@Uhat.T\n",
    "UhatTUhat = Uhat.T@Uhat\n",
    "print('UhatUhatT, UhatTUhat= '), UhatUhatT, UhatTUhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce9a3f",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "The cells above illustrate application of the  `fullmatrices=True` and `full-matrices=False` options.\n",
    "Using `full-matrices=False` returns a reduced singular value decomposition.\n",
    "\n",
    "This option implements an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius\n",
    "norm of the discrepancy between the approximating matrix and the matrix being approximated.\n",
    "\n",
    "Optimality in this sense is  established in the celebrated Eckart–Young theorem. See [https://en.wikipedia.org/wiki/Low-rank_approximation](https://en.wikipedia.org/wiki/Low-rank_approximation).\n",
    "\n",
    "When we study Dynamic Mode Decompositions below, it  will be important for us to remember the preceding properties of full and reduced SVD’s in such tall-skinny cases.\n",
    "\n",
    "Now let’s turn to a short-fat case.\n",
    "\n",
    "To illustrate this case,  we’ll set $ m = 2 < 5 = n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63433366",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2,5)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30d901",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e5525",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "print('rank X = '), rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b964cd",
   "metadata": {},
   "source": [
    "## Digression:  Polar Decomposition\n",
    "\n",
    "A singular value decomposition (SVD) of $ X $ is related to a **polar decomposition** of $ X $\n",
    "\n",
    "$$\n",
    "X  = SQ\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " S & = U\\Sigma U^T \\cr\n",
    "Q & = U V^T \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and $ S $ is evidently a symmetric matrix and $ Q $ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8d104",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "Let’s begin with a case in which $ n >> m $, so that we have many  more individuals $ n $ than attributes $ m $.\n",
    "\n",
    "The  matrix $ X $ is **short and fat**  in an  $ n >> m $ case as opposed to a **tall and skinny** case with $ m > > n $ to be discussed later.\n",
    "\n",
    "We regard  $ X $ as an  $ m \\times n $ matrix of **data**:\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ j = 1, \\ldots, n $ the column vector $ X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix} $ is a  vector of observations on variables $ \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix} $.\n",
    "\n",
    "In a **time series** setting, we would think of columns $ j $ as indexing different **times** at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "In a **cross section** setting, we would think of columns $ j $ as indexing different **individuals** for  which random variables are observed, while rows index different **attributes**.\n",
    "\n",
    "The number of positive singular values equals the rank of  matrix $ X $.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $ \\Sigma $ of into a vector $ \\sigma_R $.\n",
    "\n",
    "Set all other entries of $ \\Sigma $ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1197f",
   "metadata": {},
   "source": [
    "## Relationship of PCA to SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $ X $, first construct  the  SVD of the data matrix $ X $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca1'></a>\n",
    "$$\n",
    "X = U \\Sigma V^T = \\sigma_1 U_1 V_1^T + \\sigma_2 U_2 V_2^T + \\cdots + \\sigma_p U_p V_p^T \\tag{7.1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^T = \\begin{bmatrix}V_1^T\\\\V_2^T\\\\\\ldots\\\\V_n^T\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In equation [(7.1)](#equation-eq-pca1), each of the $ m \\times n $ matrices $ U_{j}V_{j}^T $ is evidently\n",
    "of rank $ 1 $.\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca2'></a>\n",
    "$$\n",
    "X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^T\\\\U_{21}V_{1}^T\\\\\\cdots\\\\U_{m1}V_{1}^T\\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^T\\\\U_{22}V_{2}^T\\\\\\cdots\\\\U_{m2}V_{2}^T\\\\\\end{pmatrix}+\\ldots + \\sigma_p\\begin{pmatrix}U_{1p}V_{p}^T\\\\U_{2p}V_{p}^T\\\\\\cdots\\\\U_{mp}V_{p}^T\\\\\\end{pmatrix} \\tag{7.2}\n",
    "$$\n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation [(7.2)](#equation-eq-pca2) in\n",
    "a time series context:\n",
    "\n",
    "- $ \\textrm{for each} \\   k=1, \\ldots, n $, the object $ \\lbrace V_{kj} \\rbrace_{j=1}^n $ is a time series   for the $ k $th **principal component**  \n",
    "- $ U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m $\n",
    "  is a vector of **loadings** of variables $ X_i $ on the $ k $th principal component,  $ i=1, \\ldots, m $  \n",
    "- $ \\sigma_k $ for each $ k=1, \\ldots, p $ is the strength of $ k $th **principal component**, where strength means contribution to the overall covariance of $ X $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835ccb3",
   "metadata": {},
   "source": [
    "## PCA with Eigenvalues and Eigenvectors\n",
    "\n",
    "We now  use an eigen decomposition of a sample covariance matrix to do PCA.\n",
    "\n",
    "Let $ X_{m \\times n} $ be our $ m \\times n $ data matrix.\n",
    "\n",
    "Let’s assume that sample means of all variables are zero.\n",
    "\n",
    "We can assure  this  by **pre-processing** the data by subtracting sample means.\n",
    "\n",
    "Define a sample covariance matrix $ \\Omega $ as\n",
    "\n",
    "$$\n",
    "\\Omega = XX^T\n",
    "$$\n",
    "\n",
    "Then use an eigen decomposition to represent $ \\Omega $ as follows:\n",
    "\n",
    "$$\n",
    "\\Omega =P\\Lambda P^T\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ P $ is $ m×m $ matrix of eigenvectors of $ \\Omega $  \n",
    "- $ \\Lambda $ is a diagonal matrix of eigenvalues of $ \\Omega $  \n",
    "\n",
    "\n",
    "We can then represent $ X $ as\n",
    "\n",
    "$$\n",
    "X=P\\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon = P^{-1} X\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda .\n",
    "$$\n",
    "\n",
    "We can verify that\n",
    "\n",
    "\n",
    "<a id='equation-eq-xxo'></a>\n",
    "$$\n",
    "XX^T=P\\Lambda P^T . \\tag{7.3}\n",
    "$$\n",
    "\n",
    "It follows that we can represent the data matrix $ X $  as\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we had obtained earlier through the SVD, we first note that $ \\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j $.\n",
    "\n",
    "Now define  $ \\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}} $,\n",
    "which  implies that $ \\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^T=1 $.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m} ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which  agrees with\n",
    "\n",
    "$$\n",
    "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}\n",
    "$$\n",
    "\n",
    "provided that  we set\n",
    "\n",
    "- $ U_j=P_j $ (a vector of  loadings of variables on principal component $ j $)  \n",
    "- $ {V_k}^{T}=\\tilde{\\epsilon_k} $ (the $ k $th principal component)  \n",
    "\n",
    "\n",
    "Because  there are alternative algorithms for  computing  $ P $ and $ U $ for  given a data matrix $ X $, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We can resolve such ambiguities about  $ U $ and $ P $ by\n",
    "\n",
    "1. sorting eigenvalues and singular values in descending order  \n",
    "1. imposing positive diagonals on $ P $ and $ U $ and adjusting signs in $ V^T $ accordingly  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49951c4f",
   "metadata": {},
   "source": [
    "## Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider an  SVD of an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "\n",
    "<a id='equation-eq-xxcompare'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "XX^T&=U\\Sigma V^TV\\Sigma^T U^T\\cr\n",
    "&\\equiv U\\Sigma\\Sigma^TU^T\\cr\n",
    "&\\equiv U\\Lambda U^T\n",
    "\\end{aligned} \\tag{7.4}\n",
    "$$\n",
    "\n",
    "Compare representation [(7.4)](#equation-eq-xxcompare) with equation [(7.3)](#equation-eq-xxo) above.\n",
    "\n",
    "Evidently, $ U $ in the SVD is the matrix $ P $  of\n",
    "eigenvectors of $ XX^T $ and $ \\Sigma \\Sigma^T $ is the matrix $ \\Lambda $ of eigenvalues.\n",
    "\n",
    "Second, let’s compute\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^TX &=V\\Sigma^T U^TU\\Sigma V^T\\\\\n",
    "&=V\\Sigma^T{\\Sigma}V^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the matrix $ V $ in the SVD is the matrix of eigenvectors of $ X^TX $\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "X X^T = P \\Lambda P^T\n",
    "$$\n",
    "\n",
    "where $ P $ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $ X $, we know that\n",
    "\n",
    "$$\n",
    "X X^T = U \\Sigma \\Sigma^T U^T\n",
    "$$\n",
    "\n",
    "where $ U $ is an orthonal matrix.\n",
    "\n",
    "Thus, $ P = U $ and we have the representation of $ X $\n",
    "\n",
    "$$\n",
    "X = P \\epsilon = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "U^T X = \\Sigma V^T = \\epsilon\n",
    "$$\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$\n",
    "\\epsilon \\epsilon^T = \\Sigma V^T V \\Sigma^T = \\Sigma \\Sigma^T = \\Lambda ,\n",
    "$$\n",
    "\n",
    "so that everything fits together.\n",
    "\n",
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ba3e6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Ω = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        𝜆, P = LA.eigh(self.Ω)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(𝜆.size), key=lambda x: 𝜆[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.𝜆 = 𝜆[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Λ = np.diag(self.𝜆)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.𝜆) / self.𝜆.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.𝜖 = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        𝜖 = self.𝜖[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ 𝜖\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, 𝜎, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(𝜎.size), key=lambda x: 𝜎[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.𝜎 = 𝜎[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Σ = np.zeros((self.m, self.n))\n",
    "        self.Σ[:d, :d] = np.diag(self.𝜎)\n",
    "\n",
    "        𝜎_sq = self.𝜎 ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(𝜎_sq) / 𝜎_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Σ = self.Σ[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Σ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        𝜖 = self.𝜖[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ 𝜖\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Σ = self.Σ[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Σ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b06dd",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8248a5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'λ = {da.λ}\\n')\n",
    "    print(f'σ^2 = {da.σ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.ε.T)\n",
    "    axs[0].set_title('ε')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.λ))\n",
    "    axs[1].set_title('$V^T*\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e139af4",
   "metadata": {},
   "source": [
    "For an example  PCA applied to analyzing the structure of intelligence tests see this lecture [Multivariable Normal Distribution](https://python.quantecon.org/multivariate_normal.html).\n",
    "\n",
    "Look at  parts of that lecture that describe and illustrate the classic factor analysis model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5d75b",
   "metadata": {},
   "source": [
    "## Dynamic Mode Decomposition (DMD)\n",
    "\n",
    "We turn to the **tall and skinny** case  associated with **Dynamic Mode Decomposition**, the case in  which $ m >>n $.\n",
    "\n",
    "Here an $ m \\times n $  data matrix $ \\tilde X $ contains many more attributes $ m $ than individuals $ n $.\n",
    "\n",
    "This\n",
    "\n",
    "Dynamic mode decomposition was introduced by [[Sch10](https://python.quantecon.org/zreferences.html#id16)],\n",
    "\n",
    "You can read more about Dynamic Mode Decomposition here [[KBBWP16](https://python.quantecon.org/zreferences.html#id24)] and here [[BK19](https://python.quantecon.org/zreferences.html#id25)] (section 7.2).\n",
    "\n",
    "We want to fit a **first-order vector autoregression**\n",
    "\n",
    "\n",
    "<a id='equation-eq-varfirstorder'></a>\n",
    "$$\n",
    "X_{t+1} = A X_t + C \\epsilon_{t+1} \\tag{7.5}\n",
    "$$\n",
    "\n",
    "where $ \\epsilon_{t+1} $ is the time $ t+1 $ instance of an i.i.d. $ m \\times 1 $ random vector with mean vector\n",
    "zero and identity  covariance matrix and\n",
    "\n",
    "where\n",
    "the $ m \\times 1 $ vector $ X_t $ is\n",
    "\n",
    "\n",
    "<a id='equation-eq-xvector'></a>\n",
    "$$\n",
    "X_t = \\begin{bmatrix}  X_{1,t} & X_{2,t} & \\cdots & X_{m,t}     \\end{bmatrix}^T \\tag{7.6}\n",
    "$$\n",
    "\n",
    "and where $ T $ again denotes complex transposition and $ X_{i,t} $ is an observation on variable $ i $ at time $ t $.\n",
    "\n",
    "We want to fit equation [(7.5)](#equation-eq-varfirstorder).\n",
    "\n",
    "Our data are organized in   an $ m \\times (n+1) $ matrix  $ \\tilde X $\n",
    "\n",
    "$$\n",
    "\\tilde X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n \\mid X_{n+1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ t = 1, \\ldots, n +1 $,  the $ m \\times 1 $ vector $ X_t $ is given by [(7.6)](#equation-eq-xvector).\n",
    "\n",
    "Thus, we want to estimate a  system  [(7.5)](#equation-eq-varfirstorder) that consists of $ m $ least squares regressions of **everything** on one lagged value of **everything**.\n",
    "\n",
    "The $ i $’th equation of [(7.5)](#equation-eq-varfirstorder) is a regression of $ X_{i,t+1} $ on the vector $ X_t $.\n",
    "\n",
    "We proceed as follows.\n",
    "\n",
    "From $ \\tilde X $,  we  form two $ m \\times n $ matrices\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_{n}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "X' =  \\begin{bmatrix} X_2 \\mid X_3 \\mid \\cdots \\mid X_{n+1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here $ ' $ does not indicate matrix transposition but instead is part of the name of the matrix $ X' $.\n",
    "\n",
    "In forming $ X $ and $ X' $, we have in each case  dropped a column from $ \\tilde X $,  the last column in the case of $ X $, and  the first column in the case of $ X' $.\n",
    "\n",
    "Evidently, $ X $ and $ X' $ are both $ m \\times  n $ matrices.\n",
    "\n",
    "We denote the rank of $ X $ as $ p \\leq \\min(m, n) $.\n",
    "\n",
    "Two possible cases are\n",
    "\n",
    "- $ n > > m $, so that we have many more time series  observations $ n $ than variables $ m $  \n",
    "- $ m > > n $, so that we have many more variables $ m $ than time series observations $ n $  \n",
    "\n",
    "\n",
    "At a general level that includes both of these special cases, a common formula describes the least squares estimator $ \\hat A $ of $ A $ for both cases.\n",
    "\n",
    "But some important  details differ.\n",
    "\n",
    "The common formula is\n",
    "\n",
    "\n",
    "<a id='equation-eq-commona'></a>\n",
    "$$\n",
    "\\hat A = X' X^+ \\tag{7.7}\n",
    "$$\n",
    "\n",
    "where $ X^+ $ is the pseudo-inverse of $ X $.\n",
    "\n",
    "Applicable formulas for the pseudo-inverse differ for our two cases.\n",
    "\n",
    "**Short-Fat Case:**\n",
    "\n",
    "When $ n > > m $, so that we have many more time series  observations $ n $ than variables $ m $ and when\n",
    "$ X $ has linearly independent **rows**, $ X X^T $ has an inverse and the pseudo-inverse $ X^+ $ is\n",
    "\n",
    "$$\n",
    "X^+ = X^T (X X^T)^{-1}\n",
    "$$\n",
    "\n",
    "Here $ X^+ $ is a **right-inverse** that verifies $ X X^+ = I_{m \\times m} $.\n",
    "\n",
    "In this case, our formula [(7.7)](#equation-eq-commona) for the least-squares estimator of the population matrix of regression coefficients  $ A $ becomes\n",
    "\n",
    "\n",
    "<a id='equation-eq-ahatform101'></a>\n",
    "$$\n",
    "\\hat A = X' X^T (X X^T)^{-1} \\tag{7.8}\n",
    "$$\n",
    "\n",
    "This  formula for least-squares regression coefficients widely used in econometrics.\n",
    "\n",
    "For example, it is used  to estimate vector autorgressions.\n",
    "\n",
    "The right side of formula [(7.8)](#equation-eq-ahatform101) is proportional to the empirical cross second moment matrix of $ X_{t+1} $ and $ X_t $ times the inverse\n",
    "of the second moment matrix of $ X_t $.\n",
    "\n",
    "**Tall-Skinny Case:**\n",
    "\n",
    "When $ m > > n $, so that we have many more attributes $ m $ than time series observations $ n $ and when $ X $ has linearly independent **columns**,\n",
    "$ X^T X $ has an inverse and the pseudo-inverse $ X^+ $ is\n",
    "\n",
    "$$\n",
    "X^+ = (X^T X)^{-1} X^T\n",
    "$$\n",
    "\n",
    "Here  $ X^+ $ is a **left-inverse** that verifies $ X^+ X = I_{n \\times n} $.\n",
    "\n",
    "In this case, our formula  [(7.7)](#equation-eq-commona) for a least-squares estimator of $ A $ becomes\n",
    "\n",
    "\n",
    "<a id='equation-eq-hataversion0'></a>\n",
    "$$\n",
    "\\hat A = X' (X^T X)^{-1} X^T \\tag{7.9}\n",
    "$$\n",
    "\n",
    "Please compare formulas [(7.8)](#equation-eq-ahatform101) and [(7.9)](#equation-eq-hataversion0) for $ \\hat A $.\n",
    "\n",
    "Here we are interested in formula [(7.9)](#equation-eq-hataversion0).\n",
    "\n",
    "The $ i $th  row of $ \\hat A $ is an $ m \\times 1 $ vector of regression coefficients of $ X_{i,t+1} $ on $ X_{j,t}, j = 1, \\ldots, m $.\n",
    "\n",
    "If we use formula [(7.9)](#equation-eq-hataversion0) to calculate $ \\hat A X $ we find that\n",
    "\n",
    "$$\n",
    "\\hat A X = X'\n",
    "$$\n",
    "\n",
    "so that the regression equation **fits perfectly**.\n",
    "\n",
    "This is the usual outcome in an **underdetermined least-squares** model.\n",
    "\n",
    "To reiterate, in our **tall-skinny** case  in which we have a number $ n $ of observations   that is small relative to the number $ m $ of\n",
    "attributes that appear in the vector $ X_t $,  we want to fit equation [(7.5)](#equation-eq-varfirstorder).\n",
    "\n",
    "To  offer  ideas about how we can efficiently calculate the pseudo-inverse $ X^+ $, as our  estimator $ \\hat A $ of $ A $ we form an  $ m \\times m $ matrix that  solves the least-squares best-fit problem\n",
    "\n",
    "\n",
    "<a id='equation-eq-alseqn'></a>\n",
    "$$\n",
    "\\hat A = \\textrm{argmin}_{\\check A} || X' - \\check  A X ||_F \\tag{7.10}\n",
    "$$\n",
    "\n",
    "where $ || \\cdot ||_F $ denotes the Frobenius (or Euclidean) norm of a matrix.\n",
    "\n",
    "The Frobenius norm is defined as\n",
    "\n",
    "$$\n",
    "||A||_F = \\sqrt{ \\sum_{i=1}^m \\sum_{j=1}^m |A_{ij}|^2 }\n",
    "$$\n",
    "\n",
    "The minimizer of the right side of equation [(7.10)](#equation-eq-alseqn) is\n",
    "\n",
    "\n",
    "<a id='equation-eq-hataform'></a>\n",
    "$$\n",
    "\\hat A =  X'  X^{+} \\tag{7.11}\n",
    "$$\n",
    "\n",
    "where the (possibly huge) $ n \\times m $ matrix $ X^{+} = (X^T X)^{-1} X^T $ is again a pseudo-inverse of $ X $.\n",
    "\n",
    "For some situations that we are interested in, $ X^T X $ can be close to singular, a situation that can make some numerical algorithms  be error-prone.\n",
    "\n",
    "To acknowledge that possibility, we’ll use  efficient algorithms for computing and for constructing reduced rank approximations of  $ \\hat A $ in formula [(7.9)](#equation-eq-hataversion0).\n",
    "\n",
    "The $ i $th  row of $ \\hat A $ is an $ m \\times 1 $ vector of regression coefficients of $ X_{i,t+1} $ on $ X_{j,t}, j = 1, \\ldots, m $.\n",
    "\n",
    "An efficient way to compute the pseudo-inverse $ X^+ $ is to start with  a singular value decomposition\n",
    "\n",
    "\n",
    "<a id='equation-eq-svddmd'></a>\n",
    "$$\n",
    "X =  U \\Sigma  V^T \\tag{7.12}\n",
    "$$\n",
    "\n",
    "where we remind ourselves that for a **reduced** SVD, $ X $ is an $ m \\times n $ matrix of data, $ U $ is an $ m \\times p $ matrix, $ \\Sigma $  is a $ p \\times p $ matrix, and $ V is an $n \\times p\\$ matrix.\n",
    "\n",
    "We can    efficiently  construct the pertinent pseudo-inverse $ X^+ $\n",
    "by recognizing the following string of equalities.\n",
    "\n",
    "\n",
    "<a id='equation-eq-efficientpseudoinverse'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^{+} & = (X^T X)^{-1} X^T \\\\\n",
    "  & = (V \\Sigma U^T U \\Sigma V^T)^{-1} V \\Sigma U^T \\\\\n",
    "  & = (V \\Sigma \\Sigma V^T)^{-1} V \\Sigma U^T \\\\\n",
    "  & = V \\Sigma^{-1} \\Sigma^{-1} V^T V \\Sigma U^T \\\\\n",
    "  & = V \\Sigma^{-1} U^T \n",
    "\\end{aligned} \\tag{7.13}\n",
    "$$\n",
    "\n",
    "(Since we are in the $ m > > n $ case in which $ V^T V = I_{p \\times p} $ in a reduced SVD, we can use the preceding\n",
    "string of equalities for a reduced SVD as well as for a full SVD.)\n",
    "\n",
    "Thus, we shall  construct a pseudo-inverse $ X^+ $  of $ X $ by using\n",
    "a singular value decomposition of $ X $ in equation [(7.12)](#equation-eq-svddmd)  to compute\n",
    "\n",
    "\n",
    "<a id='equation-eq-xplusformula'></a>\n",
    "$$\n",
    "X^{+} =  V \\Sigma^{-1}  U^T \\tag{7.14}\n",
    "$$\n",
    "\n",
    "where the matrix $ \\Sigma^{-1} $ is constructed by replacing each non-zero element of $ \\Sigma $ with $ \\sigma_j^{-1} $.\n",
    "\n",
    "We can  use formula [(7.14)](#equation-eq-xplusformula)   together with formula [(7.11)](#equation-eq-hataform) to compute the matrix  $ \\hat A $ of regression coefficients.\n",
    "\n",
    "Thus, our  estimator $ \\hat A = X' X^+ $ of the $ m \\times m $ matrix of coefficients $ A $    is\n",
    "\n",
    "\n",
    "<a id='equation-eq-ahatsvdformula'></a>\n",
    "$$\n",
    "\\hat A = X' V \\Sigma^{-1}  U^T \\tag{7.15}\n",
    "$$\n",
    "\n",
    "We’ll eventually use **dynamic mode decomposition** to compute a rank $ r $ approximation to $ \\hat A $,\n",
    "where $ r <  p $.\n",
    "\n",
    "**Remark:** In our Python code, we’ll sometimes use  a reduced SVD.\n",
    "\n",
    "Next, we describe alternative representations of our first-order linear dynamic system.\n",
    "\n",
    "**Guide to three representations:** In practice, we’ll be interested in Representation 3.  We present the first 2 in order to set the stage for some intermediate steps that might help us understand what is under the hood of Representation 3.  In applications, we’ll use only a small  subset of the DMD to approximate dynamics.  To to that, we’ll want to be using the  reduced  SVD’s affiliated with representation 3, not the full SVD’s affiliated with Representations 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8457d",
   "metadata": {},
   "source": [
    "## Representation 1\n",
    "\n",
    "In this representation, we shall use a **full** SVD of $ X $.\n",
    "\n",
    "We use the $ m $  **columns** of $ U $, and thus the $ m $ **rows** of $ U^T $,  to define   a $ m \\times 1 $  vector $ \\tilde b_t $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildexdef2'></a>\n",
    "$$\n",
    "\\tilde b_t = U^T X_t . \\tag{7.16}\n",
    "$$\n",
    "\n",
    "The original  data $ X_t $ can be represented as\n",
    "\n",
    "\n",
    "<a id='equation-eq-xdecoder'></a>\n",
    "$$\n",
    "X_t = U \\tilde b_t \\tag{7.17}\n",
    "$$\n",
    "\n",
    "(Here we use $ b $ to remind ourselves that we are creating a **basis** vector.)\n",
    "\n",
    "Since we are now using a **full** SVD, $ U U^T = I_{m \\times m} $.\n",
    "\n",
    "So it follows from equation [(7.16)](#equation-eq-tildexdef2) that we can reconstruct  $ X_t $ from $ \\tilde b_t $.\n",
    "\n",
    "In particular,\n",
    "\n",
    "- Equation [(7.16)](#equation-eq-tildexdef2) serves as an **encoder** that  **rotates** the $ m \\times 1 $ vector $ X_t $ to become an $ m \\times 1 $ vector $ \\tilde b_t $  \n",
    "- Equation [(7.17)](#equation-eq-xdecoder) serves as a **decoder** that **reconstructs** the $ m \\times 1 $ vector $ X_t $ by rotating  the $ m \\times 1 $ vector $ \\tilde b_t $  \n",
    "\n",
    "\n",
    "Define a  transition matrix for an $ m \\times 1 $ basis vector  $ \\tilde b_t $ by\n",
    "\n",
    "\n",
    "<a id='equation-eq-atilde0'></a>\n",
    "$$\n",
    "\\tilde A = U^T \\hat A U \\tag{7.18}\n",
    "$$\n",
    "\n",
    "We can  recover $ \\hat A $ from\n",
    "\n",
    "$$\n",
    "\\hat A = U \\tilde A U^T\n",
    "$$\n",
    "\n",
    "Dynamics of the  $ m \\times 1 $ basis vector $ \\tilde b_t $ are governed by\n",
    "\n",
    "$$\n",
    "\\tilde b_{t+1} = \\tilde A \\tilde b_t\n",
    "$$\n",
    "\n",
    "To construct forecasts $ \\overline X_t $ of  future values of $ X_t $ conditional on $ X_1 $, we can apply  decoders\n",
    "(i.e., rotators) to both sides of this\n",
    "equation and deduce\n",
    "\n",
    "$$\n",
    "\\overline X_{t+1} = U \\tilde A^t U^T X_1\n",
    "$$\n",
    "\n",
    "where we use $ \\overline X_{t+1}, t \\geq 1 $ to denote a forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e509607",
   "metadata": {},
   "source": [
    "## Representation 2\n",
    "\n",
    "This representation is related to  one originally proposed by  [[Sch10](https://python.quantecon.org/zreferences.html#id16)].\n",
    "\n",
    "It can be regarded as an intermediate step to  a related   representation 3 to be presented later\n",
    "\n",
    "As with Representation 1, we continue to\n",
    "\n",
    "- use a **full** SVD and **not** a reduced SVD  \n",
    "\n",
    "\n",
    "As we observed and illustrated  earlier in this lecture\n",
    "\n",
    "- (a) for a full SVD $ U U^T = I_{m \\times m} $ and $ U^T U = I_{p \\times p} $ are both identity matrices  \n",
    "- (b)  for  a reduced SVD of $ X $, $ U^T U $ is not an identity matrix.  \n",
    "\n",
    "\n",
    "As we shall see later, a full SVD is  too confining for what we ultimately want to do, namely,  situations in which  $ U^T U $ is **not** an identity matrix because we  use a reduced SVD of $ X $.\n",
    "\n",
    "But for now, let’s proceed under the assumption that we are using a full SVD so that  both of the  preceding two  requirements (a) and (b) are satisfied.\n",
    "\n",
    "Form an eigendecomposition of the $ m \\times m $ matrix $ \\tilde A = U^T \\hat A U $ defined in equation [(7.18)](#equation-eq-atilde0):\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaeigen'></a>\n",
    "$$\n",
    "\\tilde A = W \\Lambda W^{-1} \\tag{7.19}\n",
    "$$\n",
    "\n",
    "where $ \\Lambda $ is a diagonal matrix of eigenvalues and $ W $ is an $ m \\times m $\n",
    "matrix whose columns are eigenvectors  corresponding to rows (eigenvalues) in\n",
    "$ \\Lambda $.\n",
    "\n",
    "When $ U U^T = I_{m \\times m} $, as is true with a full SVD of $ X $, it follows that\n",
    "\n",
    "\n",
    "<a id='equation-eq-eqeigahat'></a>\n",
    "$$\n",
    "\\hat A = U \\tilde A U^T = U W \\Lambda W^{-1} U^T \\tag{7.20}\n",
    "$$\n",
    "\n",
    "According to equation [(7.20)](#equation-eq-eqeigahat), the diagonal matrix $ \\Lambda $ contains eigenvalues of\n",
    "$ \\hat A $ and corresponding eigenvectors of $ \\hat A $ are columns of the matrix $ UW $.\n",
    "\n",
    "It follows that the systematic (i.e., not random) parts of the $ X_t $ dynamics captured by our first-order vector autoregressions   are described by\n",
    "\n",
    "$$\n",
    "X_{t+1} = U W \\Lambda W^{-1} U^T  X_t\n",
    "$$\n",
    "\n",
    "Multiplying both sides of the above equation by $ W^{-1} U^T $ gives\n",
    "\n",
    "$$\n",
    "W^{-1} U^T X_{t+1} = \\Lambda W^{-1} U^T X_t\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\hat b_{t+1} = \\Lambda \\hat b_t\n",
    "$$\n",
    "\n",
    "where our **encoder**  is now\n",
    "\n",
    "$$\n",
    "\\hat b_t = W^{-1} U^T X_t\n",
    "$$\n",
    "\n",
    "and our **decoder** is\n",
    "\n",
    "$$\n",
    "X_t = U W \\hat b_t\n",
    "$$\n",
    "\n",
    "We can use this representation to construct a predictor $ \\overline X_{t+1} $ of $ X_{t+1} $ conditional on $ X_1 $  via:\n",
    "\n",
    "\n",
    "<a id='equation-eq-dssebookrepr'></a>\n",
    "$$\n",
    "\\overline X_{t+1} = U W \\Lambda^t W^{-1} U^T X_1 \\tag{7.21}\n",
    "$$\n",
    "\n",
    "In effect,\n",
    "[[Sch10](https://python.quantecon.org/zreferences.html#id16)] defined an $ m \\times m $ matrix $ \\Phi_s $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-phisfull'></a>\n",
    "$$\n",
    "\\Phi_s = UW \\tag{7.22}\n",
    "$$\n",
    "\n",
    "and a generalized inverse\n",
    "\n",
    "\n",
    "<a id='equation-eq-phisfullinv'></a>\n",
    "$$\n",
    "\\Phi_s^+ = W^{-1}U^T \\tag{7.23}\n",
    "$$\n",
    "\n",
    "[[Sch10](https://python.quantecon.org/zreferences.html#id16)] then  represented equation [(7.21)](#equation-eq-dssebookrepr) as\n",
    "\n",
    "\n",
    "<a id='equation-eq-schmidrep'></a>\n",
    "$$\n",
    "\\overline X_{t+1} = \\Phi_s \\Lambda^t \\Phi_s^+ X_1 \\tag{7.24}\n",
    "$$\n",
    "\n",
    "Components of the  basis vector $ \\hat b_t = W^{-1} U^T X_t \\equiv \\Phi_s^+ X_t $ are often  called DMD **modes**, or sometimes also\n",
    "DMD **projected modes**.\n",
    "\n",
    "To understand why they are called **projected modes**, notice that\n",
    "\n",
    "$$\n",
    "\\Phi_s^+ = ( \\Phi_s^T \\Phi_s)^{-1} \\Phi_s^T\n",
    "$$\n",
    "\n",
    "so that the $ m \\times p $ matrix\n",
    "\n",
    "$$\n",
    "\\hat b =  \\Phi_s^+ X\n",
    "$$\n",
    "\n",
    "is a matrix of regression coefficients of the $ m \\times n $ matrix $ X $ on the $ m \\times p $ matrix $ \\Phi_s $.\n",
    "\n",
    "We’ll say more about this interpretation in a related context when we discuss representation 3.\n",
    "\n",
    "We turn next  to an alternative  representation suggested by  Tu et al. [[TRL+14](https://python.quantecon.org/zreferences.html#id25)].\n",
    "\n",
    "It is more appropriate to use this alternative representation  when, as in practice is typically the case, we use a reduced SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1d051",
   "metadata": {},
   "source": [
    "## Representation 3\n",
    "\n",
    "Departing from the procedures used to construct  Representations 1 and 2, each of which deployed a **full** SVD, we now use a **reduced** SVD.\n",
    "\n",
    "Again, we let  $ p \\leq \\textrm{min}(m,n) $ be the rank of $ X $.\n",
    "\n",
    "Construct a **reduced** SVD\n",
    "\n",
    "$$\n",
    "X = \\tilde U \\tilde \\Sigma \\tilde V^T,\n",
    "$$\n",
    "\n",
    "where now $ \\tilde U $ is $ m \\times p $, $ \\tilde \\Sigma $ is $ p \\times p $, and $ \\tilde V^T $ is $ p \\times n $.\n",
    "\n",
    "Our minimum-norm least-squares estimator  approximator of  $ A $ now has representation\n",
    "\n",
    "\n",
    "<a id='equation-eq-ahatwithtildes'></a>\n",
    "$$\n",
    "\\hat A = X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T \\tag{7.25}\n",
    "$$\n",
    "\n",
    "Paralleling a step in Representation 1, define a  transition matrix for a rotated $ p \\times 1 $ state $ \\tilde b_t $ by\n",
    "\n",
    "\n",
    "<a id='equation-eq-atildered'></a>\n",
    "$$\n",
    "\\tilde A =\\tilde  U^T \\hat A \\tilde U \\tag{7.26}\n",
    "$$\n",
    "\n",
    "**Interpretation as projection coefficients**\n",
    "\n",
    "[[BK22](https://python.quantecon.org/zreferences.html#id39)] remark that $ \\tilde A $  can be interpreted in terms of a projection of $ \\hat A $ onto the $ p $ modes in $ \\tilde U $.\n",
    "\n",
    "To verify this, first note that, because  $ \\tilde U^T \\tilde U = I $, it follows that\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaverify'></a>\n",
    "$$\n",
    "\\tilde A = \\tilde U^T \\hat A \\tilde U = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T \\tilde U \n",
    "= \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T \\tag{7.27}\n",
    "$$\n",
    "\n",
    "Next, we’ll just  compute the regression coefficients in a projection of $ \\hat A $ on $ \\tilde U $ using the\n",
    "standard least-square formula\n",
    "\n",
    "$$\n",
    "(\\tilde U^T \\tilde U)^{-1} \\tilde U^T \\hat A = (\\tilde U^T \\tilde U)^{-1} \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T = \n",
    "\\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T  = \\tilde A .\n",
    "$$\n",
    "\n",
    "Note that because we are now working with a reduced SVD,  $ \\tilde U \\tilde U^T \\neq I $.\n",
    "\n",
    "Consequently,\n",
    "\n",
    "$$\n",
    "\\hat A \\neq \\tilde U \\tilde A \\tilde U^T,\n",
    "$$\n",
    "\n",
    "and we can’t simply  recover $ \\hat A $ from  $ \\tilde A $ and $ \\tilde U $.\n",
    "\n",
    "Nevertheless, we  hope for the best and proceed to construct an eigendecomposition of the\n",
    "$ p \\times p $ matrix $ \\tilde A $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaeigenred'></a>\n",
    "$$\n",
    "\\tilde A =  \\tilde  W  \\Lambda \\tilde  W^{-1} . \\tag{7.28}\n",
    "$$\n",
    "\n",
    "Mimicking our procedure in Representation 2, we cross our fingers and compute an $ m \\times p $ matrix\n",
    "\n",
    "\n",
    "<a id='equation-eq-phisred'></a>\n",
    "$$\n",
    "\\tilde \\Phi_s = \\tilde U \\tilde W \\tag{7.29}\n",
    "$$\n",
    "\n",
    "that  corresponds to [(7.22)](#equation-eq-phisfull) for a full SVD.\n",
    "\n",
    "At this point, where $ \\hat A $ is given by formula [(7.25)](#equation-eq-ahatwithtildes) it is interesting to compute $ \\hat A \\tilde  \\Phi_s $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat A \\tilde \\Phi_s & = (X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T) (\\tilde U \\tilde W) \\\\\n",
    "  & = X' \\tilde V \\tilde \\Sigma^{-1} \\tilde  W \\\\\n",
    "  & \\neq (\\tilde U \\tilde  W) \\Lambda \\\\\n",
    "  & = \\tilde \\Phi_s \\Lambda\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "That\n",
    "$ \\hat A \\tilde \\Phi_s \\neq \\tilde \\Phi_s \\Lambda $ means, that unlike the  corresponding situation in Representation 2, columns of $ \\tilde \\Phi_s = \\tilde U \\tilde  W $\n",
    "are **not** eigenvectors of $ \\hat A $ corresponding to eigenvalues  on the diagonal of matix $ \\Lambda $.\n",
    "\n",
    "But in a quest for eigenvectors of $ \\hat A $ that we **can** compute with a reduced SVD,  let’s define  the $ m \\times p $ matrix\n",
    "$ \\Phi $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-phiformula'></a>\n",
    "$$\n",
    "\\Phi \\equiv \\hat A \\tilde \\Phi_s = X' \\tilde V \\tilde \\Sigma^{-1}  \\tilde  W \\tag{7.30}\n",
    "$$\n",
    "\n",
    "It turns out that columns of $ \\Phi $ **are** eigenvectors of $ \\hat A $.\n",
    "\n",
    "This is\n",
    "a consequence of a  result established by Tu et al. [[TRL+14](https://python.quantecon.org/zreferences.html#id25)], which we now present.\n",
    "\n",
    "**Proposition** The $ p $ columns of $ \\Phi $ are eigenvectors of $ \\hat A $.\n",
    "\n",
    "**Proof:** From formula [(7.30)](#equation-eq-phiformula) we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\hat A \\Phi & =  (X' \\tilde  V \\tilde  \\Sigma^{-1} \\tilde  U^T) (X' \\tilde  V \\Sigma^{-1} \\tilde  W) \\cr\n",
    "  & = X' \\tilde V \\tilde  \\Sigma^{-1} \\tilde A \\tilde  W \\cr\n",
    "  & = X' \\tilde  V \\tilde  \\Sigma^{-1}\\tilde  W \\Lambda \\cr\n",
    "  & = \\Phi \\Lambda \n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, we  have deduced  that\n",
    "\n",
    "\n",
    "<a id='equation-eq-aphilambda'></a>\n",
    "$$\n",
    "\\hat A \\Phi = \\Phi \\Lambda \\tag{7.31}\n",
    "$$\n",
    "\n",
    "Let $ \\phi_i $ be the $ i $th  column of $ \\Phi $ and $ \\lambda_i $ be the corresponding $ i $ eigenvalue of $ \\tilde A $ from decomposition [(7.28)](#equation-eq-tildeaeigenred).\n",
    "\n",
    "Writing out the $ m \\times 1 $ vectors on both sides of  equation [(7.31)](#equation-eq-aphilambda) and equating them gives\n",
    "\n",
    "$$\n",
    "\\hat A \\phi_i = \\lambda_i \\phi_i .\n",
    "$$\n",
    "\n",
    "This equation confirms that  $ \\phi_i $ is an eigenvector of $ \\hat A $ that corresponds to eigenvalue  $ \\lambda_i $ of both  $ \\tilde A $ and $ \\hat A $.\n",
    "\n",
    "This concludes the proof.\n",
    "\n",
    "Also see [[BK22](https://python.quantecon.org/zreferences.html#id39)] (p. 238)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9699adf",
   "metadata": {},
   "source": [
    "### Decoder of  $ X $ as a linear projection\n",
    "\n",
    "From  eigendecomposition [(7.31)](#equation-eq-aphilambda) we can represent $ \\hat A $ as\n",
    "\n",
    "\n",
    "<a id='equation-eq-aform12'></a>\n",
    "$$\n",
    "\\hat A = \\Phi \\Lambda \\Phi^+ . \\tag{7.32}\n",
    "$$\n",
    "\n",
    "From formula [(7.32)](#equation-eq-aform12) we can deduce the reduced dimension dynamics\n",
    "\n",
    "$$\n",
    "\\check b_{t+1} = \\Lambda \\check b_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-eq-decoder102'></a>\n",
    "$$\n",
    "\\check b_t  = \\Phi^+ X_t \\tag{7.33}\n",
    "$$\n",
    "\n",
    "Since the $ m \\times p $ matrix $ \\Phi $ has $ p $ linearly independent columns, the generalized inverse of $ \\Phi $ is\n",
    "\n",
    "$$\n",
    "\\Phi^{+} = (\\Phi^T \\Phi)^{-1} \\Phi^T\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkbform'></a>\n",
    "$$\n",
    "\\check b = (\\Phi^T \\Phi)^{-1} \\Phi^T X \\tag{7.34}\n",
    "$$\n",
    "\n",
    "The $ p \\times n $  matrix $ \\check b $  is recognizable as a  matrix of least squares regression coefficients of the $ m \\times n $  matrix\n",
    "$ X $ on the $ m \\times p $ matrix $ \\Phi $ and consequently\n",
    "\n",
    "\n",
    "<a id='equation-eq-xcheck'></a>\n",
    "$$\n",
    "\\check X = \\Phi \\check b \\tag{7.35}\n",
    "$$\n",
    "\n",
    "is an $ m \\times n $ matrix of least squares projections of $ X $ on $ \\Phi $.\n",
    "\n",
    "By virtue of least-squares projection theory discussed here [https://python-advanced.quantecon.org/orth_proj.html](https://python-advanced.quantecon.org/orth_proj.html),\n",
    "we can represent $ X $ as the sum of the projection $ \\check X $ of $ X $ on $ \\Phi $  plus a matrix of errors.\n",
    "\n",
    "To verify this, note that the least squares projection $ \\check X $ is related to $ X $ by\n",
    "\n",
    "$$\n",
    "X = \\check X + \\epsilon\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-xbcheck'></a>\n",
    "$$\n",
    "X = \\Phi \\check b + \\epsilon \\tag{7.36}\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is an $ m \\times n $ matrix of least squares errors satisfying the least squares\n",
    "orthogonality conditions $ \\epsilon^T \\Phi =0 $ or\n",
    "\n",
    "\n",
    "<a id='equation-eq-orthls'></a>\n",
    "$$\n",
    "(X - \\Phi \\check b)^T \\Phi = 0_{m \\times p} \\tag{7.37}\n",
    "$$\n",
    "\n",
    "Rearranging  the orthogonality conditions [(7.37)](#equation-eq-orthls) gives $ X^T \\Phi = \\check b \\Phi^T \\Phi $,\n",
    "which implies formula [(7.34)](#equation-eq-checkbform)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d2578",
   "metadata": {},
   "source": [
    "### A useful approximation\n",
    "\n",
    "There is a useful  way to approximate  the $ p \\times 1 $ vector $ \\check b_t $ instead of using  formula\n",
    "[(7.33)](#equation-eq-decoder102).\n",
    "\n",
    "In particular, the following argument adapted from [[BK22](https://python.quantecon.org/zreferences.html#id39)] (page 240) provides a computationally efficient way\n",
    "to approximate $ \\check b_t $.\n",
    "\n",
    "For convenience, we’ll do this first for time $ t=1 $.\n",
    "\n",
    "For $ t=1 $, from equation [(7.36)](#equation-eq-xbcheck) we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-x1proj'></a>\n",
    "$$\n",
    "\\check X_1 = \\Phi \\check b_1 \\tag{7.38}\n",
    "$$\n",
    "\n",
    "where $ \\check b_1 $ is a $ p \\times 1 $ vector.\n",
    "\n",
    "Recall from representation 1 above that  $ X_1 =  U \\tilde b_1 $, where $ \\tilde b_1 $ is a time $ 1 $  basis vector for representation 1 and $ U $ is from a full SVD of $ X $.\n",
    "\n",
    "It  then follows from equation [(7.36)](#equation-eq-xbcheck) that\n",
    "\n",
    "$$\n",
    "U \\tilde b_1 = X' \\tilde V \\tilde \\Sigma^{-1} \\tilde  W \\check b_1 + \\epsilon_1\n",
    "$$\n",
    "\n",
    "where $ \\epsilon_1 $ is a least-squares error vector from equation [(7.36)](#equation-eq-xbcheck).\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\tilde b_1 = U^T X' V \\tilde \\Sigma^{-1} \\tilde W \\check b_1 + U^T \\epsilon_1\n",
    "$$\n",
    "\n",
    "Replacing the error term $ U^T \\epsilon_1 $ by zero, and replacing $ U $ from a full SVD of $ X $ with\n",
    "$ \\tilde U $ from a reduced SVD,  we obtain  an approximation $ \\hat b_1 $ to $ \\tilde b_1 $:\n",
    "\n",
    "$$\n",
    "\\hat b_1 = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} \\tilde  W \\check b_1\n",
    "$$\n",
    "\n",
    "Recall that  from equation [(7.27)](#equation-eq-tildeaverify),  $ \\tilde A = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} $.\n",
    "\n",
    "It then follows  that\n",
    "\n",
    "$$\n",
    "\\hat  b_1 = \\tilde   A \\tilde W \\check b_1\n",
    "$$\n",
    "\n",
    "and therefore, by the  eigendecomposition  [(7.28)](#equation-eq-tildeaeigenred) of $ \\tilde A $, we have\n",
    "\n",
    "$$\n",
    "\\hat b_1 = \\tilde W \\Lambda \\check b_1\n",
    "$$\n",
    "\n",
    "Consequently,\n",
    "\n",
    "$$\n",
    "\\hat b_1 = ( \\tilde W \\Lambda)^{-1} \\tilde b_1\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-beqnsmall'></a>\n",
    "$$\n",
    "\\hat b_1 = ( \\tilde W \\Lambda)^{-1} \\tilde U^T X_1 , \\tag{7.39}\n",
    "$$\n",
    "\n",
    "which is  computationally efficient approximation to  the following instance of  equation [(7.33)](#equation-eq-decoder102) for  the initial vector $ \\check b_1 $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-bphieqn'></a>\n",
    "$$\n",
    "\\check b_1= \\Phi^{+} X_1 \\tag{7.40}\n",
    "$$\n",
    "\n",
    "(To highlight that [(7.39)](#equation-eq-beqnsmall) is an approximation, users of  DMD sometimes call  components of the  basis vector $ \\check b_t  = \\Phi^+ X_t $  the  **exact** DMD modes.)\n",
    "\n",
    "Conditional on $ X_t $, we can compute our decoded $ \\check X_{t+j},   j = 1, 2, \\ldots $  from\n",
    "either\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkxevoln'></a>\n",
    "$$\n",
    "\\check X_{t+j} = \\Phi \\Lambda^j \\Phi^{+} X_t \\tag{7.41}\n",
    "$$\n",
    "\n",
    "or  use the approximation\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkxevoln2'></a>\n",
    "$$\n",
    "\\hat X_{t+j} = \\Phi \\Lambda^j (W \\Lambda)^{-1}  \\tilde U^T X_t . \\tag{7.42}\n",
    "$$\n",
    "\n",
    "We can then use $ \\check X_{t+j} $ or $ \\hat X_{t+j} $ to forecast $ X_{t+j} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0d994",
   "metadata": {},
   "source": [
    "### Using Fewer Modes\n",
    "\n",
    "In applications, we’ll actually want to just a few modes, often three or less.\n",
    "\n",
    "Some of the preceding formulas assume that we have retained all $ p $ modes associated with the positive\n",
    "singular values of $ X $.\n",
    "\n",
    "We can  adjust our  formulas to describe a situation in which we instead retain only\n",
    "the $ r < p $ largest singular values.\n",
    "\n",
    "In that case, we simply replace $ \\tilde \\Sigma $ with the appropriate $ r\\times r $ matrix of singular values,\n",
    "$ \\tilde U $ with the $ m \\times r $ matrix  whose columns correspond to the $ r $ largest singular values,\n",
    "and $ \\tilde V $ with the $ n \\times r $ matrix whose columns correspond to the $ r $ largest  singular values.\n",
    "\n",
    "Counterparts of all of the salient formulas above then apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49077664",
   "metadata": {},
   "source": [
    "## Source for Some Python Code\n",
    "\n",
    "You can find a Python implementation of DMD here:\n",
    "\n",
    "[https://mathlab.github.io/PyDMD/](https://mathlab.github.io/PyDMD/)"
   ]
  }
 ],
 "metadata": {
  "date": 1658714239.6678421,
  "filename": "svd_intro.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Singular Value Decomposition (SVD)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}