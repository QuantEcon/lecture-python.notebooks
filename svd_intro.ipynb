{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb32e25a",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "In addition to regular packages contained in Anaconda by default, this lecture also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10ecb3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4a4e0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbd0f9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **singular value decomposition** is a work-horse in applications of least squares projection that\n",
    "form a foundation for  some important machine learning methods.\n",
    "\n",
    "This lecture describes the singular value decomposition and two of its uses:\n",
    "\n",
    "- principal components analysis (PCA)  \n",
    "- dynamic mode decomposition (DMD)  \n",
    "\n",
    "\n",
    "Each of these can be thought of as a data-reduction procedure  designed to capture salient patterns by projecting data onto a limited set of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4e1dd",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "Let $ X $ be an $ m \\times n $ matrix of rank $ r $.\n",
    "\n",
    "Necessarily, $ r \\leq \\min(m,n) $.\n",
    "\n",
    "In this lecture, weâ€™ll think of $ X $ as a matrix of **data**.\n",
    "\n",
    "- each column is an **individual** â€“ a time period or person, depending on the application  \n",
    "- each row is a **random variable** measuring an attribute of a time period or a person, depending on the application  \n",
    "\n",
    "\n",
    "Weâ€™ll be interested in  two  cases\n",
    "\n",
    "- A **short and fat** case in which $ m << n $, so that there are many more columns than rows.  \n",
    "- A  **tall and skinny** case in which $ m >> n $, so that there are many more rows than columns.  \n",
    "\n",
    "\n",
    "Weâ€™ll apply a **singular value decomposition** of $ X $ in both situations.\n",
    "\n",
    "In the first case in which there are many more observations $ n $ than random variables $ m $, we learn about a joint distribution  by taking averages  across observations of functions of the observations.\n",
    "\n",
    "Here weâ€™ll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the second case in which there are many more random variables $ m $ than observations $ n $, weâ€™ll proceed in a different way.\n",
    "\n",
    "Weâ€™ll again use a **singular value decomposition**,  but now to do a **dynamic mode decomposition** (DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facfae58",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $ m \\times n $ matrix $ X $ of rank $ r \\leq \\min(m,n) $ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ U $ is an $ m \\times m $ matrix whose columns are eigenvectors of $ X^T X $  \n",
    "- $ V $ is an $ n \\times n $ matrix whose columns are eigenvectors of $ X X^T $  \n",
    "- $ \\Sigma $ is an $ m \\times n $ matrix in which the first $ r $ places on its main diagonal are positive numbers $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_r $ called **singular values**; remaining entries of $ \\Sigma $ are all zero  \n",
    "- The $ r $ singular values are square roots of the eigenvalues of the $ m \\times m $ matrix  $ X X^T $ and the $ n \\times n $ matrix $ X^T X $  \n",
    "- When $ U $ is a complex valued matrix, $ U^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ U $, meaning that\n",
    "  $ U_{ij}^T $ is the complex conjugate of $ U_{ji} $.  \n",
    "- Similarly, when $ V $ is a complex valued matrix, $ V^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ V $  \n",
    "\n",
    "\n",
    "In what is called a **full** SVD, the  shapes of $ U $, $ \\Sigma $, and $ V $ are $ \\left(m, m\\right) $, $ \\left(m, n\\right) $, $ \\left(n, n\\right) $, respectively.\n",
    "\n",
    "There is also an alternative shape convention called an **economy** or **reduced** SVD .\n",
    "\n",
    "Thus, note that because we assume that $ A $ has rank $ r $, there are only $ r $ nonzero singular values, where $ r=\\textrm{rank}(A)\\leq\\min\\left(m, n\\right) $.\n",
    "\n",
    "A **reduced** SVD uses this fact to express $ U $, $ \\Sigma $, and $ V $ as matrices with shapes $ \\left(m, r\\right) $, $ \\left(r, r\\right) $, $ \\left(r, n\\right) $.\n",
    "\n",
    "Sometimes, we will use a full SVD\n",
    "\n",
    "At other times, weâ€™ll use a reduced SVD  in which $ \\Sigma $ is an $ r \\times r $  diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47784f14",
   "metadata": {},
   "source": [
    "## Digression:  Polar Decomposition\n",
    "\n",
    "A singular value decomposition (SVD) is related to the **polar decomposition** of $ X $\n",
    "\n",
    "$$\n",
    "X  = SQ\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    " S & = U\\Sigma U^T \\cr\n",
    "Q & = U V^T \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and $ S $ is evidently a symmetric matrix and $ Q $ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a43a2",
   "metadata": {},
   "source": [
    "## Principle Components Analysis (PCA)\n",
    "\n",
    "Letâ€™s begin with a case in which $ n >> m $, so that we have many  more observations $ n $ than random variables $ m $.\n",
    "\n",
    "The  matrix $ X $ is **short and fat**  in an  $ n >> m $ case as opposed to a **tall and skinny** case with $ m > > n $ to be discussed later.\n",
    "\n",
    "We regard  $ X $ as an  $ m \\times n $ matrix of **data**:\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ j = 1, \\ldots, n $ the column vector $ X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix} $ is a  vector of observations on variables $ \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix} $.\n",
    "\n",
    "In a **time series** setting, we would think of columns $ j $ as indexing different **times** at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "In a **cross section** setting, we would think of columns $ j $ as indexing different **individuals** for  which random variables are observed, while rows index different **random variables**.\n",
    "\n",
    "The number of positive singular values equals the rank of  matrix $ X $.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $ \\Sigma $ of into a vector $ \\sigma_R $.\n",
    "\n",
    "Set all other entries of $ \\Sigma $ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63f04f",
   "metadata": {},
   "source": [
    "## Relationship of PCA to SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $ X $, first construct  the  SVD of the data matrix $ X $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca1'></a>\n",
    "$$\n",
    "X = U \\Sigma V^T = \\sigma_1 U_1 V_1^T + \\sigma_2 U_2 V_2^T + \\cdots + \\sigma_r U_r V_r^T \\tag{6.1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^T = \\begin{bmatrix}V_1^T\\\\V_2^T\\\\\\ldots\\\\V_n^T\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In equation [(6.1)](#equation-eq-pca1), each of the $ m \\times n $ matrices $ U_{j}V_{j}^T $ is evidently\n",
    "of rank $ 1 $.\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca2'></a>\n",
    "$$\n",
    "X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^T\\\\U_{21}V_{1}^T\\\\\\cdots\\\\U_{m1}V_{1}^T\\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^T\\\\U_{22}V_{2}^T\\\\\\cdots\\\\U_{m2}V_{2}^T\\\\\\end{pmatrix}+\\ldots + \\sigma_r\\begin{pmatrix}U_{1r}V_{r}^T\\\\U_{2r}V_{r}^T\\\\\\cdots\\\\U_{mr}V_{r}^T\\\\\\end{pmatrix} \\tag{6.2}\n",
    "$$\n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation [(6.2)](#equation-eq-pca2) in\n",
    "a time series context:\n",
    "\n",
    "- $ V_{k}^T= \\begin{bmatrix}V_{k1} &  V_{k2} & \\ldots & V_{kn}\\end{bmatrix}  \\quad \\textrm{for each} \\   k=1, \\ldots, n $ is a time series  $ \\lbrace V_{kj} \\rbrace_{j=1}^n $ for the $ k $th **principal component**  \n",
    "- $ U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m $\n",
    "  is a vector of **loadings** of variables $ X_i $ on the $ k $th principle component,  $ i=1, \\ldots, m $  \n",
    "- $ \\sigma_k $ for each $ k=1, \\ldots, r $ is the strength of $ k $th **principal component**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20181569",
   "metadata": {},
   "source": [
    "## Reduced Versus Full SVD\n",
    "\n",
    "Earlier, we mentioned **full** and **reduced** SVDâ€™s.\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    "[https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "In a **full** SVD\n",
    "\n",
    "- $ U $ is $ m \\times m $  \n",
    "- $ \\Sigma $ is $ m \\times n $  \n",
    "- $ V $ is $ n \\times n $  \n",
    "\n",
    "\n",
    "In a **reduced** SVD\n",
    "\n",
    "- $ U $ is $ m \\times r $  \n",
    "- $ \\Sigma $ is $ r \\times r $  \n",
    "- $ V $ is $ n \\times r $  \n",
    "\n",
    "\n",
    "Letâ€™s do a some  small exerecise  to compare **full** and **reduced** SVDâ€™s.\n",
    "\n",
    "First, letâ€™s study a case in which $ m = 5 > n = 2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099493b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a129a7a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7d10c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285f19f",
   "metadata": {},
   "source": [
    "**Remark:** The cells above illustrate application of the  `fullmatrices=True` and `full-matrices=False` options.\n",
    "Using `full-matrices=False` returns a reduced singular value decomposition. This option implements\n",
    "an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius\n",
    "norm of the discrepancy between the approximating matrix and the matrix being approximated.\n",
    "Optimality in this sense is  established in the celebrated Eckartâ€“Young theorem. See [https://en.wikipedia.org/wiki/Low-rank_approximation](https://en.wikipedia.org/wiki/Low-rank_approximation).\n",
    "\n",
    "Letâ€™s do another exercise, but now weâ€™ll set $ m = 2 < 5 = n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613d513",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2,5)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f37c0e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136a06a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a231157",
   "metadata": {},
   "source": [
    "## PCA with Eigenvalues and Eigenvectors\n",
    "\n",
    "We now  use an eigen decomposition of a sample covariance matrix to do PCA.\n",
    "\n",
    "Let $ X_{m \\times n} $ be our $ m \\times n $ data matrix.\n",
    "\n",
    "Letâ€™s assume that sample means of all variables are zero.\n",
    "\n",
    "We can assure  this  by **pre-processing** the data by subtracting sample means.\n",
    "\n",
    "Define the sample covariance matrix $ \\Omega $ as\n",
    "\n",
    "$$\n",
    "\\Omega = XX^T\n",
    "$$\n",
    "\n",
    "Then use an eigen decomposition to represent $ \\Omega $ as follows:\n",
    "\n",
    "$$\n",
    "\\Omega =P\\Lambda P^T\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ P $ is $ mÃ—m $ matrix of eigenvectors of $ \\Omega $  \n",
    "- $ \\Lambda $ is a diagonal matrix of eigenvalues of $ \\Omega $  \n",
    "\n",
    "\n",
    "We can then represent $ X $ as\n",
    "\n",
    "$$\n",
    "X=P\\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda .\n",
    "$$\n",
    "\n",
    "We can verify that\n",
    "\n",
    "$$\n",
    "XX^T=P\\Lambda P^T .\n",
    "$$\n",
    "\n",
    "It follows that we can represent the data matrix as\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda .\n",
    "$$\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we obtained through the SVD above, we first note that $ \\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j $.\n",
    "\n",
    "Now define  $ \\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}} $,\n",
    "which evidently implies that $ \\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^T=1 $.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m} ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which evidently agrees with\n",
    "\n",
    "$$\n",
    "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}\n",
    "$$\n",
    "\n",
    "provided that  we set\n",
    "\n",
    "- $ U_j=P_j $ (the loadings of variables on principal components)  \n",
    "- $ {V_k}^{T}=\\tilde{\\epsilon_k} $ (the principal components)  \n",
    "\n",
    "\n",
    "Since there are several possible ways of computing  $ P $ and $ U $ for  given a data matrix $ X $, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We can resolve such ambiguities about  $ U $ and $ P $ by\n",
    "\n",
    "1. sorting eigenvalues and singular values in descending order  \n",
    "1. imposing positive diagonals on $ P $ and $ U $ and adjusting signs in $ V^T $ accordingly  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b98b59",
   "metadata": {},
   "source": [
    "## Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider the following SVD of an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "XX^T&=U\\Sigma V^TV\\Sigma^T U^T\\cr\n",
    "&\\equiv U\\Sigma\\Sigma^TU^T\\cr\n",
    "&\\equiv U\\Lambda U^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, $ U $ in the SVD is the matrix $ P $  of\n",
    "eigenvectors of $ XX^T $ and $ \\Sigma \\Sigma^T $ is the matrix $ \\Lambda $ of eigenvalues.\n",
    "\n",
    "Second, letâ€™s compute\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^TX &=V\\Sigma^T U^TU\\Sigma V^T\\\\\n",
    "&=V\\Sigma^T{\\Sigma}V^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the matrix $ V $ in the SVD is the matrix of eigenvectors of $ X^TX $\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "X X^T = P \\Lambda P^T\n",
    "$$\n",
    "\n",
    "where $ P $ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $ X $, we know that\n",
    "\n",
    "$$\n",
    "X X^T = U \\Sigma \\Sigma^T U^T\n",
    "$$\n",
    "\n",
    "where $ U $ is an orthonal matrix.\n",
    "\n",
    "Thus, $ P = U $ and we have the representation of $ X $\n",
    "\n",
    "$$\n",
    "X = P \\epsilon = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "U^T X = \\Sigma V^T = \\epsilon\n",
    "$$\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$\n",
    "\\epsilon \\epsilon^T = \\Sigma V^T V \\Sigma^T = \\Sigma \\Sigma^T = \\Lambda ,\n",
    "$$\n",
    "\n",
    "so that everything fits together.\n",
    "\n",
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42535851",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Î© = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        ðœ†, P = LA.eigh(self.Î©)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(ðœ†.size), key=lambda x: ðœ†[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.ðœ† = ðœ†[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Î› = np.diag(self.ðœ†)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.ðœ†) / self.ðœ†.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.ðœ– = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        ðœ– = self.ðœ–[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ðœ–\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, ðœŽ, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(ðœŽ.size), key=lambda x: ðœŽ[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.ðœŽ = ðœŽ[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Î£ = np.zeros((self.m, self.n))\n",
    "        self.Î£[:d, :d] = np.diag(self.ðœŽ)\n",
    "\n",
    "        ðœŽ_sq = self.ðœŽ ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(ðœŽ_sq) / ðœŽ_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Î£ = self.Î£[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Î£ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        ðœ– = self.ðœ–[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ðœ–\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Î£ = self.Î£[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Î£ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ddac4d",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424aa5c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'Î» = {da.Î»}\\n')\n",
    "    print(f'Ïƒ^2 = {da.Ïƒ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.Îµ.T)\n",
    "    axs[0].set_title('Îµ')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.Î»))\n",
    "    axs[1].set_title('$V^T*\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb02ff",
   "metadata": {},
   "source": [
    "## Dynamic Mode Decomposition (DMD)\n",
    "\n",
    "We turn to the case in which $ m >>n $ in which an $ m \\times n $  data matrix $ \\tilde X $ contains many more random variables $ m $ than observations $ n $.\n",
    "\n",
    "This  **tall and skinny** case is associated with **Dynamic Mode Decomposition**.\n",
    "\n",
    "You can read about Dynamic Mode Decomposition here [[KBBWP16](https://python.quantecon.org/zreferences.html#id24)].\n",
    "\n",
    "We start  with an $ m \\times n $ matrix of data $ \\tilde X $ of the form\n",
    "\n",
    "$$\n",
    "\\tilde X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ t = 1, \\ldots, n $,  the $ m \\times 1 $ vector $ X_t $ is\n",
    "\n",
    "$$\n",
    "X_t = \\begin{bmatrix}  X_{1,t} & X_{2,t} & \\cdots & X_{m,t}     \\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "where $ T $ again denotes complex transposition and $ X_{i,t} $ is an observation on variable $ i $ at time $ t $.\n",
    "\n",
    "From $ \\tilde X $,   form two matrices\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_{n-1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "X' =  \\begin{bmatrix} X_2 \\mid X_3 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here $ ' $ does not denote matrix transposition but instead is part of the name of the matrix $ X' $.\n",
    "\n",
    "In forming $ X $ and $ X' $, we have in each case  dropped a column from $ \\tilde X $,  the last column in the case of $ X $, and  the first column in the case of $ X' $.\n",
    "\n",
    "Evidently, $ X $ and $ X' $ are both $ m \\times \\tilde n $ matrices where $ \\tilde n = n - 1 $.\n",
    "\n",
    "We denote the rank of $ X $ as $ p \\leq \\min(m, \\tilde n) = \\tilde n $.\n",
    "\n",
    "We start with a system consisting of $ m $ least squares regressions of **everything** on one lagged value of **everything**:\n",
    "\n",
    "$$\n",
    "X' = A  X + \\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-eq-afullformula'></a>\n",
    "$$\n",
    "A =  X'  X^{+} \\tag{6.3}\n",
    "$$\n",
    "\n",
    "and where the (possibly huge) $ m \\times m $ matrix $ X^{+} $ is the Moore-Penrose generalized inverse of $ X $.\n",
    "\n",
    "The $ i $th the row of $ A $ is an $ m \\times 1 $ vector of regression coefficients of $ X_{i,t+1} $ on $ X_{j,t}, j = 1, \\ldots, m $.\n",
    "\n",
    "Consider the (reduced) singular value decomposition\n",
    "\n",
    "$$\n",
    "X =  U \\Sigma  V^T\n",
    "$$\n",
    "\n",
    "where $ U $ is $ m \\times p $, $ \\Sigma $ is a $ p \\times p $ diagonal  matrix, and $ V^T $ is a $ p \\times m $ matrix.\n",
    "\n",
    "Here $ p $ is the rank of $ X $, where necessarily $ p \\leq \\tilde n $.\n",
    "\n",
    "(We  described and illustrated a **reduced** singular value decomposition above, and compared it with a **full** singular value decomposition.)\n",
    "\n",
    "We could construct the generalized inverse $ X^+ $  of $ X $ by using\n",
    "a singular value decomposition  $ X = U \\Sigma V^T $ to compute\n",
    "\n",
    "\n",
    "<a id='equation-eq-xpinverse'></a>\n",
    "$$\n",
    "X^{+} =  V \\Sigma^{-1}  U^T \\tag{6.4}\n",
    "$$\n",
    "\n",
    "where the matrix $ \\Sigma^{-1} $ is constructed by replacing each non-zero element of $ \\Sigma $ with $ \\sigma_j^{-1} $.\n",
    "\n",
    "We could use formula [(6.4)](#equation-eq-xpinverse)   together with formula [(6.3)](#equation-eq-afullformula) to compute the matrix  $ A $ of regression coefficients.\n",
    "\n",
    "Instead of doing that, weâ€™ll use **dynamic mode decomposition** to compute a rank $ r $ approximation to $ A $,\n",
    "where $ r <  p $.\n",
    "\n",
    "The idea behind **dynamic mode decomposition** is to construct this low rank  approximation to $ A $ that\n",
    "\n",
    "- constructs an $ m \\times r $ matrix $ \\Phi $ that captures effects  on all $ m $ variables of $ r \\leq p $  **modes** that are associated with the $ r $ largest eigenvalues of $ A $  \n",
    "- uses $ \\Phi $, the current value of $ X_t $, and  powers of the $ r $ largest eigenvalues of $ A $ to forecast *future* $ X_{t+j} $â€™s  \n",
    "\n",
    "\n",
    "An important properities of the DMD algorithm that we shall describe soon is that\n",
    "\n",
    "- columns of the $ m \\times r $ matrix $ \\Phi $ are   eigenvectors of $ A $ that correspond to the $ r $ largest eigenvalues of $ A $  \n",
    "- Tu et al. [[TRL+14](https://python.quantecon.org/zreferences.html#id11)] verify these useful properties  \n",
    "- Weâ€™ll provide their proof below  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99a6c7",
   "metadata": {},
   "source": [
    "### Preliminary Analysis\n",
    "\n",
    "Weâ€™ll put basic ideas on the table by starting with the special case in which $ r = p $.\n",
    "\n",
    "Thus, we retain\n",
    "all $ p $ singular values of $ X $.\n",
    "\n",
    "(Later, weâ€™ll retain only $ r < p $ of them)\n",
    "\n",
    "When $ r = p $,  formula\n",
    "[(6.4)](#equation-eq-xpinverse) implies that\n",
    "\n",
    "\n",
    "<a id='equation-eq-aformbig'></a>\n",
    "$$\n",
    "A = X' V \\Sigma^{-1}  U^T \\tag{6.5}\n",
    "$$\n",
    "\n",
    "where $ V $ is an $ \\tilde n \\times p $ matrix, $ \\Sigma^{-1} $ is a $ p \\times p $ matrix,  $ U $ is a $ p \\times m $ matrix,\n",
    "and  $ U^T  U = I_p $ and $ V V^T = I_m $.\n",
    "\n",
    "We use the $ p $  columns of $ U $, and thus the $ p $ rows of $ U^T $,  to define   a $ p \\times 1 $  vector $ \\tilde X_t $ to be used  in a lower-dimensional description of the evolution of the system:\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildexdef2'></a>\n",
    "$$\n",
    "\\tilde X_t = U^T X_t . \\tag{6.6}\n",
    "$$\n",
    "\n",
    "Since $ U^T U $ is a $ p \\times p $ identity matrix, it follows from equation [(6.6)](#equation-eq-tildexdef2) that we can recover $ X_t $ from $ \\tilde X_t $ by using\n",
    "\n",
    "\n",
    "<a id='equation-eq-xdecoder'></a>\n",
    "$$\n",
    "X_t = U \\tilde X_t . \\tag{6.7}\n",
    "$$\n",
    "\n",
    "- Equation [(6.6)](#equation-eq-tildexdef2) serves as an **encoder** that reduces summarizes the $ m \\times 1 $ vector $ X_t $ by the $ p \\times 1 $ vector $ \\tilde X_t $  \n",
    "- Equation [(6.7)](#equation-eq-xdecoder) serves as a **decoder** that recovers the $ m \\times 1 $ vector $ X_t $ from the $ p \\times 1 $ vector $ \\tilde X_t $  \n",
    "\n",
    "\n",
    "The following  $ p \\times p $ transition matrix governs the motion of $ \\tilde X_t $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-atilde0'></a>\n",
    "$$\n",
    "\\tilde A = U^T A U = U^T X' V \\Sigma^{-1} . \\tag{6.8}\n",
    "$$\n",
    "\n",
    "Evidently,\n",
    "\n",
    "\n",
    "<a id='equation-eq-xtildemotion'></a>\n",
    "$$\n",
    "\\tilde X_{t+1} = \\tilde A \\tilde X_t \\tag{6.9}\n",
    "$$\n",
    "\n",
    "Notice that if we multiply both sides of [(6.9)](#equation-eq-xtildemotion) by $ U $\n",
    "we get\n",
    "\n",
    "$$\n",
    "U \\tilde X_t = U \\tilde A \\tilde X_t =  U \\tilde A U^T X_t\n",
    "$$\n",
    "\n",
    "which by virtue of decoder equation [(6.9)](#equation-eq-xtildemotion) recovers\n",
    "\n",
    "$$\n",
    "X_{t+1} = A X_t .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fed5ab4",
   "metadata": {},
   "source": [
    "### Lower Rank Approximations\n",
    "\n",
    "Instead of using formula [(6.5)](#equation-eq-aformbig),  weâ€™ll  compute the $ r $ largest singular values of $ X $ and  form matrices $ \\tilde V, \\tilde U $ corresponding to those $ r $ singular values.\n",
    "\n",
    "Weâ€™ll then construct  a reduced-order system of dimension $ r $ by forming an  $ r \\times r $ transition matrix\n",
    "$ \\tilde A $ redefined by\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildea-1'></a>\n",
    "$$\n",
    "\\tilde A = \\tilde U^T A \\tilde U \\tag{6.10}\n",
    "$$\n",
    "\n",
    "Here we use $ \\tilde U $ rather than $ U $ as we did earlier.\n",
    "\n",
    "This redefined  $ \\tilde A $ matrix governs the dynamics of a redefined  $ r \\times 1 $ vector $ \\tilde X_t $\n",
    "according to\n",
    "\n",
    "$$\n",
    "\\tilde X_{t+1} = \\tilde A \\tilde X_t\n",
    "$$\n",
    "\n",
    "where an approximation  $ \\check X_t $ to   the original $ m \\times 1 $ vector $ X_t $ can be acquired by projecting $ X_t $ onto a subspace spanned by\n",
    "the columns of $ \\tilde U $:\n",
    "\n",
    "$$\n",
    "\\check X_t = \\tilde U \\tilde X_t .\n",
    "$$\n",
    "\n",
    "Weâ€™ll provide a formula for $ \\tilde X_t $ soon.\n",
    "\n",
    "From equation [(6.10)](#equation-eq-tildea-1) and [(6.5)](#equation-eq-aformbig) it follows that\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaform'></a>\n",
    "$$\n",
    "\\tilde A = \\tilde U^T X' \\tilde V \\Sigma^{-1} \\tag{6.11}\n",
    "$$\n",
    "\n",
    "Next, weâ€™ll Construct an eigencomposition of $ \\tilde A $ defined in equation [(6.10)](#equation-eq-tildea-1):\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildeaeigen'></a>\n",
    "$$\n",
    "\\tilde A W =  W \\Lambda \\tag{6.12}\n",
    "$$\n",
    "\n",
    "where $ \\Lambda $ is a $ r \\times r $ diagonal matrix of eigenvalues and the columns of $ W $ are corresponding eigenvectors\n",
    "of $ \\tilde A $.\n",
    "\n",
    "Both $ \\Lambda $ and $ W $ are $ r \\times r $ matrices.\n",
    "\n",
    "A key step now is to construct the $ m \\times r $ matrix\n",
    "\n",
    "\n",
    "<a id='equation-eq-phiformula'></a>\n",
    "$$\n",
    "\\Phi = X' \\tilde  V  \\tilde \\Sigma^{-1} W \\tag{6.13}\n",
    "$$\n",
    "\n",
    "As asserted above, and as we shall soon verify,   columns of $ \\Phi $ are  eigenvectors of $ A $ corresponding to the largest  $ r $  eigenvalues of $ A $.\n",
    "\n",
    "We can construct an $ r \\times m $ matrix generalized inverse  $ \\Phi^{+} $  of $ \\Phi $.\n",
    "\n",
    "We define an $ r \\times 1 $  vector $ b $ of $ r $  modes associated with the $ r $ largest singular values.\n",
    "\n",
    "\n",
    "<a id='equation-eq-bphieqn'></a>\n",
    "$$\n",
    "b= \\Phi^{+} X_1 \\tag{6.14}\n",
    "$$\n",
    "\n",
    "**Proposition** The $ r $ columns of $ \\Phi $ are eigenvectors of $ A $ that correspond to the largest $ r $ eigenvalues of $ A $.\n",
    "\n",
    "**Proof:** From formula [(6.13)](#equation-eq-phiformula) we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  A \\Phi & =  (X' \\tilde V \\tilde \\Sigma^{-1} \\tilde U^T) (X' \\tilde V \\tilde \\Sigma^{-1} W) \\cr\n",
    "  & = X' \\tilde V \\Sigma^{-1} \\tilde A W \\cr\n",
    "  & = X' \\tilde V \\tilde \\Sigma^{-1} W \\Lambda \\cr\n",
    "  & = \\Phi \\Lambda \n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, we can conclude that\n",
    "\n",
    "\n",
    "<a id='equation-eq-aphilambda'></a>\n",
    "$$\n",
    "A \\Phi = \\Phi \\Lambda \\tag{6.15}\n",
    "$$\n",
    "\n",
    "Let $ \\phi_i $ be the the $ i $the column of $ \\Phi $ and $ \\lambda_i $ be the corresponding $ i $ eigenvalue of $ \\tilde A $ from decomposition [(6.12)](#equation-eq-tildeaeigen).\n",
    "\n",
    "Writing out the $ m \\times r $ vectors on both sides of  equation [(6.15)](#equation-eq-aphilambda) and equating them gives\n",
    "\n",
    "$$\n",
    "A \\phi_i = \\lambda_i \\phi_i .\n",
    "$$\n",
    "\n",
    "Thus, $ \\phi_i $ is an eigenvector of $ A $ that corresponds to eigenvalue  $ \\lambda_i $ of $ A $.\n",
    "\n",
    "This concludes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c537ac7",
   "metadata": {},
   "source": [
    "### Putting Things Together\n",
    "\n",
    "With $ \\Lambda, \\Phi, \\Phi^{+} $ in hand, our least-squares fitted dynamics fitted to the $ r $  modes\n",
    "are governed by\n",
    "\n",
    "\n",
    "<a id='equation-eq-xdynamicsapprox'></a>\n",
    "$$\n",
    "X_{t+1}^{(r)} = \\Phi \\Lambda \\Phi^{+} X_t^{(r)} . \\tag{6.16}\n",
    "$$\n",
    "\n",
    "where $ X_t^{(r)} $ is an $ m \\times 1 $ vector.\n",
    "\n",
    "By virtue of equation [(6.15)](#equation-eq-aphilambda), it follows that **if we had kept $ r = p $**,  this equation would be equivalent with\n",
    "\n",
    "\n",
    "<a id='equation-eq-xdynamicstrue'></a>\n",
    "$$\n",
    "X_{t+1} = A X_t . \\tag{6.17}\n",
    "$$\n",
    "\n",
    "When $ r < p $, equation [(6.16)](#equation-eq-xdynamicsapprox) is an approximation (of reduced  order $ r $) to the $ X $ dynamics in equation\n",
    "[(6.17)](#equation-eq-xdynamicstrue).\n",
    "\n",
    "Conditional on $ X_t $, we construct forecasts $ \\check X_{t+j} $ of $ X_{t+j}, j = 1, 2, \\ldots, $  from\n",
    "\n",
    "\n",
    "<a id='equation-eq-checkxevoln'></a>\n",
    "$$\n",
    "\\check X_{t+j} = \\Phi \\Lambda^j \\Phi^{+} X_t \\tag{6.18}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ff74c",
   "metadata": {},
   "source": [
    "## Some Refinements\n",
    "\n",
    "Because it involves smaller matrices, formula [(6.20)](#equation-eq-beqnsmall) below is a computationally more efficient way to compute $ b $ than using equation [(6.14)](#equation-eq-bphieqn).\n",
    "\n",
    "Define  a projection  $ \\tilde X_1 $ of $ X_1 $ onto the $ r $ dominant modes by\n",
    "\n",
    "\n",
    "<a id='equation-eq-x1proj'></a>\n",
    "$$\n",
    "\\tilde X_1 = \\Phi b \\tag{6.19}\n",
    "$$\n",
    "\n",
    "- It follows that  \n",
    "  $$\n",
    "  \\tilde U \\tilde X_1 = X' \\tilde V \\tilde \\Sigma^{-1} W b\n",
    "  $$\n",
    "  and  \n",
    "  $$\n",
    "  \\tilde X_1 = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} W b\n",
    "  $$\n",
    "- Recall that $ \\tilde A = \\tilde U^T X' \\tilde V \\tilde \\Sigma^{-1} $ so that  \n",
    "  $$\n",
    "  \\tilde X_1 = \\tilde A W b\n",
    "  $$\n",
    "  and therefore, by the eigendecomposition  [(6.12)](#equation-eq-tildeaeigen) of $ \\tilde A $, we have  \n",
    "  $$\n",
    "  \\tilde X_1 = W \\Lambda b\n",
    "  $$\n",
    "- Therefore,  \n",
    "  $$\n",
    "  b = ( W \\Lambda)^{-1} \\tilde X_1 \\tag{6.20}\n",
    "  $$\n",
    "  which is  computationally more efficient than equation [(6.14)](#equation-eq-bphieqn).  \n",
    "- It follows that the following equation is equivalent with [(6.18)](#equation-eq-checkxevoln)  \n",
    "  $$\n",
    "  \\check X_{t+j} = \\Phi \\Lambda^j (W \\Lambda)^{-1} \\tilde X_t \\tag{6.21}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ee6f3",
   "metadata": {},
   "source": [
    "## Reduced-order VAR\n",
    "\n",
    "DMD  is a natural tool for estimating a **reduced order vector autoregression**,\n",
    "an object that we define in terms of the population regression equation\n",
    "\n",
    "\n",
    "<a id='equation-eq-varred'></a>\n",
    "$$\n",
    "X_{t+1} = \\check A X_t + C \\epsilon_{t+1} \\tag{6.22}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ X_t $ is an $ m \\times 1 $ vector  \n",
    "- $ \\check A $ is an $ m \\times m $ matrix of rank $ r $ whose eigenvalues are all less than $ 1 $ in modulus  \n",
    "- $ \\epsilon_{t+1} \\sim {\\mathcal N}(0, I) $ is an $ m \\times 1 $ vector of i.i.d. shocks  \n",
    "- $ E \\epsilon_{t+1} X_t^T = 0 $, so that all shocks are orthogonal to all regressors  \n",
    "\n",
    "\n",
    "To link this model to a dynamic mode decomposition (DMD), again take\n",
    "\n",
    "$$\n",
    "X = [ X_1 \\mid X_2 \\mid \\cdots \\mid X_{n-1} ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =  [ X_2 \\mid X_3 \\mid \\cdots \\mid X_n ]\n",
    "$$\n",
    "\n",
    "so that according to  model [(6.22)](#equation-eq-varred)\n",
    "\n",
    "$$\n",
    "X' = \\begin{bmatrix} \\check A X_1 + C \\epsilon_2  \\mid \\check A X_2 + C \\epsilon_3 \\mid \\cdots \\mid \\check A X_{n-1} +  C \n",
    "\\epsilon_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To illustrate some useful calculations, assume that $ n =3 $ and form\n",
    "\n",
    "$$\n",
    "X' X^T = \\begin{bmatrix} \\check A X_1 + C \\epsilon_2  &  \\check A X_2 + C \\epsilon_3 \\end{bmatrix} \n",
    "   \\begin{bmatrix} X_1^T \\cr X_2^T \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "X' X^T = \\check A ( X_1 X_1^T + X_2 X_2^T) + C( \\epsilon_2 X_1^T + \\epsilon_3 X_2^T)\n",
    "$$\n",
    "\n",
    "but because\n",
    "\n",
    "$$\n",
    "E ( \\epsilon_2 X_1^T + \\epsilon_3 X_2^T)  = 0\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "X' X^T = \\check A ( X_1 X_1^T + X_2 X_2^T)\n",
    "$$\n",
    "\n",
    "Evidently,\n",
    "\n",
    "$$\n",
    "X X^T = ( X_1 X_1^T + X_2 X_2^T)\n",
    "$$\n",
    "\n",
    "so that our  matrix  $ \\check A $ of least squares regression coefficients is\n",
    "\n",
    "$$\n",
    "\\check A = (X' X^T)  (X X^T)^+\n",
    "$$\n",
    "\n",
    "Our **assumption** that $ \\check A $ is a matrix of rank $ r $ leads us to represent it as\n",
    "\n",
    "$$\n",
    "\\check A = \\Phi \\Lambda \\Phi^{+}\n",
    "$$\n",
    "\n",
    "where $ \\Phi $ and $ \\Lambda $ are computed with the DMD algorithm described above.\n",
    "\n",
    "Associated with the VAR representation [(6.22)](#equation-eq-varred)\n",
    "is the usual moving average representation\n",
    "\n",
    "$$\n",
    "X_{t+j} = \\check A^j X_t + C \\epsilon_{t+j} + \\check A C \\epsilon_{t+j-1} + \\cdots \\check A^{j-1} \\epsilon_{t+1}\n",
    "$$\n",
    "\n",
    "After computing $ \\check A $, we can construct sample versions\n",
    "of\n",
    "\n",
    "$$\n",
    "C \\epsilon_{t+1} = X_{t+1} - \\check A X_t , \\quad t =1, \\ldots, n-1\n",
    "$$\n",
    "\n",
    "and check whether they are serially uncorrelated as assumed in [(6.22)](#equation-eq-varred).\n",
    "\n",
    "For example, we can compute spectra and cross-spectra of components of $ C \\epsilon_{t+1} $\n",
    "and check for serial-uncorrelatedness in the usual ways.\n",
    "\n",
    "We can also estimate the covariance matrix of $ C \\epsilon_{t+1} $\n",
    "from\n",
    "\n",
    "$$\n",
    "\\frac{1}{n-1} \\sum_{t=1}^{n-1} (C \\epsilon_{t+1} )( C \\epsilon_{t+1})^T\n",
    "$$\n",
    "\n",
    "It can be enlightening to diagonize  our reduced order VAR [(6.22)](#equation-eq-varred) by noting that it can\n",
    "be written\n",
    "\n",
    "$$\n",
    "X_{t+1} = \\Phi \\Lambda \\Phi^{+} X_t + C \\epsilon_{t+1}\n",
    "$$\n",
    "\n",
    "and then writing it as\n",
    "\n",
    "$$\n",
    "\\Phi^+ X_{t+1} = \\Lambda  \\Phi^{+} X_t +  \\Phi^+ C \\epsilon_{t+1}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-varmodes'></a>\n",
    "$$\n",
    "\\bar X_{t+1} = \\Lambda \\bar X_t + \\bar \\epsilon_{t+1} \\tag{6.23}\n",
    "$$\n",
    "\n",
    "where $ \\bar X_t $ is an $ r \\times 1 $ **mode** and $ \\bar \\epsilon_{t+1} $ is an $ r \\times 1 $\n",
    "shock.\n",
    "\n",
    "The $ r $ modes $ \\bar X_t $ obey the  first-order VAR [(6.23)](#equation-eq-varmodes) in which $ \\Lambda $ is an $ r \\times r $ diagonal matrix.\n",
    "\n",
    "Note that while $ \\Lambda $ is diagonal, the contemporaneous covariance matrix of $ \\bar \\epsilon_{t+1} $ need not be.\n",
    "\n",
    "**Remark:** It is permissible for $ X_t $ to contain lagged values of  observables.\n",
    "\n",
    "For example, we might have a setting in which\n",
    "\n",
    "$$\n",
    "X_t = \\begin{bmatrix}\n",
    "y_{1t} \\cr\n",
    "y_{1,t-1} \\cr\n",
    "\\vdots \\cr\n",
    "y_{1, t-k}\\cr\n",
    "y_{2,t} \\cr\n",
    "y_{2, t-1} \\cr\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d361115",
   "metadata": {},
   "source": [
    "## Source for Some Python Code\n",
    "\n",
    "You can find a Python implementation of DMD here:\n",
    "\n",
    "[https://mathlab.github.io/PyDMD/](https://mathlab.github.io/PyDMD/)"
   ]
  }
 ],
 "metadata": {
  "date": 1646970182.2520251,
  "filename": "svd_intro.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Singular Value Decomposition (SVD)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}