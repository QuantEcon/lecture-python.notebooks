{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829971ee",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "In addition to regular packages contained in Anaconda by default, this lecture also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93813ae",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5961a2a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef49508",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **singular value decomposition** is a work-horse in applications of least squares projection that\n",
    "form the backbone of important parts of modern machine learning methods.\n",
    "\n",
    "This lecture describes the singular value decomposition and two of its uses:\n",
    "\n",
    "- principal components analysis (PCA)  \n",
    "- dynamic mode decomposition (DMD)  \n",
    "\n",
    "\n",
    "Each of these can be thought of as data-reduction methods that are designed to capture principal patterns in data by projecting data onto a limited set of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e68b2",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "Let $ X $ be an $ m \\times n $ matrix of rank $ r $.\n",
    "\n",
    "In this lecture, we’ll think of $ X $ as a matrix of **data**.\n",
    "\n",
    "- each column is an **individual** – a time period or person, depending on the application  \n",
    "- each row is a **random variable** measuring an attribute of a time period or a person, depending on the application  \n",
    "\n",
    "\n",
    "We’ll be interested in  two distinct cases\n",
    "\n",
    "- A **short and fat** case in which $ m << n $, so that there are many more columns than rows.  \n",
    "- A  **tall and skinny** case in which $ m >> n $, so that there are many more rows than columns.  \n",
    "\n",
    "\n",
    "We’ll apply a **singular value decomposition** of $ X $ in both situations.\n",
    "\n",
    "In the first case in which there are many more observations $ n $ than random variables $ m $, we learn about the joint distribution of the  random variables by taking averages  across observations of functions of the observations.\n",
    "\n",
    "Here we’ll look for **patterns** by using a **singular value decomosition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the second case in which there are many more random variables $ m $ than observations $ n $, we’ll proceed in a different way.\n",
    "\n",
    "We’ll again use a **singular value decomposition**,  but now to do a **dynamic mode decomposition** (DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e1690",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $ m \\times n $ matrix $ X $ of rank $ r \\leq \\min(m,n) $ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ U $ is an $ m \\times m $ matrix whose columns are eigenvectors of $ X^T X $  \n",
    "- $ V $ is an $ n \\times n $ matrix whose columns are eigenvectors of $ X X^T $  \n",
    "- $ \\Sigma $ is an $ m \\times r $ matrix in which the first $ r $ places on its main diagonal are positive numbers $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_r $ called **singular values**; remaining entries of $ \\Sigma $ are all zero  \n",
    "- The $ r $ singular values are square roots of the eigenvalues of the $ m \\times m $ matrix  $ X X^T $ and the $ n \\times n $ matrix $ X^T X $  \n",
    "- When $ U $ is a complex valued matrix, $ U^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ U $, meaning that\n",
    "  $ U_{ij}^T $ is the complex conjugate of $ U_{ji} $.  \n",
    "- Similarly, when $ V $ is a complex valued matrix, $ V^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ V $  \n",
    "\n",
    "\n",
    "The shapes of $ U $, $ \\Sigma $, and $ V $ are $ \\left(m, m\\right) $, $ \\left(m, n\\right) $, $ \\left(n, n\\right) $, respectively.\n",
    "\n",
    "Below, we shall assume these shapes.\n",
    "\n",
    "However, though we chose not to, there is an alternative shape convention that we could have used.\n",
    "\n",
    "Thus, note that because we assume that $ A $ has rank $ r $, there are only $ r $ nonzero singular values, where $ r=\\textrm{rank}(A)\\leq\\min\\left(m, n\\right) $.\n",
    "\n",
    "Therefore,  we could also write $ U $, $ \\Sigma $, and $ V $ as matrices with shapes $ \\left(m, r\\right) $, $ \\left(r, r\\right) $, $ \\left(r, n\\right) $.\n",
    "\n",
    "Sometimes, we will choose the former one to be consistent with what is adopted by `numpy`.\n",
    "\n",
    "At other times, we’ll use the latter convention in which $ \\Sigma $ is an $ r \\times r $  diagonal matrix.\n",
    "\n",
    "Also, when we discuss the **dynamic mode decomposition** below, we’ll use a special case of the latter  convention in which it is understood that\n",
    "$ r $ is just a pre-specified small number of leading singular values that we think capture the  most interesting  dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b02fb",
   "metadata": {},
   "source": [
    "## Digression:  Polar Decomposition\n",
    "\n",
    "Through  the following identities, the singular value decomposition (SVD) is related to the **polar decomposition** of $ X $\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X & = SQ  \\cr  \n",
    "S & = U\\Sigma U^T \\cr\n",
    "Q & = U V^T \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $ S $ is evidently a symmetric matrix and $ Q $ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca317a",
   "metadata": {},
   "source": [
    "## Principle Components Analysis (PCA)\n",
    "\n",
    "Let’s begin with a case in which $ n >> m $, so that we have many  more observations $ n $ than random variables $ m $.\n",
    "\n",
    "The data matrix $ X $ is **short and fat**  in an  $ n >> m $ case as opposed to a **tall and skinny** case with $ m > > n $ to be discussed later in this lecture.\n",
    "\n",
    "We regard  $ X $ as an  $ m \\times n $ matrix of **data**:\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ j = 1, \\ldots, n $ the column vector $ X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix} $ is a  vector of observations on variables $ \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix} $.\n",
    "\n",
    "In a **time series** setting, we would think of columns $ j $ as indexing different **times** at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "In a **cross section** setting, we would think of columns $ j $ as indexing different **individuals** for  which random variables are observed, while rows index different **random variables**.\n",
    "\n",
    "The number of singular values equals the rank of  matrix $ X $.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $ \\Sigma $ of into a vector $ \\sigma_R $.\n",
    "\n",
    "Set all other entries of $ \\Sigma $ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b97cff",
   "metadata": {},
   "source": [
    "## Relationship of PCA to SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $ X $, first construct  the  SVD of the data matrix $ X $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca1'></a>\n",
    "$$\n",
    "X = U \\Sigma V^T = \\sigma_1 U_1 V_1^T + \\sigma_2 U_2 V_2^T + \\cdots + \\sigma_r U_r V_r^T \\tag{6.1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^T = \\begin{bmatrix}V_1^T\\\\V_2^T\\\\\\ldots\\\\V_n^T\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In equation [(6.1)](#equation-eq-pca1), each of the $ m \\times n $ matrices $ U_{j}V_{j}^T $ is evidently\n",
    "of rank $ 1 $.\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca2'></a>\n",
    "$$\n",
    "X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^T\\\\U_{21}V_{1}^T\\\\\\cdots\\\\U_{m1}V_{1}^T\\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^T\\\\U_{22}V_{2}^T\\\\\\cdots\\\\U_{m2}V_{2}^T\\\\\\end{pmatrix}+\\ldots + \\sigma_r\\begin{pmatrix}U_{1r}V_{r}^T\\\\U_{2r}V_{r}^T\\\\\\cdots\\\\U_{mr}V_{r}^T\\\\\\end{pmatrix} \\tag{6.2}\n",
    "$$\n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation [(6.2)](#equation-eq-pca2) in\n",
    "a time series context:\n",
    "\n",
    "- $ V_{k}^T= \\begin{bmatrix}V_{k1} &  V_{k2} & \\ldots & V_{kn}\\end{bmatrix}  \\quad \\textrm{for each} \\   k=1, \\ldots, n $ is a time series  $ \\lbrace V_{kj} \\rbrace_{j=1}^n $ for the $ k $th principal component  \n",
    "- $ U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m $\n",
    "  is a vector of loadings of variables $ X_i $ on the $ k $th principle component,  $ i=1, \\ldots, m $  \n",
    "- $ \\sigma_k $ for each $ k=1, \\ldots, r $ is the strength of $ k $th **principal component**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a0204",
   "metadata": {},
   "source": [
    "## Reduced Versus Full SVD\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    "[https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "Let’s do a small experiment to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306bef28",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644c57d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2dcb0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3328c",
   "metadata": {},
   "source": [
    "**Remark:** The cells above illustrate application of the  `fullmatrices=True` and `full-matrices=False` options.\n",
    "Using `full-matrices=False` returns a reduced singular value decomposition. This option implements\n",
    "an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius\n",
    "norm of the discrepancy between the approximating matrix and the matrix being approximated.\n",
    "Optimality in this sense is  established in the celebrated Eckart–Young theorem. See [https://en.wikipedia.org/wiki/Low-rank_approximation](https://en.wikipedia.org/wiki/Low-rank_approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cb4a7",
   "metadata": {},
   "source": [
    "## PCA with Eigenvalues and Eigenvectors\n",
    "\n",
    "We now  turn to using the eigen decomposition of a sample covariance matrix to do PCA.\n",
    "\n",
    "Let $ X_{m \\times n} $ be our $ m \\times n $ data matrix.\n",
    "\n",
    "Let’s assume that sample means of all variables are zero.\n",
    "\n",
    "We can make sure that this is true by **pre-processing** the data by substracting sample means appropriately.\n",
    "\n",
    "Define the sample covariance matrix $ \\Omega $ as\n",
    "\n",
    "$$\n",
    "\\Omega = XX^T\n",
    "$$\n",
    "\n",
    "Then use an eigen decomposition to represent $ \\Omega $ as follows:\n",
    "\n",
    "$$\n",
    "\\Omega =P\\Lambda P^T\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ P $ is $ m×m $ matrix of eigenvectors of $ \\Omega $  \n",
    "- $ \\Lambda $ is a diagonal matrix of eigenvalues of $ \\Omega $  \n",
    "\n",
    "\n",
    "We can then represent $ X $ as\n",
    "\n",
    "$$\n",
    "X=P\\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda\n",
    "$$\n",
    "\n",
    "We can verify that\n",
    "\n",
    "$$\n",
    "XX^T=P\\Lambda P^T\n",
    "$$\n",
    "\n",
    "It follows that we can represent the data matrix as\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda\n",
    "$$\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we obtained through the SVD above, we first note that $ \\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j $.\n",
    "\n",
    "Now define  $ \\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}} $\n",
    "which evidently implies that $ \\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^T=1 $.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which evidently agrees with\n",
    "\n",
    "$$\n",
    "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}\n",
    "$$\n",
    "\n",
    "provided that  we set\n",
    "\n",
    "- $ U_j=P_j $ (the loadings of variables on principal components)  \n",
    "- $ {V_k}^{T}=\\tilde{\\epsilon_k} $ (the principal components)  \n",
    "\n",
    "\n",
    "Since there are several possible ways of computing  $ P $ and $ U $ for  given a data matrix $ X $, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We want a way that leads to the same $ U $ and $ P $.\n",
    "\n",
    "In the following, we accomplish this by\n",
    "\n",
    "1. sorting eigenvalues and singular values in descending order  \n",
    "1. imposing positive diagonals on $ P $ and $ U $ and adjusting signs in $ V^T $ accordingly  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8aaea",
   "metadata": {},
   "source": [
    "## Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider the following SVD of an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "XX^T&=U\\Sigma V^TV\\Sigma^T U^T\\cr\n",
    "&\\equiv U\\Sigma\\Sigma^TU^T\\cr\n",
    "&\\equiv U\\Lambda U^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, $ U $ in the SVD is the matrix $ P $  of\n",
    "eigenvectors of $ XX^T $ and $ \\Sigma \\Sigma^T $ is the matrix $ \\Lambda $ of eigenvalues.\n",
    "\n",
    "Second, let’s compute\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^TX &=V\\Sigma^T U^TU\\Sigma V^T\\\\\n",
    "&=V\\Sigma^T{\\Sigma}V^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the matrix $ V $ in the SVD is the matrix of eigenvectors of $ X^TX $\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "X X^T = P \\Lambda P^T\n",
    "$$\n",
    "\n",
    "where $ P $ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $ X $, we know that\n",
    "\n",
    "$$\n",
    "X X^T = U \\Sigma \\Sigma^T U^T\n",
    "$$\n",
    "\n",
    "where $ U $ is an orthonal matrix.\n",
    "\n",
    "Thus, $ P = U $ and we have the representation of $ X $\n",
    "\n",
    "$$\n",
    "X = P \\epsilon = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "U^T X = \\Sigma V^T = \\epsilon\n",
    "$$\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$\n",
    "\\epsilon \\epsilon^T = \\Sigma V^T V \\Sigma^T = \\Sigma \\Sigma^T = \\Lambda ,\n",
    "$$\n",
    "\n",
    "so that everything fits together.\n",
    "\n",
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8563fe9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Ω = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        𝜆, P = LA.eigh(self.Ω)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(𝜆.size), key=lambda x: 𝜆[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.𝜆 = 𝜆[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Λ = np.diag(self.𝜆)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.𝜆) / self.𝜆.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.𝜖 = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        𝜖 = self.𝜖[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ 𝜖\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, 𝜎, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(𝜎.size), key=lambda x: 𝜎[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.𝜎 = 𝜎[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Σ = np.zeros((self.m, self.n))\n",
    "        self.Σ[:d, :d] = np.diag(self.𝜎)\n",
    "\n",
    "        𝜎_sq = self.𝜎 ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(𝜎_sq) / 𝜎_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Σ = self.Σ[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Σ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        𝜖 = self.𝜖[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ 𝜖\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Σ = self.Σ[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Σ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86119011",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a3ea7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'λ = {da.λ}\\n')\n",
    "    print(f'σ^2 = {da.σ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.ε.T)\n",
    "    axs[0].set_title('ε')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.λ))\n",
    "    axs[1].set_title('$V^T*\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc1781",
   "metadata": {},
   "source": [
    "## Dynamic Mode Decomposition (DMD)\n",
    "\n",
    "We now turn to the case in which $ m >>n $ so that there are many more random variables $ m $ than observations $ n $.\n",
    "\n",
    "This is the **tall and skinny** case associated with **Dynamic Mode Decomposition**.\n",
    "\n",
    "You can read about Dynamic Mode Decomposition here [[KBBWP16](https://python.quantecon.org/zreferences.html#id24)].\n",
    "\n",
    "We start  with an $ m \\times n $ matrix of data $ \\tilde X $ of the form\n",
    "\n",
    "$$\n",
    "\\tilde X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ t = 1, \\ldots, n $,  the $ m \\times 1 $ vector $ X_t $ is\n",
    "\n",
    "$$\n",
    "X_t = \\begin{bmatrix}  X_{1,t} & X_{2,t} & \\cdots & X_{m,t}     \\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "where $ T $ denotes transposition and $ X_{i,t} $ is an observations on variable $ i $ at time $ t $.\n",
    "\n",
    "From $ \\tilde X $,   form two matrices\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_{n-1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "X' =  \\begin{bmatrix} X_2 \\mid X_3 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(Note that here $ ' $ does not denote matrix transposition but instead is part of the name of the matrix $ X' $.)\n",
    "\n",
    "In forming $ X $ and $ X' $, we have in each case  dropped a column from $ \\tilde X $.\n",
    "\n",
    "Evidently, $ X $ and $ X' $ are both $ m \\times \\tilde n $ matrices where $ \\tilde n = n - 1 $.\n",
    "\n",
    "We start with a system consisting of $ m $ least squares regressions of **everything** on one lagged value of **everything**:\n",
    "\n",
    "$$\n",
    "X' = A  X + \\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "A =  X'  X^{+}\n",
    "$$\n",
    "\n",
    "and where the (huge) $ m \\times m $ matrix $ X^{+} $ is the Moore-Penrose generalized inverse of $ X $ that we could compute\n",
    "as\n",
    "\n",
    "$$\n",
    "X^{+} = V \\Sigma^{-1} U^T\n",
    "$$\n",
    "\n",
    "where the matrix $ \\Sigma^{-1} $ is constructed by replacing each non-zero element of $ \\Sigma $ with $ \\sigma_j^{-1} $.\n",
    "\n",
    "The idea behind **dynamic mode decomposition** is to construct an approximation that\n",
    "\n",
    "- sidesteps computing the generalized inverse $ X^{+} $  \n",
    "- retains only the largest  $ \\tilde r< < r $ eigenvalues and associated eigenvectors of $ U $ and $ V^T $  \n",
    "- constructs an $ m \\times \\tilde r $ matrix $ \\Phi $ that captures effects  on all $ m $ variables of $ r $ dynamic modes  \n",
    "- uses $ \\Phi $ and  powers of $ \\tilde r $ leading singular values to forecast *future* $ X_t $’s  \n",
    "\n",
    "\n",
    "The magic of **dynamic mode decomposition** is that we accomplish this without ever computing the regression coefficients $ A = X' X^{+} $.\n",
    "\n",
    "To construct a DMD, we deploy the following steps:\n",
    "\n",
    "- Compute the singular value decomposition  \n",
    "  $$\n",
    "  X = U \\Sigma V^T\n",
    "  $$\n",
    "  where $ U $ is $ m \\times r $, $ \\Sigma $ is an $ r \\times r $ diagonal  matrix, and $ V^T $ is an $ r \\times \\tilde n $ matrix.  \n",
    "- Notice that (though it would be costly), we could compute $ A $ by solving  \n",
    "  $$\n",
    "  A = X' V \\Sigma^{-1} U^T\n",
    "  $$\n",
    "  But we won’t do that.  \n",
    "  Instead we’ll proceed as follows.  \n",
    "  Note that since,  $ X' = A U \\Sigma V^T $, we know that  \n",
    "  $$\n",
    "  A U  =  X' V \\Sigma^{-1}\n",
    "  $$\n",
    "  so that  \n",
    "  $$\n",
    "  U^T X' V \\Sigma^{-1} = U^T A U \\equiv \\tilde A \\tag{6.3}\n",
    "  $$\n",
    "- At this point,  we  deploy a reduced-dimension version of formula {eq}`eq:tildeAform} by  \n",
    "- using only the  columns of $ U $ that correspond to the $ \\tilde r $ largest singular values.  \n",
    "  Tu et al. [[TRL+14](https://python.quantecon.org/zreferences.html#id11)] verify that eigenvalues and eigenvectors of $ \\tilde A $ equal the leading eigenvalues and associated eigenvectors of $ A $.  \n",
    "- Construct an eigencomposition of $ \\tilde A $ that satisfies  \n",
    "  $$\n",
    "  \\tilde A W =  W \\Lambda\n",
    "  $$\n",
    "  where $ \\Lambda $ is a $ \\tilde r \\times \\tilde r $ diagonal matrix of eigenvalues and the columns of $ W $ are corresponding eigenvectors\n",
    "  of $ \\tilde A $.   Both $ \\Lambda $ and $ W $ are $ \\tilde r \\times \\tilde r $ matrices.  \n",
    "- Construct the $ m \\times \\tilde r $ matrix  \n",
    "  $$\n",
    "  \\Phi = X' V \\Sigma^{-1} W\n",
    "  $$\n",
    "  Let $ \\Phi^{+} $ be a generalized inverse of $ \\Phi $; $ \\Phi^{+} $ is an $ \\tilde r \\times m $ matrix.  \n",
    "- Define an initial vector $ b $ of dominant modes by  \n",
    "  $$\n",
    "  b= \\Phi^{+} X_1\n",
    "  $$\n",
    "  where evidently $ b $ is an $ \\tilde r \\times 1 $ vector.  \n",
    "\n",
    "\n",
    "With $ \\Lambda, \\Phi, \\Phi^{+} $ in hand, our least-squares fitted dynamics fitted to the $ \\tilde r $ dominant modes\n",
    "are governed by\n",
    "\n",
    "$$\n",
    "X_{t+1} = \\Phi \\Lambda \\Phi^{+} X_t\n",
    "$$\n",
    "\n",
    "Conditional on $ X_t $, we construct forecasts $ \\check X_{t+j} $ of $ X_{t+j}, j = 1, 2, \\ldots, $  from\n",
    "\n",
    "$$\n",
    "\\check X_{t+j} = \\Phi \\Lambda^j \\Phi^{+} X_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1bdd1e",
   "metadata": {},
   "source": [
    "## Reduced-order VAR\n",
    "\n",
    "DMD  is a natural tool for estimating a **reduced order vector autoregression**,\n",
    "an object that we define in terms of the populations regression equation\n",
    "\n",
    "\n",
    "<a id='equation-eq-varred'></a>\n",
    "$$\n",
    "X_{t+1} = \\check A X_t + C \\epsilon_{t+1} \\tag{6.4}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ X_t $ is an $ m \\times 1 $ vector  \n",
    "- $ \\check A $ is an $ m \\times m $ matrix of rank $ r $ whose eigenvalues are all less than $ 1 $ in modulus  \n",
    "- $ \\epsilon_{t+1} \\sim {\\mathcal N}(0, I) $ is an $ m \\times 1 $ vector of i.i.d. shocks  \n",
    "- $ E \\epsilon_{t+1} X_t = 0 $, so that the shocks are orthogonal to the regressors  \n",
    "\n",
    "\n",
    "To link this model to a dynamic mode decomposition (DMD), again take\n",
    "\n",
    "$$\n",
    "X = [ X_1 \\mid X_2 \\mid \\cdots \\mid X_{n-1} ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =  [ X_2 \\mid X_3 \\mid \\cdots \\mid X_n ]\n",
    "$$\n",
    "\n",
    "so that according to  model [(6.4)](#equation-eq-varred)\n",
    "\n",
    "$$\n",
    "X' = \\begin{bmatrix} \\check A X_1 + C \\epsilon_2  \\mid \\check A X_2 + C \\epsilon_3 \\mid \\cdots \\mid \\check A X_{n-1} +  C \n",
    "\\epsilon_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To illustrate some useful calculations, assume that $ n =3 $ and form\n",
    "\n",
    "$$\n",
    "X' X^T = \\begin{bmatrix} \\check A X_1 + C \\epsilon_2  &  \\check A X_2 + C \\epsilon_3 \\end{bmatrix} \n",
    "   \\begin{bmatrix} X_1^T \\cr X_2^T \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "X' X^T = \\check A ( X_1 X_1^T + X_2 X_2^T) + C( \\epsilon_2 X_1^T + \\epsilon_3 X_2^T)\n",
    "$$\n",
    "\n",
    "but because\n",
    "\n",
    "$$\n",
    "E ( \\epsilon_2 X_1^T + \\epsilon_3 X_2^T)  = 0\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "X' X^T = \\check A ( X_1 X_1^T + X_2 X_2^T)\n",
    "$$\n",
    "\n",
    "Evidently,\n",
    "\n",
    "$$\n",
    "X X^T = ( X_1 X_1^T + X_2 X_2^T)\n",
    "$$\n",
    "\n",
    "so that our  matrix  $ \\check A $ of least squares regression coefficients is\n",
    "\n",
    "$$\n",
    "\\check A = (X' X^T)  (X X^T)^+\n",
    "$$\n",
    "\n",
    "Our **assumption** that $ \\check A $ is a matrix of rank $ r $ leads us to represent it as\n",
    "\n",
    "$$\n",
    "\\check A = \\Phi \\Lambda \\Phi^{+}\n",
    "$$\n",
    "\n",
    "where $ \\Phi $ and $ \\Lambda $ are computed with the DMD algorithm described above.\n",
    "\n",
    "Associated with the VAR representation [(6.4)](#equation-eq-varred)\n",
    "is the usual moving average representation\n",
    "\n",
    "$$\n",
    "X_{t+j} = \\check A^j X_t + C \\epsilon_{t+j} + \\check A C \\epsilon_{t+j-1} + \\cdots \\check A^{j-1} \\epsilon_{t+1}\n",
    "$$\n",
    "\n",
    "After computing $ \\check A $, we can construct sample versions\n",
    "of\n",
    "\n",
    "$$\n",
    "C \\epsilon_{t+1} = X_{t+1} - \\check A X_t , \\quad t =1, \\ldots, n-1\n",
    "$$\n",
    "\n",
    "and check whether they are serially uncorrelated as assumed in [(6.4)](#equation-eq-varred).\n",
    "\n",
    "For example, we can compute spectra and cross-spectra of components of $ C \\epsilon_{t+1} $\n",
    "and check for serial-uncorrelatedness in the usual ways.\n",
    "\n",
    "We can also estimate the covariance matrix of $ C \\epsilon_{t+1} $\n",
    "from\n",
    "\n",
    "$$\n",
    "\\frac{1}{n-1} \\sum_{t=1}^{n-1} (C \\epsilon_{t+1} )( C \\epsilon_{t+1})^T\n",
    "$$\n",
    "\n",
    "It can be enlightening to diagonize  our reduced order VAR [(6.4)](#equation-eq-varred) by noting that it can\n",
    "be written\n",
    "\n",
    "$$\n",
    "X_{t+1} = \\Phi \\Lambda \\Phi^{+} X_t + C \\epsilon_{t+1}\n",
    "$$\n",
    "\n",
    "and then writing it as\n",
    "\n",
    "$$\n",
    "\\Phi^+ X_{t+1} = \\Lambda  \\Phi^{+} X_t +  \\Phi^+ C \\epsilon_{t+1}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-eq-varmodes'></a>\n",
    "$$\n",
    "\\tilde X_{t+1} = \\Lambda \\tilde X_t + \\tilde \\epsilon_{t+1} \\tag{6.5}\n",
    "$$\n",
    "\n",
    "where $ \\tilde X_t $ is an $ r \\times 1 $ **mode** and $ \\tilde \\epsilon_{t+1} $ is an $ r \\times 1 $\n",
    "shock.\n",
    "\n",
    "The $ r $ modes $ \\tilde X_t $ obey the  first-order VAR [(6.5)](#equation-eq-varmodes) in which $ \\Lambda $ is an $ r \\times r $ diagonal matrix.\n",
    "\n",
    "Note that while $ \\Lambda $ is diagonal, the contemporaneous covariance matrix of $ \\tilde \\epsilon_{t+1} $ need not be.\n",
    "\n",
    "**Remark:** It is permissible for $ X_t $ to contain lagged values of  observables.\n",
    "\n",
    "For example, we might have a setting in which\n",
    "\n",
    "$$\n",
    "X_t = \\begin{bmatrix}\n",
    "y_{1t} \\cr\n",
    "y_{1,t-1} \\cr\n",
    "\\vdots \\cr\n",
    "y_{1, t-k}\\cr\n",
    "y_{2,t} \\cr\n",
    "y_{2, t-1} \\cr\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd30baa",
   "metadata": {},
   "source": [
    "## Source for Some Python Code\n",
    "\n",
    "You can find a Python implementation of DMD here:\n",
    "\n",
    "[https://mathlab.github.io/PyDMD/](https://mathlab.github.io/PyDMD/)"
   ]
  }
 ],
 "metadata": {
  "date": 1645562516.4338682,
  "filename": "svd_intro.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Singular Value Decomposition (SVD)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}