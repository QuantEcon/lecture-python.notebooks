{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bbe584",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "In addition to regular packages contained in Anaconda by default, this lecture also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ba8ad",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d037e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8d8a2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **singular value decomposition** is a work-horse in applications of least squares projection that\n",
    "form a foundation for  some important machine learning methods.\n",
    "\n",
    "This lecture describes the singular value decomposition and two of its uses:\n",
    "\n",
    "- principal components analysis (PCA)  \n",
    "- dynamic mode decomposition (DMD)  \n",
    "\n",
    "\n",
    "Each of these can be thought of as a data-reduction procedure  designed to capture salient patterns by projecting data onto a limited set of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1306a4f",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "Let $ X $ be an $ m \\times n $ matrix of rank $ r $.\n",
    "\n",
    "Necessarily, $ r \\leq \\min(m,n) $.\n",
    "\n",
    "In this lecture, we’ll think of $ X $ as a matrix of **data**.\n",
    "\n",
    "- each column is an **individual** – a time period or person, depending on the application  \n",
    "- each row is a **random variable** measuring an attribute of a time period or a person, depending on the application  \n",
    "\n",
    "\n",
    "We’ll be interested in  two  cases\n",
    "\n",
    "- A **short and fat** case in which $ m << n $, so that there are many more columns than rows.  \n",
    "- A  **tall and skinny** case in which $ m >> n $, so that there are many more rows than columns.  \n",
    "\n",
    "\n",
    "We’ll apply a **singular value decomposition** of $ X $ in both situations.\n",
    "\n",
    "In the first case in which there are many more observations $ n $ than random variables $ m $, we learn about a joint distribution  by taking averages  across observations of functions of the observations.\n",
    "\n",
    "Here we’ll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the second case in which there are many more random variables $ m $ than observations $ n $, we’ll proceed in a different way.\n",
    "\n",
    "We’ll again use a **singular value decomposition**,  but now to do a **dynamic mode decomposition** (DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f0114",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $ m \\times n $ matrix $ X $ of rank $ r \\leq \\min(m,n) $ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ U $ is an $ m \\times m $ matrix whose columns are eigenvectors of $ X^T X $  \n",
    "- $ V $ is an $ n \\times n $ matrix whose columns are eigenvectors of $ X X^T $  \n",
    "- $ \\Sigma $ is an $ m \\times n $ matrix in which the first $ r $ places on its main diagonal are positive numbers $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_r $ called **singular values**; remaining entries of $ \\Sigma $ are all zero  \n",
    "- The $ r $ singular values are square roots of the eigenvalues of the $ m \\times m $ matrix  $ X X^T $ and the $ n \\times n $ matrix $ X^T X $  \n",
    "- When $ U $ is a complex valued matrix, $ U^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ U $, meaning that\n",
    "  $ U_{ij}^T $ is the complex conjugate of $ U_{ji} $.  \n",
    "- Similarly, when $ V $ is a complex valued matrix, $ V^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ V $  \n",
    "\n",
    "\n",
    "In what is called a **full** SVD, the  shapes of $ U $, $ \\Sigma $, and $ V $ are $ \\left(m, m\\right) $, $ \\left(m, n\\right) $, $ \\left(n, n\\right) $, respectively.\n",
    "\n",
    "There is also an alternative shape convention called an **economy** or **reduced** SVD .\n",
    "\n",
    "Thus, note that because we assume that $ A $ has rank $ r $, there are only $ r $ nonzero singular values, where $ r=\\textrm{rank}(A)\\leq\\min\\left(m, n\\right) $.\n",
    "\n",
    "A **reduced** SVD uses this fact to express $ U $, $ \\Sigma $, and $ V $ as matrices with shapes $ \\left(m, r\\right) $, $ \\left(r, r\\right) $, $ \\left(r, n\\right) $.\n",
    "\n",
    "Sometimes, we will use a full SVD\n",
    "\n",
    "At other times, we’ll use a reduced SVD  in which $ \\Sigma $ is an $ r \\times r $  diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b19c6a",
   "metadata": {},
   "source": [
    "## Digression:  Polar Decomposition\n",
    "\n",
    "A singular value decomposition (SVD) is related to the **polar decomposition** of $ X $\n",
    "\n",
    "$$\n",
    "X  = SQ\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    " S & = U\\Sigma U^T \\cr\n",
    "Q & = U V^T \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and $ S $ is evidently a symmetric matrix and $ Q $ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c5a51",
   "metadata": {},
   "source": [
    "## Principle Components Analysis (PCA)\n",
    "\n",
    "Let’s begin with a case in which $ n >> m $, so that we have many  more observations $ n $ than random variables $ m $.\n",
    "\n",
    "The  matrix $ X $ is **short and fat**  in an  $ n >> m $ case as opposed to a **tall and skinny** case with $ m > > n $ to be discussed later.\n",
    "\n",
    "We regard  $ X $ as an  $ m \\times n $ matrix of **data**:\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ j = 1, \\ldots, n $ the column vector $ X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix} $ is a  vector of observations on variables $ \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix} $.\n",
    "\n",
    "In a **time series** setting, we would think of columns $ j $ as indexing different **times** at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "In a **cross section** setting, we would think of columns $ j $ as indexing different **individuals** for  which random variables are observed, while rows index different **random variables**.\n",
    "\n",
    "The number of positive singular values equals the rank of  matrix $ X $.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $ \\Sigma $ of into a vector $ \\sigma_R $.\n",
    "\n",
    "Set all other entries of $ \\Sigma $ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005048eb",
   "metadata": {},
   "source": [
    "## Relationship of PCA to SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $ X $, first construct  the  SVD of the data matrix $ X $:\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca1'></a>\n",
    "$$\n",
    "X = U \\Sigma V^T = \\sigma_1 U_1 V_1^T + \\sigma_2 U_2 V_2^T + \\cdots + \\sigma_r U_r V_r^T \\tag{6.1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^T = \\begin{bmatrix}V_1^T\\\\V_2^T\\\\\\ldots\\\\V_n^T\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In equation [(6.1)](#equation-eq-pca1), each of the $ m \\times n $ matrices $ U_{j}V_{j}^T $ is evidently\n",
    "of rank $ 1 $.\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\n",
    "<a id='equation-eq-pca2'></a>\n",
    "$$\n",
    "X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^T\\\\U_{21}V_{1}^T\\\\\\cdots\\\\U_{m1}V_{1}^T\\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^T\\\\U_{22}V_{2}^T\\\\\\cdots\\\\U_{m2}V_{2}^T\\\\\\end{pmatrix}+\\ldots + \\sigma_r\\begin{pmatrix}U_{1r}V_{r}^T\\\\U_{2r}V_{r}^T\\\\\\cdots\\\\U_{mr}V_{r}^T\\\\\\end{pmatrix} \\tag{6.2}\n",
    "$$\n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation [(6.2)](#equation-eq-pca2) in\n",
    "a time series context:\n",
    "\n",
    "- $ V_{k}^T= \\begin{bmatrix}V_{k1} &  V_{k2} & \\ldots & V_{kn}\\end{bmatrix}  \\quad \\textrm{for each} \\   k=1, \\ldots, n $ is a time series  $ \\lbrace V_{kj} \\rbrace_{j=1}^n $ for the $ k $th **principal component**  \n",
    "- $ U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m $\n",
    "  is a vector of **loadings** of variables $ X_i $ on the $ k $th principle component,  $ i=1, \\ldots, m $  \n",
    "- $ \\sigma_k $ for each $ k=1, \\ldots, r $ is the strength of $ k $th **principal component**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47457641",
   "metadata": {},
   "source": [
    "## Reduced Versus Full SVD\n",
    "\n",
    "Earlier, we mentioned **full** and **reduced** SVD’s.\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    "[https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "In a **full** SVD\n",
    "\n",
    "- $ U $ is $ m \\times m $  \n",
    "- $ \\Sigma $ is $ m \\times n $  \n",
    "- $ V $ is $ n \\times n $  \n",
    "\n",
    "\n",
    "In a **reduced** SVD\n",
    "\n",
    "- $ U $ is $ m \\times r $  \n",
    "- $ \\Sigma $ is $ r \\times r $  \n",
    "- $ V $ is $ n \\times r $  \n",
    "\n",
    "\n",
    "Let’s do a some  small exerecise  to compare **full** and **reduced** SVD’s.\n",
    "\n",
    "First, let’s study a case in which $ m = 5 > n = 2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02049b1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75a09f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d89d8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7237b",
   "metadata": {},
   "source": [
    "**Remark:** The cells above illustrate application of the  `fullmatrices=True` and `full-matrices=False` options.\n",
    "Using `full-matrices=False` returns a reduced singular value decomposition. This option implements\n",
    "an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius\n",
    "norm of the discrepancy between the approximating matrix and the matrix being approximated.\n",
    "Optimality in this sense is  established in the celebrated Eckart–Young theorem. See [https://en.wikipedia.org/wiki/Low-rank_approximation](https://en.wikipedia.org/wiki/Low-rank_approximation).\n",
    "\n",
    "Let’s do another exercise, but now we’ll set $ m = 2 < 5 = n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f32e1b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2,5)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b6858",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e62e1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2750a",
   "metadata": {},
   "source": [
    "## PCA with Eigenvalues and Eigenvectors\n",
    "\n",
    "We now  use an eigen decomposition of a sample covariance matrix to do PCA.\n",
    "\n",
    "Let $ X_{m \\times n} $ be our $ m \\times n $ data matrix.\n",
    "\n",
    "Let’s assume that sample means of all variables are zero.\n",
    "\n",
    "We can assure  this  by **pre-processing** the data by subtracting sample means.\n",
    "\n",
    "Define the sample covariance matrix $ \\Omega $ as\n",
    "\n",
    "$$\n",
    "\\Omega = XX^T\n",
    "$$\n",
    "\n",
    "Then use an eigen decomposition to represent $ \\Omega $ as follows:\n",
    "\n",
    "$$\n",
    "\\Omega =P\\Lambda P^T\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ P $ is $ m×m $ matrix of eigenvectors of $ \\Omega $  \n",
    "- $ \\Lambda $ is a diagonal matrix of eigenvalues of $ \\Omega $  \n",
    "\n",
    "\n",
    "We can then represent $ X $ as\n",
    "\n",
    "$$\n",
    "X=P\\epsilon\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda .\n",
    "$$\n",
    "\n",
    "We can verify that\n",
    "\n",
    "$$\n",
    "XX^T=P\\Lambda P^T .\n",
    "$$\n",
    "\n",
    "It follows that we can represent the data matrix as\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^T=\\Lambda .\n",
    "$$\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we obtained through the SVD above, we first note that $ \\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j $.\n",
    "\n",
    "Now define  $ \\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}} $,\n",
    "which evidently implies that $ \\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^T=1 $.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m} ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which evidently agrees with\n",
    "\n",
    "$$\n",
    "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}\n",
    "$$\n",
    "\n",
    "provided that  we set\n",
    "\n",
    "- $ U_j=P_j $ (the loadings of variables on principal components)  \n",
    "- $ {V_k}^{T}=\\tilde{\\epsilon_k} $ (the principal components)  \n",
    "\n",
    "\n",
    "Since there are several possible ways of computing  $ P $ and $ U $ for  given a data matrix $ X $, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We can resolve such ambiguities about  $ U $ and $ P $ by\n",
    "\n",
    "1. sorting eigenvalues and singular values in descending order  \n",
    "1. imposing positive diagonals on $ P $ and $ U $ and adjusting signs in $ V^T $ accordingly  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493da52",
   "metadata": {},
   "source": [
    "## Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider the following SVD of an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "XX^T&=U\\Sigma V^TV\\Sigma^T U^T\\cr\n",
    "&\\equiv U\\Sigma\\Sigma^TU^T\\cr\n",
    "&\\equiv U\\Lambda U^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, $ U $ in the SVD is the matrix $ P $  of\n",
    "eigenvectors of $ XX^T $ and $ \\Sigma \\Sigma^T $ is the matrix $ \\Lambda $ of eigenvalues.\n",
    "\n",
    "Second, let’s compute\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^TX &=V\\Sigma^T U^TU\\Sigma V^T\\\\\n",
    "&=V\\Sigma^T{\\Sigma}V^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the matrix $ V $ in the SVD is the matrix of eigenvectors of $ X^TX $\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "X X^T = P \\Lambda P^T\n",
    "$$\n",
    "\n",
    "where $ P $ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $ X $, we know that\n",
    "\n",
    "$$\n",
    "X X^T = U \\Sigma \\Sigma^T U^T\n",
    "$$\n",
    "\n",
    "where $ U $ is an orthonal matrix.\n",
    "\n",
    "Thus, $ P = U $ and we have the representation of $ X $\n",
    "\n",
    "$$\n",
    "X = P \\epsilon = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "U^T X = \\Sigma V^T = \\epsilon\n",
    "$$\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$\n",
    "\\epsilon \\epsilon^T = \\Sigma V^T V \\Sigma^T = \\Sigma \\Sigma^T = \\Lambda ,\n",
    "$$\n",
    "\n",
    "so that everything fits together.\n",
    "\n",
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d049433",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Ω = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        𝜆, P = LA.eigh(self.Ω)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(𝜆.size), key=lambda x: 𝜆[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.𝜆 = 𝜆[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Λ = np.diag(self.𝜆)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.𝜆) / self.𝜆.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.𝜖 = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        𝜖 = self.𝜖[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ 𝜖\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, 𝜎, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(𝜎.size), key=lambda x: 𝜎[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.𝜎 = 𝜎[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Σ = np.zeros((self.m, self.n))\n",
    "        self.Σ[:d, :d] = np.diag(self.𝜎)\n",
    "\n",
    "        𝜎_sq = self.𝜎 ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(𝜎_sq) / 𝜎_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Σ = self.Σ[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Σ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        𝜖 = self.𝜖[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ 𝜖\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Σ = self.Σ[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Σ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05ed85",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b0277",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'λ = {da.λ}\\n')\n",
    "    print(f'σ^2 = {da.σ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.ε.T)\n",
    "    axs[0].set_title('ε')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.λ))\n",
    "    axs[1].set_title('$V^T*\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b4f40",
   "metadata": {},
   "source": [
    "For an example  PCA applied to analyzing the structure of intelligence tests see this lecture [Multivariable Normal Distribution](https://python.quantecon.org/multivariate_normal.html).\n",
    "\n",
    "Look at the parts of that lecture that describe and illustrate the classic factor analysis model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34bb67",
   "metadata": {},
   "source": [
    "## Dynamic Mode Decomposition (DMD)\n",
    "\n",
    "We turn to the case in which $ m >>n $ in which an $ m \\times n $  data matrix $ \\tilde X $ contains many more random variables $ m $ than observations $ n $.\n",
    "\n",
    "This  **tall and skinny** case is associated with **Dynamic Mode Decomposition**.\n",
    "\n",
    "You can read about Dynamic Mode Decomposition here [[KBBWP16](https://python.quantecon.org/zreferences.html#id24)] and here [[BK19](https://python.quantecon.org/zreferences.html#id25)] (section 7.2).\n",
    "\n",
    "We want to fit a **first-order vector autoregression**\n",
    "\n",
    "\n",
    "<a id='equation-eq-varfirstorder'></a>\n",
    "$$\n",
    "X_{t+1} = A X_t + C \\epsilon_{t+1} \\tag{6.3}\n",
    "$$\n",
    "\n",
    "where\n",
    "the $ m \\times 1 $ vector $ X_t $ is\n",
    "\n",
    "\n",
    "<a id='equation-eq-xvector'></a>\n",
    "$$\n",
    "X_t = \\begin{bmatrix}  X_{1,t} & X_{2,t} & \\cdots & X_{m,t}     \\end{bmatrix}^T \\tag{6.4}\n",
    "$$\n",
    "\n",
    "and where $ T $ again denotes complex transposition and $ X_{i,t} $ is an observation on variable $ i $ at time $ t $.\n",
    "\n",
    "We want to fit equation [(6.3)](#equation-eq-varfirstorder).\n",
    "\n",
    "Our data is assembled in the form of  an $ m \\times n $ matrix  $ \\tilde X $\n",
    "\n",
    "$$\n",
    "\\tilde X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $ t = 1, \\ldots, n $,  the $ m \\times 1 $ vector $ X_t $ is given by [(6.4)](#equation-eq-xvector).\n",
    "\n",
    "We want to estimate system  [(6.3)](#equation-eq-varfirstorder) consisting of $ m $ least squares regressions of **everything** on one lagged value of **everything**.\n",
    "\n",
    "We proceed as follows.\n",
    "\n",
    "From $ \\tilde X $,  we  form two matrices\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_{n-1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "X' =  \\begin{bmatrix} X_2 \\mid X_3 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here $ ' $ does not denote matrix transposition but instead is part of the name of the matrix $ X' $.\n",
    "\n",
    "In forming $ X $ and $ X' $, we have in each case  dropped a column from $ \\tilde X $,  the last column in the case of $ X $, and  the first column in the case of $ X' $.\n",
    "\n",
    "Evidently, $ X $ and $ X' $ are both $ m \\times \\tilde n $ matrices where $ \\tilde n = n - 1 $.\n",
    "\n",
    "We denote the rank of $ X $ as $ p \\leq \\min(m, \\tilde n) $.\n",
    "\n",
    "Two possible cases are when\n",
    "\n",
    "- $ \\tilde n > > m $, so that we have many more time series  observations $ \\tilde n $ than variables $ m $  \n",
    "- $ m > > \\tilde n $, so that we have many more variables $ m $ than time series observations $ \\tilde n $  \n",
    "\n",
    "\n",
    "At a general level that includes both of these special cases, a common formula describes the least squares estimator $ \\hat A $ of $ A $ for both cases, but important  details differ.\n",
    "\n",
    "The common formula is\n",
    "\n",
    "$$\n",
    "\\hat A = X' X^+\n",
    "$$\n",
    "\n",
    "where $ X^+ $ is the pseudo-inverse of $ X $.\n",
    "\n",
    "Formulas for the pseudo-inverse differ for our two cases.\n",
    "\n",
    "When $ \\tilde n > > m $, so that we have many more time series  observations $ \\tilde n $ than variables $ m $ and when\n",
    "$ X $ has linearly independent **rows**, $ X X^T $ has an inverse and the pseudo-inverse $ X^+ $ is\n",
    "\n",
    "$$\n",
    "X^+ = X^T (X X^T)^{-1}\n",
    "$$\n",
    "\n",
    "Here $ X^+ $ is a **right-inverse** that verifies $ X X^+ = I_{m \\times m} $.\n",
    "\n",
    "In this case, our formula for the least-squares estimator of $ A $ becomes\n",
    "\n",
    "$$\n",
    "\\hat A = X' X^T (X X^T)^{-1}\n",
    "$$\n",
    "\n",
    "This is formula is widely used in economics to estimate vector autorgressions.\n",
    "\n",
    "The left side is proportional to the empirical cross second moment matrix of $ X_{t+1} $ and $ X_t $ times the inverse\n",
    "of the second moment matrix of $ X_t $, the least-squares formula widely used in econometrics.\n",
    "\n",
    "When $ m > > \\tilde n $, so that we have many more variables $ m $ than time series observations $ \\tilde n $ and when $ X $ has linearly independent **columns**,\n",
    "$ X^T X $ has an inverse and the pseudo-inverse $ X^+ $ is\n",
    "\n",
    "$$\n",
    "X^+ = (X^T X)^{-1} X^T\n",
    "$$\n",
    "\n",
    "Here  $ X^+ $ is a **left-inverse** that verifies $ X^+ X = I_{\\tilde n \\times \\tilde n} $.\n",
    "\n",
    "In this case, our formula for a least-squares estimator of $ A $ becomes\n",
    "\n",
    "\n",
    "<a id='equation-eq-hataversion0'></a>\n",
    "$$\n",
    "\\hat A = X' (X^T X)^{-1} X^T \\tag{6.5}\n",
    "$$\n",
    "\n",
    "This is the case that we are interested in here.\n",
    "\n",
    "Thus, we want to fit equation [(6.3)](#equation-eq-varfirstorder) in a situation in which we have a number $ n $ of observations  that is small relative to the number $ m $ of\n",
    "variables that appear in the vector $ X_t $.\n",
    "\n",
    "We’ll use  efficient algorithms for computing and for constructing reduced rank approximations of  $ \\hat A $ in formula [(6.5)](#equation-eq-hataversion0).\n",
    "\n",
    "To reiterate and supply more  detail about how we can efficiently calculate the pseudo-inverse $ X^+ $, as our  estimator $ \\hat A $ of $ A $ we form an  $ m \\times m $ matrix that  solves the least-squares best-fit problem\n",
    "\n",
    "\n",
    "<a id='equation-eq-alseqn'></a>\n",
    "$$\n",
    "\\hat A = \\textrm{argmin}_{\\check A} || X' - \\check  A X ||_F \\tag{6.6}\n",
    "$$\n",
    "\n",
    "where $ || \\cdot ||_F $ denotes the Frobeneus norm of a matrix.\n",
    "\n",
    "The solution of the problem on the right side of equation [(6.6)](#equation-eq-alseqn) is\n",
    "\n",
    "\n",
    "<a id='equation-eq-hataform'></a>\n",
    "$$\n",
    "\\hat A =  X'  X^{+} . \\tag{6.7}\n",
    "$$\n",
    "\n",
    "Here the (possibly huge) $ \\tilde n \\times m $ matrix $ X^{+} $ is the pseudo-inverse of $ X $.\n",
    "\n",
    "The $ i $th  row of $ \\hat A $ is an $ m \\times 1 $ vector of pseudo-regression coefficients of $ X_{i,t+1} $ on $ X_{j,t}, j = 1, \\ldots, m $.\n",
    "\n",
    "An efficient way to compute the pseudo-inverse $ X^+ $ is to start with  the (reduced) singular value decomposition\n",
    "\n",
    "\n",
    "<a id='equation-eq-svddmd'></a>\n",
    "$$\n",
    "X =  U \\Sigma  V^T \\tag{6.8}\n",
    "$$\n",
    "\n",
    "where $ U $ is $ m \\times p $, $ \\Sigma $ is a $ p \\times p $ diagonal  matrix, and $ V^T $ is a $ p \\times \\tilde n $ matrix.\n",
    "\n",
    "Here $ p $ is the rank of $ X $, where necessarily $ p \\leq \\tilde n $.\n",
    "\n",
    "(We  described and illustrated a **reduced** singular value decomposition above, and compared it with a **full** singular value decomposition.)\n",
    "\n",
    "We can construct a pseudo-inverse $ X^+ $  of $ X $ by using\n",
    "a singular value decomposition of $ X $ in equation [(6.8)](#equation-eq-svddmd)  to compute\n",
    "\n",
    "\n",
    "<a id='equation-eq-xplusformula'></a>\n",
    "$$\n",
    "X^{+} =  V \\Sigma^{-1}  U^T \\tag{6.9}\n",
    "$$\n",
    "\n",
    "where the matrix $ \\Sigma^{-1} $ is constructed by replacing each non-zero element of $ \\Sigma $ with $ \\sigma_j^{-1} $.\n",
    "\n",
    "We can  use formula [(6.9)](#equation-eq-xplusformula)   together with formula [(6.7)](#equation-eq-hataform) to compute the matrix  $ \\hat A $ of regression coefficients.\n",
    "\n",
    "Thus, our  estimator $ \\hat A = X' X^+ $ of the $ m \\times m $ matrix of coefficients $ A $    is\n",
    "\n",
    "$$\n",
    "\\hat A = X' V \\Sigma^{-1}  U^T\n",
    "$$\n",
    "\n",
    "In addition to doing that, we’ll eventually use **dynamic mode decomposition** to compute a rank $ r $ approximation to $ A $,\n",
    "where $ r <  p $.\n",
    "\n",
    "Next, we turn to two alternative **reduced order** representations of our dynamic system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f9745",
   "metadata": {},
   "source": [
    "## Representation 1\n",
    "\n",
    "We use the $ p $  columns of $ U $, and thus the $ p $ rows of $ U^T $,  to define   a $ p \\times 1 $  vector $ \\tilde X_t $ as follows\n",
    "\n",
    "\n",
    "<a id='equation-eq-tildexdef2'></a>\n",
    "$$\n",
    "\\tilde b_t = U^T X_t \\tag{6.10}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "<a id='equation-eq-xdecoder'></a>\n",
    "$$\n",
    "X_t - U \\tilde b_t \\tag{6.11}\n",
    "$$\n",
    "\n",
    "(Here we use $ b $ to remind us that we are creating a **basis** vector.)\n",
    "\n",
    "Since $ U U^T $ is an $ m \\times m $ identity matrix, it follows from equation [(6.10)](#equation-eq-tildexdef2) that we can reconstruct  $ X_t $ from $ \\tilde b_t $ by using\n",
    "\n",
    "- Equation [(6.10)](#equation-eq-tildexdef2) serves as an **encoder** that  summarizes the $ m \\times 1 $ vector $ X_t $ by a $ p \\times 1 $ vector $ \\tilde b_t $  \n",
    "- Equation [(6.11)](#equation-eq-xdecoder) serves as a **decoder** that recovers the $ m \\times 1 $ vector $ X_t $ from the $ p \\times 1 $ vector $ \\tilde b_t $  \n",
    "\n",
    "\n",
    "Define the reduced transition matrix\n",
    "\n",
    "$$\n",
    "\\tilde A = U^T \\hat A U\n",
    "$$\n",
    "\n",
    "We can evidently recover $ A $ from\n",
    "\n",
    "$$\n",
    "\\hat A = U \\tilde A U^T\n",
    "$$\n",
    "\n",
    "Dynamics of the reduced $ p \\times 1 $ state $ \\tilde b_t $ are governed by\n",
    "\n",
    "$$\n",
    "\\tilde b_{t+1} = \\tilde A \\tilde b_t\n",
    "$$\n",
    "\n",
    "To construct forecasts $ \\overline X_t $ of  future values of $ X_t $ conditional on $ X_1 $, we can apply  decoders to both sides of this\n",
    "equation and deduce\n",
    "\n",
    "$$\n",
    "\\overline X_{t+1} = U \\tilde A^t U^T X_1\n",
    "$$\n",
    "\n",
    "where we use $ \\overline X_t $ to denote a forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ba0af",
   "metadata": {},
   "source": [
    "## Representation 2\n",
    "\n",
    "Form an eigendecomposition of $ \\tilde A $:\n",
    "\n",
    "$$\n",
    "\\tilde A = W \\Lambda W^{-1}\n",
    "$$\n",
    "\n",
    "where $ \\Lambda $ is a diagonal matrix of eigenvalues and $ W $ is a $ p \\times p $\n",
    "matrix whose columns are eigenvectors  corresponding to rows (eigenvalues) in\n",
    "$ \\Lambda $.\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\n",
    "A = U \\tilde U^T = U W \\Lambda W^{-1} U^T\n",
    "$$\n",
    "\n",
    "Thus, the systematic part of the $ X_t $ dynamics captured by our first-order vector autoregressions   are described by\n",
    "\n",
    "$$\n",
    "X_{t+1} = U W \\Lambda W^{-1} U^T  X_t\n",
    "$$\n",
    "\n",
    "Multiplying both sides of the above equation by $ W^{-1} U^T $ gives\n",
    "\n",
    "$$\n",
    "W^{-1} U^T X_{t+1} = \\Lambda W^{-1} U^T X_t\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\hat b_{t+1} = \\Lambda \\hat b_t\n",
    "$$\n",
    "\n",
    "where now our endoder is\n",
    "\n",
    "$$\n",
    "\\hat b_t = W^{-1} U^T X_t\n",
    "$$\n",
    "\n",
    "and our decoder is\n",
    "\n",
    "$$\n",
    "X_t = U W \\hat b_t\n",
    "$$\n",
    "\n",
    "We can use this representation to predict future $ X_t $’s via:\n",
    "\n",
    "$$\n",
    "\\overline X_{t+1} = U W \\Lambda^t W^{-1} U^T X_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636710d3",
   "metadata": {},
   "source": [
    "## Using Fewer Modes\n",
    "\n",
    "The preceding formulas assume that we have retained all $ p $ modes associated with the positive\n",
    "singular values of $ X $.\n",
    "\n",
    "We can easily adapt all of the formulas to describe a situation in which we instead retain only\n",
    "the $ r < p $ largest singular values.\n",
    "\n",
    "In that case, we simply replace $ \\Sigma $ with the appropriate $ r \\times r $ matrix of singular values,\n",
    "$ U $ with the $ m \\times r $ matrix of whose columns correspond to the $ r $ largest singular values,\n",
    "and $ V $ with the $ \\tilde n \\times r $ matrix whose columns correspond to the $ r $ largest  singular values.\n",
    "\n",
    "Counterparts of all of the salient formulas above then apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca575d",
   "metadata": {},
   "source": [
    "## Source for Some Python Code\n",
    "\n",
    "You can find a Python implementation of DMD here:\n",
    "\n",
    "[https://mathlab.github.io/PyDMD/](https://mathlab.github.io/PyDMD/)"
   ]
  }
 ],
 "metadata": {
  "date": 1648457980.473455,
  "filename": "svd_intro.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Singular Value Decomposition (SVD)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}