{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6daae",
   "metadata": {},
   "source": [
    "\n",
    "<a id='divergence-measures'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224ed60",
   "metadata": {},
   "source": [
    "# Statistical Divergence Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6bab7",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Statistical Divergence Measures](#Statistical-Divergence-Measures)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Primer on entropy, cross-entropy, KL divergence](#Primer-on-entropy,-cross-entropy,-KL-divergence)  \n",
    "  - [Two Beta distributions: running example](#Two-Beta-distributions:-running-example)  \n",
    "  - [Kullback–Leibler divergence](#Kullback–Leibler-divergence)  \n",
    "  - [Jensen-Shannon divergence](#Jensen-Shannon-divergence)  \n",
    "  - [Chernoff entropy](#Chernoff-entropy)  \n",
    "  - [Comparing divergence measures](#Comparing-divergence-measures)  \n",
    "  - [KL divergence and maximum-likelihood estimation](#KL-divergence-and-maximum-likelihood-estimation)  \n",
    "  - [Related lectures](#Related-lectures)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e8041",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A statistical divergence quantifies discrepancies between two distinct\n",
    "probability distributions that can be   challenging to distinguish for the following reason:\n",
    "\n",
    "- every event that has positive probability  under one of the distributions also has positive probability under the other distribution  \n",
    "- this means that  there is no “smoking gun” event whose occurrence  tells  a statistician that one of the probability distributions surely governs the data  \n",
    "\n",
    "\n",
    "A statistical divergence is a **function** that maps two  probability distributions into a nonnegative real number.\n",
    "\n",
    "Statistical divergence functions  play important roles in statistics, information theory, and what many people now call “machine learning”.\n",
    "\n",
    "This lecture describes  three divergence measures:\n",
    "\n",
    "- **Kullback–Leibler (KL) divergence**  \n",
    "- **Jensen–Shannon (JS) divergence**  \n",
    "- **Chernoff entropy**  \n",
    "\n",
    "\n",
    "These will appear in several quantecon lectures.\n",
    "\n",
    "Let’s start by importing the necessary Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef0ce0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13dd9b",
   "metadata": {},
   "source": [
    "## Primer on entropy, cross-entropy, KL divergence\n",
    "\n",
    "Before diving in, we’ll introduce some useful concepts in a simple setting.\n",
    "\n",
    "We’ll temporarily assume that $ f $ and $ g $ are two probability mass functions for discrete random variables\n",
    "on state space $ I = \\{1, 2, \\ldots, n\\} $  that satisfy $ f_i \\geq 0, \\sum_{i} f_i =1, g_i \\geq 0, \\sum_{i} g_i =1 $.\n",
    "\n",
    "We follow some  statisticians and information theorists who  define the **surprise** or **surprisal**\n",
    "associated with having  observed a single draw $ x = i $ from distribution $ f $  as\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{1}{f_i}\\right)\n",
    "$$\n",
    "\n",
    "They then define the **information** that you can   anticipate  to gather from observing a single realization\n",
    "as the expected surprisal\n",
    "\n",
    "$$\n",
    "H(f) = \\sum_i f_i \\log\\left(\\frac{1}{f_i}\\right).\n",
    "$$\n",
    "\n",
    "Claude Shannon [[Shannon, 1948](https://python.quantecon.org/zreferences.html#id9)] called $ H(f) $ the **entropy** of distribution $ f $.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">By maximizing $ H(f) $ with respect to $ \\{f_1, f_2, \\ldots, f_n\\} $ subject to $ \\sum_i f_i = 1 $, we can verify that the distribution\n",
    "that maximizes entropy is the uniform distribution\n",
    "$ f_i = \\frac{1}{n} . $\n",
    "Entropy $ H(f) $ for the uniform distribution evidently equals $ - \\log(n) $.\n",
    "\n",
    "Kullback and Leibler [[Kullback and Leibler, 1951](https://python.quantecon.org/zreferences.html#id10)] define the amount of information that a single draw of $ x $ provides for distinguishing $ f $ from $ g $  as the log likelihood ratio\n",
    "\n",
    "$$\n",
    "\\log \\frac{f(x)}{g(x)}\n",
    "$$\n",
    "\n",
    "The following two  concepts are widely used to compare two distributions $ f $ and $ g $.\n",
    "\n",
    "**Cross-Entropy:**\n",
    "\n",
    "\n",
    "<a id='equation-77176920-a54b-4d0b-b46a-f0be9677b82f'></a>\n",
    "$$\n",
    "\\begin{equation}\n",
    "H(f,g) = -\\sum_{i} f_i \\log g_i\n",
    "\\end{equation} \\tag{21.1}\n",
    "$$\n",
    "\n",
    "**Kullback-Leibler (KL) Divergence:**\n",
    "\n",
    "\n",
    "<a id='equation-6aa21655-5abb-4f33-9b9b-5e05b9941c17'></a>\n",
    "$$\n",
    "\\begin{equation}\n",
    "D_{KL}(f \\parallel g) = \\sum_{i} f_i \\log\\left[\\frac{f_i}{g_i}\\right]\n",
    "\\end{equation} \\tag{21.2}\n",
    "$$\n",
    "\n",
    "These concepts are related by the following equality.\n",
    "\n",
    "\n",
    "<a id='equation-eq-klcross'></a>\n",
    "$$\n",
    "D_{KL}(f \\parallel g) = H(f,g) - H(f) \\tag{21.3}\n",
    "$$\n",
    "\n",
    "To prove [(21.3)](#equation-eq-klcross), note that\n",
    "\n",
    "\n",
    "<a id='equation-7bc23e14-4731-4e15-8924-6515bed8b335'></a>\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(f \\parallel g) &= \\sum_{i} f_i \\log\\left[\\frac{f_i}{g_i}\\right] \\\\\n",
    "&= \\sum_{i} f_i \\left[\\log f_i - \\log g_i\\right] \\\\\n",
    "&= \\sum_{i} f_i \\log f_i - \\sum_{i} f_i \\log g_i \\\\\n",
    "&= -H(f) + H(f,g) \\\\\n",
    "&= H(f,g) - H(f)\n",
    "\\end{align} \\tag{21.4}\n",
    "$$\n",
    "\n",
    "Remember that $ H(f) $ is the anticipated surprisal from drawing $ x $ from $ f $.\n",
    "\n",
    "Then the above equation tells us that  the KL divergence is an anticipated “excess surprise” that comes from anticipating that $ x $ is drawn from $ f $ when it is\n",
    "actually drawn from $ g $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d9450",
   "metadata": {},
   "source": [
    "## Two Beta distributions: running example\n",
    "\n",
    "We’ll use Beta distributions extensively to illustrate concepts.\n",
    "\n",
    "The Beta distribution is particularly convenient as it’s defined on $ [0,1] $ and exhibits diverse shapes by appropriately choosing its  two parameters.\n",
    "\n",
    "The density of a Beta distribution with parameters $ a $ and $ b $ is given by\n",
    "\n",
    "$$\n",
    "f(z; a, b) = \\frac{\\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\\Gamma(a) \\Gamma(b)}\n",
    "\\quad \\text{where} \\quad\n",
    "\\Gamma(p) := \\int_{0}^{\\infty} x^{p-1} e^{-x} dx\n",
    "$$\n",
    "\n",
    "We introduce two Beta distributions $ f(x) $ and $ g(x) $, which we will use to illustrate the different divergence measures.\n",
    "\n",
    "Let’s define parameters and density functions in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a61ea",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters in the two Beta distributions\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# The two density functions\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))\n",
    "\n",
    "# Plot the distributions\n",
    "x_range = np.linspace(0.001, 0.999, 1000)\n",
    "f_vals = [f(x) for x in x_range]\n",
    "g_vals = [g(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f_vals, 'b-', linewidth=2, label=r'$f(x) \\sim \\text{Beta}(1,1)$')\n",
    "plt.plot(x_range, g_vals, 'r-', linewidth=2, label=r'$g(x) \\sim \\text{Beta}(3,1.2)$')\n",
    "\n",
    "# Fill overlap region\n",
    "overlap = np.minimum(f_vals, g_vals)\n",
    "plt.fill_between(x_range, 0, overlap, alpha=0.3, color='purple', label='overlap')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe37f0",
   "metadata": {},
   "source": [
    "\n",
    "<a id='rel-entropy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f29c5",
   "metadata": {},
   "source": [
    "## Kullback–Leibler divergence\n",
    "\n",
    "Our  first divergence function is the **Kullback–Leibler (KL) divergence**.\n",
    "\n",
    "For probability densities (or pmfs) $ f $ and $ g $ it is defined by\n",
    "\n",
    "$$\n",
    "D_{KL}(f\\|g) = KL(f, g) = \\int f(x) \\log \\frac{f(x)}{g(x)} \\, dx.\n",
    "$$\n",
    "\n",
    "We can interpret $ D_{KL}(f\\|g) $ as the expected excess log loss (expected excess surprisal) incurred when we use $ g $ while the data are generated by $ f $.\n",
    "\n",
    "It has several important properties:\n",
    "\n",
    "- Non-negativity (Gibbs’ inequality): $ D_{KL}(f\\|g) \\ge 0 $ with equality if and only if $ f=g $ almost everywhere.  \n",
    "- Asymmetry: $ D_{KL}(f\\|g) \\neq D_{KL}(g\\|f) $ in general (hence it is not a metric)  \n",
    "- Information decomposition:\n",
    "  $ D_{KL}(f\\|g) = H(f,g) - H(f) $, where $ H(f,g) $ is the cross entropy and $ H(f) $ is the Shannon entropy of $ f $.  \n",
    "- Chain rule: For joint distributions $ f(x, y) $ and $ g(x, y) $,\n",
    "  $ D_{KL}(f(x,y)\\|g(x,y)) = D_{KL}(f(x)\\|g(x)) + E_{f}\\left[D_{KL}(f(y|x)\\|g(y|x))\\right] $  \n",
    "\n",
    "\n",
    "KL divergence plays a central role in statistical inference, including model selection and hypothesis testing.\n",
    "\n",
    "[Likelihood Ratio Processes](https://python.quantecon.org/likelihood_ratio_process.html) describes a link between KL divergence and the expected log likelihood ratio,\n",
    "and the lecture [A Problem that Stumped Milton Friedman](https://python.quantecon.org/wald_friedman.html) connects it to the test performance of the sequential probability ratio test.\n",
    "\n",
    "Let’s compute the KL divergence between our example distributions $ f $ and $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961073e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL(f, g):\n",
    "    \"\"\"\n",
    "    Compute KL divergence KL(f, g) via numerical integration\n",
    "    \"\"\"\n",
    "    def integrand(w):\n",
    "        fw = f(w)\n",
    "        gw = g(w)\n",
    "        return fw * np.log(fw / gw)\n",
    "    val, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return val\n",
    "\n",
    "# Compute KL divergences between our example distributions\n",
    "kl_fg = compute_KL(f, g)\n",
    "kl_gf = compute_KL(g, f)\n",
    "\n",
    "print(f\"KL(f, g) = {kl_fg:.4f}\")\n",
    "print(f\"KL(g, f) = {kl_gf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b774f54",
   "metadata": {},
   "source": [
    "The asymmetry of KL divergence has important practical implications.\n",
    "\n",
    "$ D_{KL}(f\\|g) $ penalizes regions where $ f > 0 $ but $ g $ is close to zero, reflecting the cost of using $ g $ to model $ f $ and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c206964",
   "metadata": {},
   "source": [
    "## Jensen-Shannon divergence\n",
    "\n",
    "Sometimes we want a symmetric measure of divergence that captures the difference between two distributions without favoring one over the other.\n",
    "\n",
    "This often arises in applications like clustering, where we want to compare distributions without assuming one is the true model.\n",
    "\n",
    "The **Jensen-Shannon (JS) divergence** symmetrizes KL divergence by comparing both distributions to their mixture:\n",
    "\n",
    "$$\n",
    "JS(f,g) = \\frac{1}{2} D_{KL}(f\\|m) + \\frac{1}{2} D_{KL}(g\\|m), \\quad m = \\frac{1}{2}(f+g).\n",
    "$$\n",
    "\n",
    "where $ m $ is a mixture distribution that averages $ f $ and $ g $\n",
    "\n",
    "Let’s also visualize the mixture distribution $ m $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef4061",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def m(x):\n",
    "    return 0.5 * (f(x) + g(x))\n",
    "\n",
    "m_vals = [m(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f_vals, 'b-', linewidth=2, label=r'$f(x)$')\n",
    "plt.plot(x_range, g_vals, 'r-', linewidth=2, label=r'$g(x)$')\n",
    "plt.plot(x_range, m_vals, 'g--', linewidth=2, label=r'$m(x) = \\frac{1}{2}(f(x) + g(x))$')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693ba2f",
   "metadata": {},
   "source": [
    "The JS divergence has several useful properties:\n",
    "\n",
    "- Symmetry: $ JS(f,g)=JS(g,f) $.  \n",
    "- Boundedness: $ 0 \\le JS(f,g) \\le \\log 2 $.  \n",
    "- Its square root $ \\sqrt{JS} $ is a metric (Jensen–Shannon distance) on the space of probability distributions.  \n",
    "- JS divergence equals the mutual information between a binary random variable $ Z \\sim \\text{Bernoulli}(1/2) $ indicating the source and a sample $ X $ drawn from $ f $ if $ Z=0 $ or from $ g $ if $ Z=1 $.  \n",
    "\n",
    "\n",
    "The Jensen–Shannon divergence plays a key role in the optimization of certain\n",
    "generative models, as it is bounded, symmetric, and smoother than KL divergence,\n",
    "often providing more stable gradients for training.\n",
    "\n",
    "Let’s compute the JS divergence between our example distributions $ f $ and $ g $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f0bba",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_JS(f, g):\n",
    "    \"\"\"Compute Jensen-Shannon divergence.\"\"\"\n",
    "    def m(w):\n",
    "        return 0.5 * (f(w) + g(w))\n",
    "    js_div = 0.5 * compute_KL(f, m) + 0.5 * compute_KL(g, m)\n",
    "    return js_div\n",
    "\n",
    "js_div = compute_JS(f, g)\n",
    "print(f\"Jensen-Shannon divergence JS(f,g) = {js_div:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443783a",
   "metadata": {},
   "source": [
    "We can easily generalize to more than two distributions using the generalized Jensen-Shannon divergence with weights $ \\alpha = (\\alpha_i)_{i=1}^{n} $:\n",
    "\n",
    "$$\n",
    "JS_\\alpha(f_1, \\ldots, f_n) = \n",
    "H\\left(\\sum_{i=1}^n \\alpha_i f_i\\right) - \\sum_{i=1}^n \\alpha_i H(f_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\alpha_i \\geq 0 $ and $ \\sum_{i=1}^n \\alpha_i = 1 $, and  \n",
    "- $ H(f) = -\\int f(x) \\log f(x) dx $ is the **Shannon entropy** of distribution $ f $  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804cc81",
   "metadata": {},
   "source": [
    "## Chernoff entropy\n",
    "\n",
    "Chernoff entropy originates from early applications of the [theory of large deviations](https://en.wikipedia.org/wiki/Large_deviations_theory), which refines central limit approximations by providing exponential decay rates for rare events.\n",
    "\n",
    "For densities $ f $ and $ g $ the Chernoff entropy is\n",
    "\n",
    "$$\n",
    "C(f,g) = - \\log \\min_{\\phi \\in (0,1)} \\int f^{\\phi}(x) g^{1-\\phi}(x) \\, dx.\n",
    "$$\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- The inner integral is the **Chernoff coefficient**.  \n",
    "- At $ \\phi=1/2 $ it becomes the **Bhattacharyya coefficient** $ \\int \\sqrt{f g} $.  \n",
    "- In binary hypothesis testing with $ T $ iid observations, the optimal error probability decays as $ e^{-C(f,g) T} $.  \n",
    "\n",
    "\n",
    "We will see an example of the third point in the lecture [Likelihood Ratio Processes](https://python.quantecon.org/likelihood_ratio_process.html),\n",
    "where we study the Chernoff entropy in the context of model selection.\n",
    "\n",
    "Let’s compute the Chernoff entropy between our example distributions $ f $ and $ g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c932b0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def chernoff_integrand(ϕ, f, g):\n",
    "    \"\"\"Integral entering Chernoff entropy for a given ϕ.\"\"\"\n",
    "    def integrand(w):\n",
    "        return f(w)**ϕ * g(w)**(1-ϕ)\n",
    "    result, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return result\n",
    "\n",
    "def compute_chernoff_entropy(f, g):\n",
    "    \"\"\"Compute Chernoff entropy C(f,g).\"\"\"\n",
    "    def objective(ϕ):\n",
    "        return chernoff_integrand(ϕ, f, g)\n",
    "    result = minimize_scalar(objective, bounds=(1e-5, 1-1e-5), method='bounded')\n",
    "    min_value = result.fun\n",
    "    ϕ_optimal = result.x\n",
    "    chernoff_entropy = -np.log(min_value)\n",
    "    return chernoff_entropy, ϕ_optimal\n",
    "\n",
    "C_fg, ϕ_optimal = compute_chernoff_entropy(f, g)\n",
    "print(f\"Chernoff entropy C(f,g) = {C_fg:.4f}\")\n",
    "print(f\"Optimal ϕ = {ϕ_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9fe0d",
   "metadata": {},
   "source": [
    "## Comparing divergence measures\n",
    "\n",
    "We now compare these measures across several pairs of Beta distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae63ca0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "distribution_pairs = [\n",
    "    # (f_params, g_params)\n",
    "    ((1, 1), (0.1, 0.2)),\n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (0.3, 0.4)),\n",
    "    ((1, 1), (0.5, 0.5)),\n",
    "    ((1, 1), (0.7, 0.6)),\n",
    "    ((1, 1), (0.9, 0.8)),\n",
    "    ((1, 1), (1.1, 1.05)),\n",
    "    ((1, 1), (1.2, 1.1)),\n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),\n",
    "    ((1, 1), (2.5, 1.8)),\n",
    "    ((1, 1), (3, 1.2)),\n",
    "    ((1, 1), (4, 1)),\n",
    "    ((1, 1), (5, 1))\n",
    "]\n",
    "\n",
    "# Create comparison table\n",
    "results = []\n",
    "for i, ((f_a, f_b), (g_a, g_b)) in enumerate(distribution_pairs):\n",
    "    f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "    g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "    kl_fg = compute_KL(f, g)\n",
    "    kl_gf = compute_KL(g, f)\n",
    "    js_div = compute_JS(f, g)\n",
    "    chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "    results.append({\n",
    "        'Pair (f, g)': f\"\\\\text{{Beta}}({f_a},{f_b}), \\\\text{{Beta}}({g_a},{g_b})\",\n",
    "        'KL(f, g)': f\"{kl_fg:.4f}\",\n",
    "        'KL(g, f)': f\"{kl_gf:.4f}\",\n",
    "        'JS': f\"{js_div:.4f}\",\n",
    "        'C': f\"{chernoff_ent:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "# Sort by JS divergence\n",
    "df['JS_numeric'] = df['JS'].astype(float)\n",
    "df = df.sort_values('JS_numeric').drop('JS_numeric', axis=1)\n",
    "\n",
    "columns = ' & '.join([f'\\\\text{{{col}}}' for col in df.columns])\n",
    "rows = ' \\\\\\\\\\n'.join(\n",
    "    [' & '.join([f'{val}' for val in row]) \n",
    "     for row in df.values])\n",
    "\n",
    "latex_code = rf\"\"\"\n",
    "\\begin{{array}}{{lcccc}}\n",
    "{columns} \\\\\n",
    "\\hline\n",
    "{rows}\n",
    "\\end{{array}}\n",
    "\"\"\"\n",
    "\n",
    "display(Math(latex_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c2db0",
   "metadata": {},
   "source": [
    "We can clearly see co-movement across the divergence measures as we vary the parameters of the Beta distributions.\n",
    "\n",
    "Next we visualize relationships among KL, JS, and Chernoff entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a3ba3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "kl_fg_values = [float(result['KL(f, g)']) for result in results]\n",
    "js_values = [float(result['JS']) for result in results]\n",
    "chernoff_values = [float(result['C']) for result in results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(kl_fg_values, js_values, alpha=0.7, s=60)\n",
    "axes[0].set_xlabel('KL divergence KL(f, g)')\n",
    "axes[0].set_ylabel('JS divergence')\n",
    "axes[0].set_title('JS divergence vs KL divergence')\n",
    "\n",
    "axes[1].scatter(js_values, chernoff_values, alpha=0.7, s=60)\n",
    "axes[1].set_xlabel('JS divergence')\n",
    "axes[1].set_ylabel('Chernoff entropy')\n",
    "axes[1].set_title('Chernoff entropy vs JS divergence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb4499",
   "metadata": {},
   "source": [
    "We now generate plots illustrating how overlap visually diminishes as divergence measures increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b8689",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    ((1, 1), (1, 1)),   \n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),  \n",
    "    ((1, 1), (3, 1.2)),  \n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (5, 1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c08f4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_dist_diff(para_grid):\n",
    "    \"\"\"Plot overlap of selected Beta distribution pairs.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    divergence_data = []\n",
    "    for i, ((f_a, f_b), (g_a, g_b)) in enumerate(param_grid):\n",
    "        row, col = divmod(i, 2)\n",
    "        f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "        g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "        kl_fg = compute_KL(f, g)\n",
    "        js_div = compute_JS(f, g)\n",
    "        chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "        divergence_data.append({\n",
    "            'f_params': (f_a, f_b),\n",
    "            'g_params': (g_a, g_b),\n",
    "            'kl_fg': kl_fg,\n",
    "            'js_div': js_div,\n",
    "            'chernoff': chernoff_ent\n",
    "        })\n",
    "        x_range = np.linspace(0, 1, 200)\n",
    "        f_vals = [f(x) for x in x_range]\n",
    "        g_vals = [g(x) for x in x_range]\n",
    "        axes[row, col].plot(x_range, f_vals, 'b-', \n",
    "                        linewidth=2, label=f'f ~ Beta({f_a},{f_b})')\n",
    "        axes[row, col].plot(x_range, g_vals, 'r-', \n",
    "                        linewidth=2, label=f'g ~ Beta({g_a},{g_b})')\n",
    "        overlap = np.minimum(f_vals, g_vals)\n",
    "        axes[row, col].fill_between(x_range, 0, \n",
    "                        overlap, alpha=0.3, color='purple', label='overlap')\n",
    "        axes[row, col].set_title(\n",
    "            f'KL(f,g)={kl_fg:.3f}, JS={js_div:.3f}, C={chernoff_ent:.3f}', \n",
    "            fontsize=12)\n",
    "        axes[row, col].legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return divergence_data\n",
    "\n",
    "divergence_data = plot_dist_diff(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f635b1f",
   "metadata": {},
   "source": [
    "## KL divergence and maximum-likelihood estimation\n",
    "\n",
    "Given a sample of $ n $ observations $ X = \\{x_1, x_2, \\ldots, x_n\\} $, the **empirical distribution** is\n",
    "\n",
    "$$\n",
    "p_e(x) = \\frac{1}{n} \\sum_{i=1}^n \\delta(x - x_i)\n",
    "$$\n",
    "\n",
    "where $ \\delta(x - x_i) $ is the Dirac delta function centered at $ x_i $:\n",
    "\n",
    "$$\n",
    "\\delta(x - x_i) = \\begin{cases}\n",
    "+\\infty & \\text{if } x = x_i \\\\\n",
    "0 & \\text{if } x \\neq x_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Discrete probability measure**: Assigns probability $ \\frac{1}{n} $ to each observed data point  \n",
    "- **Empirical expectation**: $ \\langle X \\rangle_{p_e} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{\\mu} $  \n",
    "- **Support**: Only on the observed data points $ \\{x_1, x_2, \\ldots, x_n\\} $  \n",
    "\n",
    "\n",
    "The KL divergence from the empirical distribution $ p_e $ to a parametric model $ p_\\theta(x) $ is:\n",
    "\n",
    "$$\n",
    "D_{KL}(p_e \\parallel p_\\theta) = \\int p_e(x) \\log \\frac{p_e(x)}{p_\\theta(x)} dx\n",
    "$$\n",
    "\n",
    "Using the mathematics of the Dirac delta function, it follows that\n",
    "\n",
    "$$\n",
    "D_{KL}(p_e \\parallel p_\\theta) = \\sum_{i=1}^n \\frac{1}{n} \\log \\frac{\\left(\\frac{1}{n}\\right)}{p_\\theta(x_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{1}{n} - \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\log n - \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(x_i)\n",
    "$$\n",
    "\n",
    "Since the log-likelihood function for parameter $ \\theta $ is:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; X) = \\sum_{i=1}^n \\log p_\\theta(x_i) ,\n",
    "$$\n",
    "\n",
    "it follows that maximum likelihood chooses parameters to minimize\n",
    "\n",
    "$$\n",
    "D_{KL}(p_e \\parallel p_\\theta)\n",
    "$$\n",
    "\n",
    "Thus, MLE is equivalent to minimizing the KL divergence from the empirical distribution to the statistical model $ p_\\theta $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77c992",
   "metadata": {},
   "source": [
    "## Related lectures\n",
    "\n",
    "This lecture has introduced tools  that we’ll encounter elsewhere.\n",
    "\n",
    "- Other quantecon lectures  that apply  connections between divergence measures and statistical inference include  [Likelihood Ratio Processes](https://python.quantecon.org/likelihood_ratio_process.html), [A Problem that Stumped Milton Friedman](https://python.quantecon.org/wald_friedman.html), and [Incorrect Models](https://python.quantecon.org/mix_model.html).  \n",
    "- Statistical divergence functions also take center stage in  [Heterogeneous Beliefs and Financial Markets](https://python.quantecon.org/likelihood_ratio_process_2.html) that studies Lawrence Blume and David Easley’s model of  heterogeneous beliefs and financial markets.  "
   ]
  }
 ],
 "metadata": {
  "date": 1761785055.9239454,
  "filename": "divergence_measures.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Statistical Divergence Measures"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}